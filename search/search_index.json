{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"THIS SITE IS UNDER CONSTRUCTION \u00b6 Welcome to ICPC.NINJA \u00b6 SHARE & GET useful problem solving techniques All Articles in this site is implemented on c++17 Why I created this site? \u00b6 Ipsum nemo veritatis illum nulla veniam. Aut esse blanditiis placeat omnis culpa. Odit iusto repudiandae dolorem molestiae labore. Sed a dolores iusto esse placeat officia Obcaecati corporis perspiciatis sequi consequatur. How I created this site? \u00b6 Dolor eveniet incidunt esse nostrum dolor Cumque totam veniam dolore culpa sapiente Repudiandae velit odit magni commodi quasi? Voluptates officiis ab veniam eos totam Dolorem commodi deleniti unde adipisci nihil How to contribute \u00b6 Dolor vitae soluta praesentium obcaecati minus Enim consequuntur expedita voluptatibus eius dignissimos Praesentium provident commodi natus quisquam amet! Eum illo nisi laboriosam rem veritatis Excepturi sed assumenda libero qui accusamus? emojis \u00b6","title":"Home"},{"location":"#this_site_is_under_construction","text":"","title":"THIS SITE IS UNDER CONSTRUCTION"},{"location":"#welcome_to_icpcninja","text":"SHARE & GET useful problem solving techniques All Articles in this site is implemented on c++17","title":"Welcome to ICPC.NINJA"},{"location":"#why_i_created_this_site","text":"Ipsum nemo veritatis illum nulla veniam. Aut esse blanditiis placeat omnis culpa. Odit iusto repudiandae dolorem molestiae labore. Sed a dolores iusto esse placeat officia Obcaecati corporis perspiciatis sequi consequatur.","title":"Why I created this site?"},{"location":"#how_i_created_this_site","text":"Dolor eveniet incidunt esse nostrum dolor Cumque totam veniam dolore culpa sapiente Repudiandae velit odit magni commodi quasi? Voluptates officiis ab veniam eos totam Dolorem commodi deleniti unde adipisci nihil","title":"How I created this site?"},{"location":"#how_to_contribute","text":"Dolor vitae soluta praesentium obcaecati minus Enim consequuntur expedita voluptatibus eius dignissimos Praesentium provident commodi natus quisquam amet! Eum illo nisi laboriosam rem veritatis Excepturi sed assumenda libero qui accusamus?","title":"How to contribute"},{"location":"#emojis","text":"","title":"emojis"},{"location":"Algorithms/EuclideanAlgorithm/","text":"Euclidean Algorithm \u00b6 Euclidean algorithm, or Euclid's algorithm, is an efficient method for computing the GCD(greatest common divisor) Implementation \u00b6 1 2 3 4 int gcd ( int a , int b ) { if ( ! a ) return b ; return gcd ( b % a , a ); } Proof \u00b6 This proof consists of two parts. - part 1: proves that Euclidean Algorithm gives us a common factor of two integers. - part 2: proves that the common divisor that Euclidean Algorithm produces is the largest possible Part1 \u00b6 Euclidean Algorithm gives us a common factor of two integers($a, b$). $$ \\begin{aligned} a & = q_1b + r_1, &\\text{where}(0<r<b)& \\\\ b & = q_2r_1 + r_2, &\\text{where}(0<r_2<r_1)& \\\\ r_1 & = q_3r_2 + r_3, &\\text{where}(0<r_3<r_2)&\\\\ &\\vdots \\\\ r_i & = q_{i+2}r_{i+1} + r_{i+2}, & \\text{where}( 0 < r_{i+2} < r_{i+1})& \\\\ &\\vdots \\\\ r_{k-2} & = q_{k}r_{k-1} + r_{k}\\\\ r_{k-1} & = q_{k+1}r_k \\\\ \\end{aligned} $$ From the last euqation, we know that $r_k|r_{k-1}$. So, we know that we can express $r_{k-1} = cr_k$. where $c$ is an integer. Now consider the previous equation. $$ \\begin{aligned} r_{k-2} & = q_{k} r_{k-1} + r_{k} \\\\ & = q_{k} cr_{k} + r_{k} \\\\ & = r_{k} (q_{k}c + 1) \\\\ \\end{aligned} $$ Thus, we have that $r_{k} | r_{k-2}$. In our equation previous to that one, we have $$ r_{k-3} = q_{k-1} r_{k-2} + r_{k-1} $$ From here, since $r_{k} | r_{k-1}$ and $r_{k}| r{k-2}$, using our rules of divisibility we have that $r_{k} | r_{k-3}$. As you can see, we can continue this process, considering each previous equation until we get to the last two, where we will find that $r_{k} | a$ and $r_{k} | b$. Thus, we find that Euclids algorithm gives us a common factor of a and b. Part2 \u00b6 The common divisor Euclidean Algorithm produces is the largest possible. We will start by assuming that $a$ and $b$ have a common factor $d$, and then show that $d | r_{k}$. consider an arbitrary common factor d of $a$ and $b$. If $d$ is a common factor, we can rewrite $a$ and $b$ as follows: $$ a = d a^{\\prime}, b = d b^{\\prime}, \\text{where } d, a, b \\text{ are all positive integers } $$ Now, consider the first euqation from Euclidean algorithm: $$ \\begin{aligned} a & = q_{1} b + r_{1} \\\\ r_{1} & = a - q_{1}b \\\\ & = da^{\\prime} - q_{1} d b^{\\prime} \\\\ & = d(a^{\\prime} - q_{1}b^{\\prime}) \\end{aligned} $$ Thus, we have that $d|r_{1}$. Now, consider the second equation, and repeat the steps we did on the first, this time solving for $r_{2}$ $$ \\begin{aligned} b & = q_{2}r_{1} + r_{2} \\\\ r_{2} & = b - q_{2}r_{1} \\\\ & = d b^{\\prime} - q_{2}dr_1^{\\prime} \\\\ & = d (b^{\\prime} - q_{2} r_1^{\\prime}) \\end{aligned} $$ As you can see, we can continue this process through each of the equations until we hit the second to last one, where we will have $$ \\begin{aligned} r_{k-2} & = q_{k}r_{k-1} + r_{k} \\\\ r_{k} & = q_{k}r_{k-1} - r_{k-2} \\\\ & = q_{k}dr_{k-1}^{\\prime} - d r_{k-2}^{\\prime} \\\\ & = d(q_{k}r_{k-1}^{\\prime} - r_{k-2}^{\\prime}) \\end{aligned} $$ Thus, $d| r_k$ But this says that any arbitrary common factor of $a$ and $b$ that we originally picked divides into $r_{k}$, the value that Euclidean algorithm produced. Since we know that $r_{k}$ is a common factor to both $a$ and $b$, this shows that is must be the largest possible common factor, or $gcd(a, b)$ $\\blacksquare$ Extended Euclidean Algorithm \u00b6 Given integers $a$ and $b$, there is always an integral solution to the equation $$ ax + by = gcd(a, b) $$ and we can find the values of $x$ and $y$. implementation \u00b6 not yet Proof \u00b6 Consider writing down the steps of Euclidean Algorithm $$ \\begin{aligned} a & = q_1b + r_1, &\\text{where}(0<r<b)& \\\\ b & = q_2r_1 + r_2, &\\text{where}(0<r_2<r_1)& \\\\ r_1 & = q_3r_2 + r_3, &\\text{where}(0<r_3<r_2)&\\\\ &\\vdots \\\\ r_i & = q_{i+2}r_{i+1} + r_{i+2}, & \\text{where}( 0 < r_{i+2} < r_{i+1})& \\\\ &\\vdots \\\\ r_{k-2} & = q_{k}r_{k-1} + r_{k} & \\text{where}(0 < r_k < r_{k-1}) \\\\ r_{k-1} & = q_{k+1}r_k \\\\ \\end{aligned} $$ Consider solving the second to last euqation for $r_k$. You get $$ \\begin{aligned} r_{k-2} & = q_{k}r_{k-1} + r_{k} \\\\ r_{k} & = r_{k-2} - q_{k}r_{k-1} \\\\ gcd(a, b) & = r_{k-2} - q_{k}r_{k-1} \\end{aligned} $$ Now, solve the previous equation for $r_{k-1}$ $$ \\begin{aligned} r_{k-3} & = q_{k-1}r_{k-2} + r_{k-1} \\\\ r_{k-1} & = r_{k-3} - q_{k-1}r_{k-2} \\\\ \\end{aligned} $$ and we substitute this value in to the previous derived equation $$ \\begin{aligned} gcd(a, b) & = r_{k-2} - q_{k}r_{k-1} \\\\ & = r_{k-2} - q_k(r_{k-3} - q_{k-1}r_{k-2}) \\\\ & = r_{k-2}(1 - q_{k-1}) - q_kr_{k-3} \\end{aligned} $$ Notice that now we have expressed $gcd(a, b)$ as a linear combination of $r_{k-2}$ and $r_{k-3}$. Next we can substitute for of $r_{k-2}$ in terms of $r_{k-3}$ and $r_{k-4}$, so that the $gcd(a, b)$ can be expressed as the linear combination of $r_{k-3}$ and $r_{k-4}$. Eventually, by continuing this process, $gcd(a, b)$ will be expressed as a linear combination of $a$ and $b$ as desired. This process will be much easier to see with examples: Find integers $x$ and $y$ such that $$ 135x + 50y = 5 $$ Use Euclidean Algorithm to compute $gcd(135, 50)$ $$ \\begin{aligned} 135 & = 2 \\sdot 50 + 35 \\cdots\\text{\\textcircled 1}\\\\ 50 & = 1 \\sdot 35 + 15 \\cdots\\text{\\textcircled 2}\\\\ 35 & = 2 \\sdot 15 + 5 \\cdots\\text{\\textcircled 3}\\\\ 15 & = 3 \\sdot 5 \\end{aligned} $$ Now, let's use the Extended Euclidean algorithm to solve the problem $ 5 = 35 - 2 \\sdot 15 $ from equation 3 But, we have that $ 15 = 50 - 35 $ from euqation $\\text{\\textcircled 2}$ Now we, substitute this value into the previously derived equation: $$ \\begin{aligned} 5 & = 35 - 2 \\sdot 15 \\\\ 5 & = 35 - 2 \\sdot (50 - 35) \\\\ 5 & = 3 \\sdot 35 - 2 \\sdot 50 \\end{aligned} $$ Now, finally use the first equation to determine an expression for $35$ as a linear combination of $135$ and $50$ $$ 35 = 135 - 2 \\sdot 50 \\text{ from equation}\\text{\\textcircled 1} $$ Plug this into our last euqation: $$ \\begin{aligned} 5 & = 3 \\sdot 35 - 2 \\sdot 50 \\\\ 5 & = 3 \\sdot (135 - 2 \\sdot 50) - 2 \\sdot 50 \\\\ 5 & = 3 \\sdot 135 - 8 \\sdot 50 \\end{aligned} $$ So, a set of solutions to the equation is $x=3, y = -8$ This article is from 'COT3100Euclid01' \u00b6","title":"Euclidean"},{"location":"Algorithms/EuclideanAlgorithm/#euclidean_algorithm","text":"Euclidean algorithm, or Euclid's algorithm, is an efficient method for computing the GCD(greatest common divisor)","title":"Euclidean Algorithm"},{"location":"Algorithms/EuclideanAlgorithm/#implementation","text":"1 2 3 4 int gcd ( int a , int b ) { if ( ! a ) return b ; return gcd ( b % a , a ); }","title":"Implementation"},{"location":"Algorithms/EuclideanAlgorithm/#proof","text":"This proof consists of two parts. - part 1: proves that Euclidean Algorithm gives us a common factor of two integers. - part 2: proves that the common divisor that Euclidean Algorithm produces is the largest possible","title":"Proof"},{"location":"Algorithms/EuclideanAlgorithm/#part1","text":"Euclidean Algorithm gives us a common factor of two integers($a, b$). $$ \\begin{aligned} a & = q_1b + r_1, &\\text{where}(0<r<b)& \\\\ b & = q_2r_1 + r_2, &\\text{where}(0<r_2<r_1)& \\\\ r_1 & = q_3r_2 + r_3, &\\text{where}(0<r_3<r_2)&\\\\ &\\vdots \\\\ r_i & = q_{i+2}r_{i+1} + r_{i+2}, & \\text{where}( 0 < r_{i+2} < r_{i+1})& \\\\ &\\vdots \\\\ r_{k-2} & = q_{k}r_{k-1} + r_{k}\\\\ r_{k-1} & = q_{k+1}r_k \\\\ \\end{aligned} $$ From the last euqation, we know that $r_k|r_{k-1}$. So, we know that we can express $r_{k-1} = cr_k$. where $c$ is an integer. Now consider the previous equation. $$ \\begin{aligned} r_{k-2} & = q_{k} r_{k-1} + r_{k} \\\\ & = q_{k} cr_{k} + r_{k} \\\\ & = r_{k} (q_{k}c + 1) \\\\ \\end{aligned} $$ Thus, we have that $r_{k} | r_{k-2}$. In our equation previous to that one, we have $$ r_{k-3} = q_{k-1} r_{k-2} + r_{k-1} $$ From here, since $r_{k} | r_{k-1}$ and $r_{k}| r{k-2}$, using our rules of divisibility we have that $r_{k} | r_{k-3}$. As you can see, we can continue this process, considering each previous equation until we get to the last two, where we will find that $r_{k} | a$ and $r_{k} | b$. Thus, we find that Euclids algorithm gives us a common factor of a and b.","title":"Part1"},{"location":"Algorithms/EuclideanAlgorithm/#part2","text":"The common divisor Euclidean Algorithm produces is the largest possible. We will start by assuming that $a$ and $b$ have a common factor $d$, and then show that $d | r_{k}$. consider an arbitrary common factor d of $a$ and $b$. If $d$ is a common factor, we can rewrite $a$ and $b$ as follows: $$ a = d a^{\\prime}, b = d b^{\\prime}, \\text{where } d, a, b \\text{ are all positive integers } $$ Now, consider the first euqation from Euclidean algorithm: $$ \\begin{aligned} a & = q_{1} b + r_{1} \\\\ r_{1} & = a - q_{1}b \\\\ & = da^{\\prime} - q_{1} d b^{\\prime} \\\\ & = d(a^{\\prime} - q_{1}b^{\\prime}) \\end{aligned} $$ Thus, we have that $d|r_{1}$. Now, consider the second equation, and repeat the steps we did on the first, this time solving for $r_{2}$ $$ \\begin{aligned} b & = q_{2}r_{1} + r_{2} \\\\ r_{2} & = b - q_{2}r_{1} \\\\ & = d b^{\\prime} - q_{2}dr_1^{\\prime} \\\\ & = d (b^{\\prime} - q_{2} r_1^{\\prime}) \\end{aligned} $$ As you can see, we can continue this process through each of the equations until we hit the second to last one, where we will have $$ \\begin{aligned} r_{k-2} & = q_{k}r_{k-1} + r_{k} \\\\ r_{k} & = q_{k}r_{k-1} - r_{k-2} \\\\ & = q_{k}dr_{k-1}^{\\prime} - d r_{k-2}^{\\prime} \\\\ & = d(q_{k}r_{k-1}^{\\prime} - r_{k-2}^{\\prime}) \\end{aligned} $$ Thus, $d| r_k$ But this says that any arbitrary common factor of $a$ and $b$ that we originally picked divides into $r_{k}$, the value that Euclidean algorithm produced. Since we know that $r_{k}$ is a common factor to both $a$ and $b$, this shows that is must be the largest possible common factor, or $gcd(a, b)$ $\\blacksquare$","title":"Part2"},{"location":"Algorithms/EuclideanAlgorithm/#extended_euclidean_algorithm","text":"Given integers $a$ and $b$, there is always an integral solution to the equation $$ ax + by = gcd(a, b) $$ and we can find the values of $x$ and $y$.","title":"Extended Euclidean Algorithm"},{"location":"Algorithms/EuclideanAlgorithm/#implementation_1","text":"not yet","title":"implementation"},{"location":"Algorithms/EuclideanAlgorithm/#proof_1","text":"Consider writing down the steps of Euclidean Algorithm $$ \\begin{aligned} a & = q_1b + r_1, &\\text{where}(0<r<b)& \\\\ b & = q_2r_1 + r_2, &\\text{where}(0<r_2<r_1)& \\\\ r_1 & = q_3r_2 + r_3, &\\text{where}(0<r_3<r_2)&\\\\ &\\vdots \\\\ r_i & = q_{i+2}r_{i+1} + r_{i+2}, & \\text{where}( 0 < r_{i+2} < r_{i+1})& \\\\ &\\vdots \\\\ r_{k-2} & = q_{k}r_{k-1} + r_{k} & \\text{where}(0 < r_k < r_{k-1}) \\\\ r_{k-1} & = q_{k+1}r_k \\\\ \\end{aligned} $$ Consider solving the second to last euqation for $r_k$. You get $$ \\begin{aligned} r_{k-2} & = q_{k}r_{k-1} + r_{k} \\\\ r_{k} & = r_{k-2} - q_{k}r_{k-1} \\\\ gcd(a, b) & = r_{k-2} - q_{k}r_{k-1} \\end{aligned} $$ Now, solve the previous equation for $r_{k-1}$ $$ \\begin{aligned} r_{k-3} & = q_{k-1}r_{k-2} + r_{k-1} \\\\ r_{k-1} & = r_{k-3} - q_{k-1}r_{k-2} \\\\ \\end{aligned} $$ and we substitute this value in to the previous derived equation $$ \\begin{aligned} gcd(a, b) & = r_{k-2} - q_{k}r_{k-1} \\\\ & = r_{k-2} - q_k(r_{k-3} - q_{k-1}r_{k-2}) \\\\ & = r_{k-2}(1 - q_{k-1}) - q_kr_{k-3} \\end{aligned} $$ Notice that now we have expressed $gcd(a, b)$ as a linear combination of $r_{k-2}$ and $r_{k-3}$. Next we can substitute for of $r_{k-2}$ in terms of $r_{k-3}$ and $r_{k-4}$, so that the $gcd(a, b)$ can be expressed as the linear combination of $r_{k-3}$ and $r_{k-4}$. Eventually, by continuing this process, $gcd(a, b)$ will be expressed as a linear combination of $a$ and $b$ as desired. This process will be much easier to see with examples: Find integers $x$ and $y$ such that $$ 135x + 50y = 5 $$ Use Euclidean Algorithm to compute $gcd(135, 50)$ $$ \\begin{aligned} 135 & = 2 \\sdot 50 + 35 \\cdots\\text{\\textcircled 1}\\\\ 50 & = 1 \\sdot 35 + 15 \\cdots\\text{\\textcircled 2}\\\\ 35 & = 2 \\sdot 15 + 5 \\cdots\\text{\\textcircled 3}\\\\ 15 & = 3 \\sdot 5 \\end{aligned} $$ Now, let's use the Extended Euclidean algorithm to solve the problem $ 5 = 35 - 2 \\sdot 15 $ from equation 3 But, we have that $ 15 = 50 - 35 $ from euqation $\\text{\\textcircled 2}$ Now we, substitute this value into the previously derived equation: $$ \\begin{aligned} 5 & = 35 - 2 \\sdot 15 \\\\ 5 & = 35 - 2 \\sdot (50 - 35) \\\\ 5 & = 3 \\sdot 35 - 2 \\sdot 50 \\end{aligned} $$ Now, finally use the first equation to determine an expression for $35$ as a linear combination of $135$ and $50$ $$ 35 = 135 - 2 \\sdot 50 \\text{ from equation}\\text{\\textcircled 1} $$ Plug this into our last euqation: $$ \\begin{aligned} 5 & = 3 \\sdot 35 - 2 \\sdot 50 \\\\ 5 & = 3 \\sdot (135 - 2 \\sdot 50) - 2 \\sdot 50 \\\\ 5 & = 3 \\sdot 135 - 8 \\sdot 50 \\end{aligned} $$ So, a set of solutions to the equation is $x=3, y = -8$","title":"Proof"},{"location":"Algorithms/EuclideanAlgorithm/#this_article_is_from_cot3100euclid01","text":"","title":"This article is from 'COT3100Euclid01'"},{"location":"Algorithms/FastPower/","text":"Fast Power Algorithm \u00b6 FAST! time complexity $\\Omicron(pow)$ Implementation \u00b6 Recursive \u00b6 1 2 3 4 5 6 7 int power ( int base , int pow ) { if ( ! pow ) return 1 ; if ( n & 1 ) // if n is odd return base * power ( base , ( pow - 1 ) >> 1 ) * power ( base , ( pow - 1 ) >> 1 ); return power ( base , pow >> 1 ) * power ( base , pow >> 1 ); } Iterative \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 int power ( int base , int pow ) { if ( ! pow ) return 1 ; int result = 1 ; while ( pow ) { if ( pow & 1 ) { result *= base ; pow -- ; // not necessary } pow >>= 1 ; base *= base ; } return result ; } How it works? \u00b6","title":"FastPower"},{"location":"Algorithms/FastPower/#fast_power_algorithm","text":"FAST! time complexity $\\Omicron(pow)$","title":"Fast Power Algorithm"},{"location":"Algorithms/FastPower/#implementation","text":"","title":"Implementation"},{"location":"Algorithms/FastPower/#recursive","text":"1 2 3 4 5 6 7 int power ( int base , int pow ) { if ( ! pow ) return 1 ; if ( n & 1 ) // if n is odd return base * power ( base , ( pow - 1 ) >> 1 ) * power ( base , ( pow - 1 ) >> 1 ); return power ( base , pow >> 1 ) * power ( base , pow >> 1 ); }","title":"Recursive"},{"location":"Algorithms/FastPower/#iterative","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 int power ( int base , int pow ) { if ( ! pow ) return 1 ; int result = 1 ; while ( pow ) { if ( pow & 1 ) { result *= base ; pow -- ; // not necessary } pow >>= 1 ; base *= base ; } return result ; }","title":"Iterative"},{"location":"Algorithms/FastPower/#how_it_works","text":"","title":"How it works?"},{"location":"Algorithms/Graph/Basics/","text":"Graph \u00b6 A graph is consists of nodes and edges Path: A path leads from node a to node b through edges of the graph. Length: The length of a path is the number of edges in it. Cycle: A path is a cycle if the first and last node is the same. Simple: A path is simple if each node appears at most once in the path. Terminologies \u00b6 1. Connectivity \u00b6 A graph is connected if there is a path between any two nodes. Components: Connected parts of its grpah. Tree: a tree is a connected graph that consists of $n$ nodes and $n-1$ edges. 2. Edge directions \u00b6 A graph is directed if the edges can be traversed in one direction only 3. Edge weights \u00b6 In a weighted graph, each edge is assigned a weight. often interpreted as edge length 4. Neighbors and degrees \u00b6 Two nodes are $neighbors$ or $adjacent$ if there is an edge between them Degree: The degrege of a node is number of its neighbors Indegree: The indegree of a node is the number of edges that end at the node Outdegree: The outdegree of a node is the number of edges that start at the node Regular graph: A graph is regular if the degree of every node is a constant d Complete graph: A graph is complete if the degree of every node is $n-1$ Colorings \u00b6 In a coloring of a graph, each node is assigned a color so that no adjacent nodes have the same color Note It turns out that a graph is bipartite exactly when it does not contain a cycle with an odd number of edges Simplicity \u00b6 A graph is simple if no edge starts and ends at the same node(loop), and there are no multiple edges between two nodes. Graph representation \u00b6 There are several ways to represent graphs in algorithms. The choice of a data structure depends on the size of graph and the way the algorithm processes it 1. Adjacency list representation \u00b6 In the adjacency list representation, each node x in the graph is assigned an adjacency list that consists of nodes to which there is an edge from x we can efficiently find the nodes to which we can move from a given node through an edge 2. Adjacency matrix representation \u00b6 An adjacency matrix is two dimensional array that indicates which edges the graph contains. we can efficiently check from an adjacency matrix if there is an edge between two nodes. 3. Edge list representation \u00b6 An edge list contains all edges of a graph in some order. This is convenient way to represent a graph if the algorithm proesses all edges of the graph and it is not needed to find edges that start at a given node.","title":"Basics"},{"location":"Algorithms/Graph/Basics/#graph","text":"A graph is consists of nodes and edges Path: A path leads from node a to node b through edges of the graph. Length: The length of a path is the number of edges in it. Cycle: A path is a cycle if the first and last node is the same. Simple: A path is simple if each node appears at most once in the path.","title":"Graph"},{"location":"Algorithms/Graph/Basics/#terminologies","text":"","title":"Terminologies"},{"location":"Algorithms/Graph/Basics/#1_connectivity","text":"A graph is connected if there is a path between any two nodes. Components: Connected parts of its grpah. Tree: a tree is a connected graph that consists of $n$ nodes and $n-1$ edges.","title":"1. Connectivity"},{"location":"Algorithms/Graph/Basics/#2_edge_directions","text":"A graph is directed if the edges can be traversed in one direction only","title":"2. Edge directions"},{"location":"Algorithms/Graph/Basics/#3_edge_weights","text":"In a weighted graph, each edge is assigned a weight. often interpreted as edge length","title":"3. Edge weights"},{"location":"Algorithms/Graph/Basics/#4_neighbors_and_degrees","text":"Two nodes are $neighbors$ or $adjacent$ if there is an edge between them Degree: The degrege of a node is number of its neighbors Indegree: The indegree of a node is the number of edges that end at the node Outdegree: The outdegree of a node is the number of edges that start at the node Regular graph: A graph is regular if the degree of every node is a constant d Complete graph: A graph is complete if the degree of every node is $n-1$","title":"4. Neighbors and degrees"},{"location":"Algorithms/Graph/Basics/#colorings","text":"In a coloring of a graph, each node is assigned a color so that no adjacent nodes have the same color Note It turns out that a graph is bipartite exactly when it does not contain a cycle with an odd number of edges","title":"Colorings"},{"location":"Algorithms/Graph/Basics/#simplicity","text":"A graph is simple if no edge starts and ends at the same node(loop), and there are no multiple edges between two nodes.","title":"Simplicity"},{"location":"Algorithms/Graph/Basics/#graph_representation","text":"There are several ways to represent graphs in algorithms. The choice of a data structure depends on the size of graph and the way the algorithm processes it","title":"Graph representation"},{"location":"Algorithms/Graph/Basics/#1_adjacency_list_representation","text":"In the adjacency list representation, each node x in the graph is assigned an adjacency list that consists of nodes to which there is an edge from x we can efficiently find the nodes to which we can move from a given node through an edge","title":"1. Adjacency list representation"},{"location":"Algorithms/Graph/Basics/#2_adjacency_matrix_representation","text":"An adjacency matrix is two dimensional array that indicates which edges the graph contains. we can efficiently check from an adjacency matrix if there is an edge between two nodes.","title":"2. Adjacency matrix representation"},{"location":"Algorithms/Graph/Basics/#3_edge_list_representation","text":"An edge list contains all edges of a graph in some order. This is convenient way to represent a graph if the algorithm proesses all edges of the graph and it is not needed to find edges that start at a given node.","title":"3. Edge list representation"},{"location":"Algorithms/Graph/GraphTraversal/","text":"Graph Traversal \u00b6 We will cover two fundamental graph algorithms. depth-first search & breadth-first search. BFS vs DFS Both algorithms are given starting node in the graph and they visit all nodes that can be reached from the starting node. The difference in algorithms is the order in which they visit the nodes. Depth-first search($\\text{DFS}$) \u00b6 Depth-first search always follows a single path in the graph as long as it find new nodes. After this, it returns to previous nodes and begin to explore other parts of the graph. The algorithm keeps track of visited nodes, so that it processes each node only once Implementation \u00b6 Using adjacency lists in an array maintain an array visited[N] 1 2 3 4 5 6 7 8 9 10 11 vector < int > adj [ N ]; //adjacency lists bool visited [ N ]; void dfs ( int s ) { //starting node s if ( visited [ s ]) return ; visited [ s ] = true ; // process node for ( auto u : adj [ s ]) { dfs ( u ); } } Breadth-first search($\\text{BFS}$) \u00b6 Breadth-first search visits the nodes in increasing order of their distance from the starting node. Thus, we can calculate the distance from the starting node to all other nodes using $BFS$. Implementation \u00b6 Typical implementation is based on a queue that contains nodes. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 queue < int > q ; bool visited [ N ]; int distance [ N ]; visited [ s ] = true ; // starting node s distance [ s ] = 0 ; q . push ( s ); while ( ! q . empty ()) { int cur = q . front (); q . pop (); // process node for ( auto next : adj [ cur ]) { if ( visited [ next ]) continue ; visited [ next ] = true ; distance [ next ] = distance [ cur ] + 1 ; q . push ( next ); } } Applications \u00b6 You can use any of both to check properties of graph but in practice, depth-first search is a better choice, because of ease of implementation 1. Connectivity check \u00b6 A graph is connected if there is a path between any two nodes of the graph Implementation \u00b6 Connected Graph: If a search did not visit all the nodes, we can conclude that the graph is not connected Find all components of Graph: iterating through the nodes and always starting a new depth-first search if the current node does not belong to any component yet 2. Finding cycles \u00b6 Implementation \u00b6 Way1: A graph contains a cycle if during a graph traversal, we find a node whose neighbor (other than the previous node in the current path) has already been visited Way2(math): if a component contains c nodes and no cycle, it must contain exactly c-1 edges. if there are c or more edges, the component surely contains a cycle 3. Bipartiteness check \u00b6 A graph is bipartite if its nodes can be colored using two colors so that there are no adjacent nodes with the same color Implementation \u00b6 The idea is to color the starting node blue, all its neighbors red, all their neighbors blue, and so on. If at some point of the search we notice that two adjacent nodes have the same color, this means that the graph is not bipartite. Otherwise, the graph is bipartite. Why it works? This algorithm always works, because when there are only two colors available, the color of the starting node in a component determines the colors of all other nodes in the component NP-hard Note that in the general case, it is difficult to find out if the nodes in a graph can be colored using $k$ colors so that no adjacent nodes have the same color. Even when $k=3$, no efficient algorithm is known but the problem is NP-hard","title":"Graph Traversal"},{"location":"Algorithms/Graph/GraphTraversal/#graph_traversal","text":"We will cover two fundamental graph algorithms. depth-first search & breadth-first search. BFS vs DFS Both algorithms are given starting node in the graph and they visit all nodes that can be reached from the starting node. The difference in algorithms is the order in which they visit the nodes.","title":"Graph Traversal"},{"location":"Algorithms/Graph/GraphTraversal/#depth-first_searchtextdfs","text":"Depth-first search always follows a single path in the graph as long as it find new nodes. After this, it returns to previous nodes and begin to explore other parts of the graph. The algorithm keeps track of visited nodes, so that it processes each node only once","title":"Depth-first search($\\text{DFS}$)"},{"location":"Algorithms/Graph/GraphTraversal/#implementation","text":"Using adjacency lists in an array maintain an array visited[N] 1 2 3 4 5 6 7 8 9 10 11 vector < int > adj [ N ]; //adjacency lists bool visited [ N ]; void dfs ( int s ) { //starting node s if ( visited [ s ]) return ; visited [ s ] = true ; // process node for ( auto u : adj [ s ]) { dfs ( u ); } }","title":"Implementation"},{"location":"Algorithms/Graph/GraphTraversal/#breadth-first_searchtextbfs","text":"Breadth-first search visits the nodes in increasing order of their distance from the starting node. Thus, we can calculate the distance from the starting node to all other nodes using $BFS$.","title":"Breadth-first search($\\text{BFS}$)"},{"location":"Algorithms/Graph/GraphTraversal/#implementation_1","text":"Typical implementation is based on a queue that contains nodes. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 queue < int > q ; bool visited [ N ]; int distance [ N ]; visited [ s ] = true ; // starting node s distance [ s ] = 0 ; q . push ( s ); while ( ! q . empty ()) { int cur = q . front (); q . pop (); // process node for ( auto next : adj [ cur ]) { if ( visited [ next ]) continue ; visited [ next ] = true ; distance [ next ] = distance [ cur ] + 1 ; q . push ( next ); } }","title":"Implementation"},{"location":"Algorithms/Graph/GraphTraversal/#applications","text":"You can use any of both to check properties of graph but in practice, depth-first search is a better choice, because of ease of implementation","title":"Applications"},{"location":"Algorithms/Graph/GraphTraversal/#1_connectivity_check","text":"A graph is connected if there is a path between any two nodes of the graph","title":"1. Connectivity check"},{"location":"Algorithms/Graph/GraphTraversal/#implementation_2","text":"Connected Graph: If a search did not visit all the nodes, we can conclude that the graph is not connected Find all components of Graph: iterating through the nodes and always starting a new depth-first search if the current node does not belong to any component yet","title":"Implementation"},{"location":"Algorithms/Graph/GraphTraversal/#2_finding_cycles","text":"","title":"2. Finding cycles"},{"location":"Algorithms/Graph/GraphTraversal/#implementation_3","text":"Way1: A graph contains a cycle if during a graph traversal, we find a node whose neighbor (other than the previous node in the current path) has already been visited Way2(math): if a component contains c nodes and no cycle, it must contain exactly c-1 edges. if there are c or more edges, the component surely contains a cycle","title":"Implementation"},{"location":"Algorithms/Graph/GraphTraversal/#3_bipartiteness_check","text":"A graph is bipartite if its nodes can be colored using two colors so that there are no adjacent nodes with the same color","title":"3. Bipartiteness check"},{"location":"Algorithms/Graph/GraphTraversal/#implementation_4","text":"The idea is to color the starting node blue, all its neighbors red, all their neighbors blue, and so on. If at some point of the search we notice that two adjacent nodes have the same color, this means that the graph is not bipartite. Otherwise, the graph is bipartite. Why it works? This algorithm always works, because when there are only two colors available, the color of the starting node in a component determines the colors of all other nodes in the component NP-hard Note that in the general case, it is difficult to find out if the nodes in a graph can be colored using $k$ colors so that no adjacent nodes have the same color. Even when $k=3$, no efficient algorithm is known but the problem is NP-hard","title":"Implementation"},{"location":"Algorithms/Graph/DFS/FindingCutEdges/","text":"Finding Cut Edges \u00b6 The code below works properly because of the lemma above(first lemma) 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 h [ root ] = 0 par [ v ] = - 1 dfs ( v ): d [ v ] = h [ v ] color [ v ] = gray for u in adj [ v ]: if color [ u ] == white : then par [ u ] = v and dfs ( u ) and d [ v ] = min ( d [ v ], d [ u ]) if d [ u ] > h [ v ] then the edge v - u is a cut edge else if u != par [ v ]: then d [ v ] = min ( d [ v ], h [ u ]) color [ v ] = black in this code, h[v] = height of vertex v in the DFS tree and d[v] = min(h[w] where there is at least vertex u in subtree of v in the DFS tree where there is an edge between $u$ and $w$) First lemma will be placed here \u21a9","title":"FindingCutEdges"},{"location":"Algorithms/Graph/DFS/FindingCutEdges/#finding_cut_edges","text":"The code below works properly because of the lemma above(first lemma) 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 h [ root ] = 0 par [ v ] = - 1 dfs ( v ): d [ v ] = h [ v ] color [ v ] = gray for u in adj [ v ]: if color [ u ] == white : then par [ u ] = v and dfs ( u ) and d [ v ] = min ( d [ v ], d [ u ]) if d [ u ] > h [ v ] then the edge v - u is a cut edge else if u != par [ v ]: then d [ v ] = min ( d [ v ], h [ u ]) color [ v ] = black in this code, h[v] = height of vertex v in the DFS tree and d[v] = min(h[w] where there is at least vertex u in subtree of v in the DFS tree where there is an edge between $u$ and $w$) First lemma will be placed here \u21a9","title":"Finding Cut Edges"},{"location":"Algorithms/Graph/DFS/Preface/","text":"DFS \u00b6 The most useful graph algorithms are search algorithms. DFS(Depth First Search) is one of them. While running DFS, we assign colors to the vertices (initially white) Algorithm itself is really simple 1 2 3 4 5 6 dfs ( v ): color [ v ] = gray for u in adj [ v ]: if color [ u ] == white : then dfs ( u ) color [ v ] = black Black color here is not used, but you can use it sometimes. Time complexity: $O(n + m)$","title":"Preface"},{"location":"Algorithms/Graph/DFS/Preface/#dfs","text":"The most useful graph algorithms are search algorithms. DFS(Depth First Search) is one of them. While running DFS, we assign colors to the vertices (initially white) Algorithm itself is really simple 1 2 3 4 5 6 dfs ( v ): color [ v ] = gray for u in adj [ v ]: if color [ u ] == white : then dfs ( u ) color [ v ] = black Black color here is not used, but you can use it sometimes. Time complexity: $O(n + m)$","title":"DFS"},{"location":"Algorithms/Graph/DFS/StartingFinishingTime/","text":"Starting time, finishing time \u00b6 Starting time of a vertex is the time we enter it (the order we enter it) and its finishing time is the time we leave it. Calculating these are easy 1 2 3 4 5 6 7 8 9 TIME = 0 dfs ( v ): st [ v ] = TIME ++ color [ v ] = gray for u in adj [ v ]: if color [ u ] == white : then dfs ( u ) color [ v ] = black ft [ v ] = TIME # or we can use TIME ++ It is useable in specially data structure problems (convert the tree into an array). Lemma-1 : if we run $dfs(root)$ in a rooted tree, then v is an ancestor of $u$ if and only if $st_v\\leq st_u\\leq ft_u\\leq ft_v$. So, given arrays $st$ and $ft$ we can rebuild the tree.","title":"StartingFinishingTime"},{"location":"Algorithms/Graph/DFS/StartingFinishingTime/#starting_time_finishing_time","text":"Starting time of a vertex is the time we enter it (the order we enter it) and its finishing time is the time we leave it. Calculating these are easy 1 2 3 4 5 6 7 8 9 TIME = 0 dfs ( v ): st [ v ] = TIME ++ color [ v ] = gray for u in adj [ v ]: if color [ u ] == white : then dfs ( u ) color [ v ] = black ft [ v ] = TIME # or we can use TIME ++ It is useable in specially data structure problems (convert the tree into an array). Lemma-1 : if we run $dfs(root)$ in a rooted tree, then v is an ancestor of $u$ if and only if $st_v\\leq st_u\\leq ft_u\\leq ft_v$. So, given arrays $st$ and $ft$ we can rebuild the tree.","title":"Starting time, finishing time"},{"location":"Algorithms/Graph/DFS/Tree/","text":"DFS tree \u00b6 DFS Tree is a rooted tree that is built like this 1 2 3 4 5 6 7 let T be a new tree dfs ( v ): color [ v ] = gray for u in adj [ v ]: if color [ u ] == white : then dfs ( u ) and par [ u ] = v ( in T ) color [ v ] = black Lemma : There is no cross edges, it means if there is an edge between $V$ and $u$, then $v=par[u]$ or $u=par[v]$","title":"DFSTree"},{"location":"Algorithms/Graph/DFS/Tree/#dfs_tree","text":"DFS Tree is a rooted tree that is built like this 1 2 3 4 5 6 7 let T be a new tree dfs ( v ): color [ v ] = gray for u in adj [ v ]: if color [ u ] == white : then dfs ( u ) and par [ u ] = v ( in T ) color [ v ] = black Lemma : There is no cross edges, it means if there is an edge between $V$ and $u$, then $v=par[u]$ or $u=par[v]$","title":"DFS tree"},{"location":"Algorithms/Graph/ShortestPaths/Bellman-Ford/","text":"Bellman-Ford Algorithm \u00b6 The Bellman-Ford algorithm finds shortest paths from a starting node to all nodes of the graph. The algorithm reduces the distance by finding edges that shorten the paths until it is not possible to reduce any distances. Bellman-Ford can process all kinds of graphs The algorithm can process all kinds of graphs, provided that the graph does not contain a cycle with negative length. If the graph contains a negative cycle, the algorithm can detect this. Implementation \u00b6 Assume that the graph is stored as an edge list edge that consists of tuples of the form$(a, b, w)$, meaing that there is an edge from node $a$ to node $b$ with weight $w$. The algorithm consists of $n-1$ rounds, and on each round the round the algorithm goes through all edges of the graph and tries to reduce the distances. The algorithm constructs an array $\\text{distance}$ that will contain the distance from x to all nodes of the graph. The constant INF denotes an infinite distance. $n = \\text{number of vertices(nodes)}$, $m = \\text{number of edges}$ 1 2 3 4 5 6 7 8 9 10 11 12 int const INF = 2e9 ; tuple < int , int , int > edges [ m ]; //edge list for ( int i = 1 ; i <= n ; i ++ ) distance [ i ] = INF ; distance [ s ] = 0 ; // starting node s for ( int i = 1 ; i <= n - 1 ; i ++ ) { for ( auto e : edges ) { int a , b , w ; tie ( a , b , w ) = e ; distance [ b ] = min ( distance [ b ], distance [ a ] + w ); } } Time Complexity \u00b6 $\\Omicron(nm)$ Negative Cycles \u00b6 The algorithm can also be used to check if the graph contains a cycle with negative length. A negative cycle can be detected using the Bellman-Ford algorithm by running the algorithm for $n$ rounds If the n-th round reduces any distance, the graph contains a negative cycle. Negative cycle in the whole graph Note that this algorithm can be used to search for a negative cycle in the whole graph regardless of the starting node","title":"Bellman-Ford"},{"location":"Algorithms/Graph/ShortestPaths/Bellman-Ford/#bellman-ford_algorithm","text":"The Bellman-Ford algorithm finds shortest paths from a starting node to all nodes of the graph. The algorithm reduces the distance by finding edges that shorten the paths until it is not possible to reduce any distances. Bellman-Ford can process all kinds of graphs The algorithm can process all kinds of graphs, provided that the graph does not contain a cycle with negative length. If the graph contains a negative cycle, the algorithm can detect this.","title":"Bellman-Ford Algorithm"},{"location":"Algorithms/Graph/ShortestPaths/Bellman-Ford/#implementation","text":"Assume that the graph is stored as an edge list edge that consists of tuples of the form$(a, b, w)$, meaing that there is an edge from node $a$ to node $b$ with weight $w$. The algorithm consists of $n-1$ rounds, and on each round the round the algorithm goes through all edges of the graph and tries to reduce the distances. The algorithm constructs an array $\\text{distance}$ that will contain the distance from x to all nodes of the graph. The constant INF denotes an infinite distance. $n = \\text{number of vertices(nodes)}$, $m = \\text{number of edges}$ 1 2 3 4 5 6 7 8 9 10 11 12 int const INF = 2e9 ; tuple < int , int , int > edges [ m ]; //edge list for ( int i = 1 ; i <= n ; i ++ ) distance [ i ] = INF ; distance [ s ] = 0 ; // starting node s for ( int i = 1 ; i <= n - 1 ; i ++ ) { for ( auto e : edges ) { int a , b , w ; tie ( a , b , w ) = e ; distance [ b ] = min ( distance [ b ], distance [ a ] + w ); } }","title":"Implementation"},{"location":"Algorithms/Graph/ShortestPaths/Bellman-Ford/#time_complexity","text":"$\\Omicron(nm)$","title":"Time Complexity"},{"location":"Algorithms/Graph/ShortestPaths/Bellman-Ford/#negative_cycles","text":"The algorithm can also be used to check if the graph contains a cycle with negative length. A negative cycle can be detected using the Bellman-Ford algorithm by running the algorithm for $n$ rounds If the n-th round reduces any distance, the graph contains a negative cycle. Negative cycle in the whole graph Note that this algorithm can be used to search for a negative cycle in the whole graph regardless of the starting node","title":"Negative Cycles"},{"location":"Algorithms/Graph/ShortestPaths/Dijkstra/","text":"Dijkstra's Algorithm \u00b6 The algorithm finds shortest paths from the starting node to all nodes of the graph, Like Bellman-Ford algorithm . The Benefit of Dijkstra's algorithm is that it is more efficent and can be used for processing large graphs. Dijkstra's algorithm is efficient, because it only process each edge in the graph once, using the fact that there are no negative edges. Negative edges Dijkstra's algorithm requires that there are no negative weight edges in the graph Implementation \u00b6 Assume that the graph is stored as an adjacency lists so that $adj[a]$ contains a pair $(b, w)$ always when there is an edge from node $a$ to node $b$ with weight $w$ Use priority queue that contains nodes the nodes ordered by their distances. Using priority queue, the next node to be processed can be retrieved in logarithmic time 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 int distance []; bool processed []; priority_queue < int , int > q ; // (-dis, node) for ( int i = 1 ; i <= n ; i ++ ) distance [ i ] = INF ; distance [ s ] = 0 ; // starting node s q . push ({ 0 , x }); while ( ! q . empty ()) { int a = q . top (). second ; q . pop (); if ( processed [ a ]) continue ; processed [ a ] = true ; for ( auto u : adj [ a ]) { int b = u . first , w = u . second ; if ( distance [ a ] + w < distance [ b ]) { distance [ b ] = distance [ a ] + w ; q . push ( - distance [ b ], b ); } } } Time Complexity \u00b6 Not yet","title":"Dijkstra"},{"location":"Algorithms/Graph/ShortestPaths/Dijkstra/#dijkstras_algorithm","text":"The algorithm finds shortest paths from the starting node to all nodes of the graph, Like Bellman-Ford algorithm . The Benefit of Dijkstra's algorithm is that it is more efficent and can be used for processing large graphs. Dijkstra's algorithm is efficient, because it only process each edge in the graph once, using the fact that there are no negative edges. Negative edges Dijkstra's algorithm requires that there are no negative weight edges in the graph","title":"Dijkstra's Algorithm"},{"location":"Algorithms/Graph/ShortestPaths/Dijkstra/#implementation","text":"Assume that the graph is stored as an adjacency lists so that $adj[a]$ contains a pair $(b, w)$ always when there is an edge from node $a$ to node $b$ with weight $w$ Use priority queue that contains nodes the nodes ordered by their distances. Using priority queue, the next node to be processed can be retrieved in logarithmic time 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 int distance []; bool processed []; priority_queue < int , int > q ; // (-dis, node) for ( int i = 1 ; i <= n ; i ++ ) distance [ i ] = INF ; distance [ s ] = 0 ; // starting node s q . push ({ 0 , x }); while ( ! q . empty ()) { int a = q . top (). second ; q . pop (); if ( processed [ a ]) continue ; processed [ a ] = true ; for ( auto u : adj [ a ]) { int b = u . first , w = u . second ; if ( distance [ a ] + w < distance [ b ]) { distance [ b ] = distance [ a ] + w ; q . push ( - distance [ b ], b ); } } }","title":"Implementation"},{"location":"Algorithms/Graph/ShortestPaths/Dijkstra/#time_complexity","text":"Not yet","title":"Time Complexity"},{"location":"Algorithms/Graph/ShortestPaths/Floyd-Warshall/","text":"Floyd-Warshall Algorithm \u00b6 Floyd-Warshall algorithm provides an alternative way to approach th problem of finding shortest paths. Floyd-Warhsall algorithm finds all shortests paths between the nodes in a single run. Floyd-Warhsall algorithm maintains a two-dimensional array that contains distances between the nodes. Floyd-warshall algorithm is easy to implement. Floyd-warhsall algorithm reduces distance by intermediate nodes in paths. Implementation \u00b6 Assume using adjacency matrix 1 2 3 4 5 6 7 8 9 10 11 12 // first build distance (2-dimensional array) int const INF = 2e9 ; int adj [][]; int distance [][]; for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 1 ; j <= n ; j ++ ) { if ( i == j ) distance [ i ][ j ] = 0 ; else if ( adj [ i ][ j ]) distance [ i ][ j ] = adj [ i ][ j ]; else distance [ i ][ j ] = INF ; } } 1 2 3 4 5 6 7 8 9 // process for ( int k = 1 ; k <= n ; k ++ ) { // k == intermediate node for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 1 ; j <= n ; j ++ ) { distance [ i ][ j ] = min ( distance [ i ][ j ], distance [ i ][ k ] + distance [ k ][ j ]) } } } Time Complexity \u00b6 $\\Omicron(n^3)$ Related Problems \u00b6 K-th Path","title":"Floyd-Warshall"},{"location":"Algorithms/Graph/ShortestPaths/Floyd-Warshall/#floyd-warshall_algorithm","text":"Floyd-Warshall algorithm provides an alternative way to approach th problem of finding shortest paths. Floyd-Warhsall algorithm finds all shortests paths between the nodes in a single run. Floyd-Warhsall algorithm maintains a two-dimensional array that contains distances between the nodes. Floyd-warshall algorithm is easy to implement. Floyd-warhsall algorithm reduces distance by intermediate nodes in paths.","title":"Floyd-Warshall Algorithm"},{"location":"Algorithms/Graph/ShortestPaths/Floyd-Warshall/#implementation","text":"Assume using adjacency matrix 1 2 3 4 5 6 7 8 9 10 11 12 // first build distance (2-dimensional array) int const INF = 2e9 ; int adj [][]; int distance [][]; for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 1 ; j <= n ; j ++ ) { if ( i == j ) distance [ i ][ j ] = 0 ; else if ( adj [ i ][ j ]) distance [ i ][ j ] = adj [ i ][ j ]; else distance [ i ][ j ] = INF ; } } 1 2 3 4 5 6 7 8 9 // process for ( int k = 1 ; k <= n ; k ++ ) { // k == intermediate node for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 1 ; j <= n ; j ++ ) { distance [ i ][ j ] = min ( distance [ i ][ j ], distance [ i ][ k ] + distance [ k ][ j ]) } } }","title":"Implementation"},{"location":"Algorithms/Graph/ShortestPaths/Floyd-Warshall/#time_complexity","text":"$\\Omicron(n^3)$","title":"Time Complexity"},{"location":"Algorithms/Graph/ShortestPaths/Floyd-Warshall/#related_problems","text":"K-th Path","title":"Related Problems"},{"location":"Algorithms/Graph/ShortestPaths/Preface/","text":"Shortest Paths \u00b6 Finding a shortest path between two nodes of a graph is an important problem that has many practical applications. In an uweighted graph, the length of a path equals the number of its edges, and we can simply use breath-first search to find a shortest path. However, in this chapter we focus on weighted graphs where more sophisticated algorithms are needed for shortest paths. Diff \u00b6 $n = \\text{number of nodes}$, $m = \\text{number of edges}$ TimeComplexity DS - Bellman-Ford $\\Omicron(nm)$ edge list neg-cycle detection SPFA $\\Omicron(nm)$ Dijkstra $\\Omicron(n + m\\lg(m))$ adjacency lists no neg edges Floyd-Warshall $\\Omicron(n^3)$ adjacency matrix finds all shortest paths between the nodes","title":"Preface"},{"location":"Algorithms/Graph/ShortestPaths/Preface/#shortest_paths","text":"Finding a shortest path between two nodes of a graph is an important problem that has many practical applications. In an uweighted graph, the length of a path equals the number of its edges, and we can simply use breath-first search to find a shortest path. However, in this chapter we focus on weighted graphs where more sophisticated algorithms are needed for shortest paths.","title":"Shortest Paths"},{"location":"Algorithms/Graph/ShortestPaths/Preface/#diff","text":"$n = \\text{number of nodes}$, $m = \\text{number of edges}$ TimeComplexity DS - Bellman-Ford $\\Omicron(nm)$ edge list neg-cycle detection SPFA $\\Omicron(nm)$ Dijkstra $\\Omicron(n + m\\lg(m))$ adjacency lists no neg edges Floyd-Warshall $\\Omicron(n^3)$ adjacency matrix finds all shortest paths between the nodes","title":"Diff"},{"location":"Algorithms/Sort/BubbleSort/","text":"Bubble Sort \u00b6 Bubble Sort C++ \u00b6 1 2 3 4 5 6 7 8 9 template < typename It > void BubbleSort ( It begin , It end ) { if ( begin == end ) return ; //return if container is empty for ( It i = end - 1 ; i != begin ; i -- ) { for ( It j = begin ; j != i ; j ++ ) { if ( * j > * ( j + 1 )) swap ( * j , * ( j + 1 )); } } }","title":"BubbleSort"},{"location":"Algorithms/Sort/BubbleSort/#bubble_sort","text":"Bubble Sort","title":"Bubble Sort"},{"location":"Algorithms/Sort/BubbleSort/#c","text":"1 2 3 4 5 6 7 8 9 template < typename It > void BubbleSort ( It begin , It end ) { if ( begin == end ) return ; //return if container is empty for ( It i = end - 1 ; i != begin ; i -- ) { for ( It j = begin ; j != i ; j ++ ) { if ( * j > * ( j + 1 )) swap ( * j , * ( j + 1 )); } } }","title":"C++"},{"location":"Algorithms/Sort/InsertionSort/","text":"Insertion Sort \u00b6 Insertion Sort is very simple algorithm it works exactly like the way you sort a deck of card C++ \u00b6 1 2 3 4 5 6 7 8 9 10 template < typenme It > // Iterator void insertionSort ( It begin , It end ) { //TODO add comparator if ( begin == end ) return ; // return if container is empty for ( It i = begin ; i != end ; i ++ ) { for ( It j = i ; j != begin ; j -- ) { if ( * ( j - 1 ) < * j ) break ; swap ( * ( j - 1 ), * j ); } } }","title":"InsertionSort"},{"location":"Algorithms/Sort/InsertionSort/#insertion_sort","text":"Insertion Sort is very simple algorithm it works exactly like the way you sort a deck of card","title":"Insertion Sort"},{"location":"Algorithms/Sort/InsertionSort/#c","text":"1 2 3 4 5 6 7 8 9 10 template < typenme It > // Iterator void insertionSort ( It begin , It end ) { //TODO add comparator if ( begin == end ) return ; // return if container is empty for ( It i = begin ; i != end ; i ++ ) { for ( It j = i ; j != begin ; j -- ) { if ( * ( j - 1 ) < * j ) break ; swap ( * ( j - 1 ), * j ); } } }","title":"C++"},{"location":"Algorithms/Tree/Basics/","text":"Tree \u00b6 A tree is a connected, acyclic graph that consist of $n$ nodes and $n-1$ edges. Removing any edges from a tree divides it into two components , and adding any edge to a tree creates a cycle . Moreover, there is always a unique path between any two nodes of a tree. Leaves \u00b6 leaves of a tree are the nodes with degree 1, i.e., with only one neighbor Rooted tree \u00b6 In a rooted tree, one of the nodes is appointed the root of the tree, and all other nodes are placed underneath the root. Children & Parent of a node \u00b6 In a rooted tree, the children of a node are its lower negibors, and the parent of a node is its upper neighbor. Recursive structure \u00b6 The structure of rooted tree is recursive each node of the tree ats as the root of subtree that contains the node itself and all nodes that are in the subtrees of its children","title":"Basics"},{"location":"Algorithms/Tree/Basics/#tree","text":"A tree is a connected, acyclic graph that consist of $n$ nodes and $n-1$ edges. Removing any edges from a tree divides it into two components , and adding any edge to a tree creates a cycle . Moreover, there is always a unique path between any two nodes of a tree.","title":"Tree"},{"location":"Algorithms/Tree/Basics/#leaves","text":"leaves of a tree are the nodes with degree 1, i.e., with only one neighbor","title":"Leaves"},{"location":"Algorithms/Tree/Basics/#rooted_tree","text":"In a rooted tree, one of the nodes is appointed the root of the tree, and all other nodes are placed underneath the root.","title":"Rooted tree"},{"location":"Algorithms/Tree/Basics/#children_parent_of_a_node","text":"In a rooted tree, the children of a node are its lower negibors, and the parent of a node is its upper neighbor.","title":"Children &amp; Parent of a node"},{"location":"Algorithms/Tree/Basics/#recursive_structure","text":"The structure of rooted tree is recursive each node of the tree ats as the root of subtree that contains the node itself and all nodes that are in the subtrees of its children","title":"Recursive structure"},{"location":"Algorithms/Tree/DP/","text":"Tree Dynamic Programming \u00b6 Dynamic programming can be used to calculate some information during a tree traversal. Time Complexities \u00b6 Not yet added The number of nodes in its subtree \u00b6 The subtree contains the node itself and all nodes in the subtrees of its children. so we can calculate the number of nodes recursively using the following code Time complexity: $\\Omicron(n)$ 1 2 3 4 5 6 7 8 9 10 int count [ # nodes ]; void dfs ( int s , int e ) { //current node s, previous node e; count [ s ] = 1 ; for ( auto u : adj [ s ]) { if ( u == e ) continue ; dfs ( u , s ); count [ s ] += count [ u ]; } } Diameter \u00b6 The Diameter of a tree is the maximum length of a path between two nodes. Algorithm 1 (based on DP) \u00b6 A general way to approach many tree problems is to first root the tree arbitrarily . After this, we can try to solve the problem separately for each subtree. Our first algorithm for calculating the diameter is based on this idea. An important observation is that every path in a rooted tree has a highest point : the highest node that belongs to the path. Thus we can calculate for each node the length of the longest path whose heighest point is the node. One of those path corresponds to the diameter of the tree. We calculate for each node $x$ two values: - toLeaf(x): the maximum length of a path from x to any leaf - maxLength(x): the maximum length of a path whose highest point is $x$ $f(x)$: Longest path starts from node $x$ and goes into its subtree. $g(x)$: Longest path starts in subtree of $x$, passes through $x$ and ends in subtree of $x$ If for all nodes $x$, we take maximum of $f(x), g(x)$, then we can get the diameter. Dynamic programming can be used to calculate the above values for all nodes in $\\Omicron(n)$ time. Implementation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 //adjacency list //adj[i] contains all neighbors of i vector < int > adj [ N ]; //functions as defined above int f [ N ], g [ N ], diameter ; // pV is parent of node V void dfs ( int V , int pV ) { //this vector will store f for all children of V vector < int > fValues ; //traverse over all children for ( auto v : adj [ V ]) { if ( v == pV ) continue ; dfs ( v , V ); fValues . push_back ( f [ v ]); } //sort to get top two values // you can also get top two values without sorting in O(N) // current complexity is n lg n sort ( fValues . begin (), fValues . end ()); f [ V ] = 1 ; if ( ! fValues . empty ()) f [ V ] += fValues . back (); if ( fValues . size () >= 2 ) g [ V ] = 2 + fValues . back () + fValues [ fValues . size () - 2 ]; diameter = max ( diameter , max ( f [ V ], g [ V ])); } More General Implementation \u00b6 with weighted edges 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 //adjacency list //adj[i] contains all neighbors of i, and weights to i -> its neighbor vector < vector < pair < int , int >>> adj [ N ]; int diameter = 0 ; void dfs ( int V , int pV ) { vector < int > fValues ; for ( auto v : adj [ V ]) { if ( v . first == pV ) continue ; dfs ( v . first , V ); fValues . push_back ( f [ v . first ] + v . second ); // fvalue of child + weight of edge; } int a = - 1 , b = - 1 ; // a is biggest, b is second to biggest for ( auto x : fValues ) { if ( x > a ) { b = a ; a = x ; } else if ( x > b ) { b = x ; } } f [ V ] = 0 ; if ( a > 0 ) f [ V ] = a ; if ( a > 0 && b > 0 ) g [ V ] = a + b ; diameter = max ( diameter , max ( f [ V ], g [ V ])); } Algorithm 2 (based on DFS) \u00b6 Another efficient way to calculate the diameter of a tree is based on two depth-first searches. First, we choose an arbitrary node $a$ in the tree and find the farthest node $b$ from $a$. Then, we find the farthest node $c$ from $b$. The diameter of the tree is the distance between $b$ and $c$. How this works? \u00b6 Not yet added implementation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 int firstFar ; int diameter = 0 ; void dfs ( int V , int pV , int dis ) { if ( diameter < dis ) { diameter = dis ; firstFar = V ; } for ( auto v : adj [ V ]) { if ( v . first == pV ) continue ; dfs ( v . first , V , dis + v . second ); // v.first == neighbor node, v.second weight of edge } } dfs ( 0 , 0 , 0 ) //initial call // diameter = 0; // not necessary; dfs ( firstFar , 0 , 0 ); // now diameter is diameter of tree resources \u00b6 https://codeforces.com/blog/entry/20935","title":"DP"},{"location":"Algorithms/Tree/DP/#tree_dynamic_programming","text":"Dynamic programming can be used to calculate some information during a tree traversal.","title":"Tree Dynamic Programming"},{"location":"Algorithms/Tree/DP/#time_complexities","text":"Not yet added","title":"Time Complexities"},{"location":"Algorithms/Tree/DP/#the_number_of_nodes_in_its_subtree","text":"The subtree contains the node itself and all nodes in the subtrees of its children. so we can calculate the number of nodes recursively using the following code Time complexity: $\\Omicron(n)$ 1 2 3 4 5 6 7 8 9 10 int count [ # nodes ]; void dfs ( int s , int e ) { //current node s, previous node e; count [ s ] = 1 ; for ( auto u : adj [ s ]) { if ( u == e ) continue ; dfs ( u , s ); count [ s ] += count [ u ]; } }","title":"The number of nodes in its subtree"},{"location":"Algorithms/Tree/DP/#diameter","text":"The Diameter of a tree is the maximum length of a path between two nodes.","title":"Diameter"},{"location":"Algorithms/Tree/DP/#algorithm_1_based_on_dp","text":"A general way to approach many tree problems is to first root the tree arbitrarily . After this, we can try to solve the problem separately for each subtree. Our first algorithm for calculating the diameter is based on this idea. An important observation is that every path in a rooted tree has a highest point : the highest node that belongs to the path. Thus we can calculate for each node the length of the longest path whose heighest point is the node. One of those path corresponds to the diameter of the tree. We calculate for each node $x$ two values: - toLeaf(x): the maximum length of a path from x to any leaf - maxLength(x): the maximum length of a path whose highest point is $x$ $f(x)$: Longest path starts from node $x$ and goes into its subtree. $g(x)$: Longest path starts in subtree of $x$, passes through $x$ and ends in subtree of $x$ If for all nodes $x$, we take maximum of $f(x), g(x)$, then we can get the diameter. Dynamic programming can be used to calculate the above values for all nodes in $\\Omicron(n)$ time.","title":"Algorithm 1 (based on DP)"},{"location":"Algorithms/Tree/DP/#implementation","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 //adjacency list //adj[i] contains all neighbors of i vector < int > adj [ N ]; //functions as defined above int f [ N ], g [ N ], diameter ; // pV is parent of node V void dfs ( int V , int pV ) { //this vector will store f for all children of V vector < int > fValues ; //traverse over all children for ( auto v : adj [ V ]) { if ( v == pV ) continue ; dfs ( v , V ); fValues . push_back ( f [ v ]); } //sort to get top two values // you can also get top two values without sorting in O(N) // current complexity is n lg n sort ( fValues . begin (), fValues . end ()); f [ V ] = 1 ; if ( ! fValues . empty ()) f [ V ] += fValues . back (); if ( fValues . size () >= 2 ) g [ V ] = 2 + fValues . back () + fValues [ fValues . size () - 2 ]; diameter = max ( diameter , max ( f [ V ], g [ V ])); }","title":"Implementation"},{"location":"Algorithms/Tree/DP/#more_general_implementation","text":"with weighted edges 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 //adjacency list //adj[i] contains all neighbors of i, and weights to i -> its neighbor vector < vector < pair < int , int >>> adj [ N ]; int diameter = 0 ; void dfs ( int V , int pV ) { vector < int > fValues ; for ( auto v : adj [ V ]) { if ( v . first == pV ) continue ; dfs ( v . first , V ); fValues . push_back ( f [ v . first ] + v . second ); // fvalue of child + weight of edge; } int a = - 1 , b = - 1 ; // a is biggest, b is second to biggest for ( auto x : fValues ) { if ( x > a ) { b = a ; a = x ; } else if ( x > b ) { b = x ; } } f [ V ] = 0 ; if ( a > 0 ) f [ V ] = a ; if ( a > 0 && b > 0 ) g [ V ] = a + b ; diameter = max ( diameter , max ( f [ V ], g [ V ])); }","title":"More General Implementation"},{"location":"Algorithms/Tree/DP/#algorithm_2_based_on_dfs","text":"Another efficient way to calculate the diameter of a tree is based on two depth-first searches. First, we choose an arbitrary node $a$ in the tree and find the farthest node $b$ from $a$. Then, we find the farthest node $c$ from $b$. The diameter of the tree is the distance between $b$ and $c$.","title":"Algorithm 2 (based on DFS)"},{"location":"Algorithms/Tree/DP/#how_this_works","text":"Not yet added","title":"How this works?"},{"location":"Algorithms/Tree/DP/#implementation_1","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 int firstFar ; int diameter = 0 ; void dfs ( int V , int pV , int dis ) { if ( diameter < dis ) { diameter = dis ; firstFar = V ; } for ( auto v : adj [ V ]) { if ( v . first == pV ) continue ; dfs ( v . first , V , dis + v . second ); // v.first == neighbor node, v.second weight of edge } } dfs ( 0 , 0 , 0 ) //initial call // diameter = 0; // not necessary; dfs ( firstFar , 0 , 0 ); // now diameter is diameter of tree","title":"implementation"},{"location":"Algorithms/Tree/DP/#resources","text":"https://codeforces.com/blog/entry/20935","title":"resources"},{"location":"Algorithms/Tree/SpanningTree/","text":"Spanning Trees \u00b6 A spanning tree of a graph consists of all nodes of the graph and some of the edges of the graph so that there is a path between any two nodes. Like trees in general, spanning trees are connected and acyclic. Usually there are several ways to construct a spanning tree. Note that a graph may have several minimum and maximum spanning trees, so the trees ar not unique. It turns out that several greedy methods can be used to construct minimum and maximum spanning trees. terminologies \u00b6 Weight of spanning tree: sum of its edge weights. Minimum spanning tree: a spanning tree whose weight is as small as possible Kruskal's Algorithms \u00b6 The initial spanning tree only contains the nodes of the graph and does not contain any edges. Then the algorithm goes through edges ordered by their weights, and always adds an edge to the tree if it does not create a cycle. The algorithm maintains the components of the tree. Initially each node of the graph belongs to a separate component. Always when an edge is added to the tree, two components are joined. Finally, all nodes belong to the same component and a minimum spanning tree has been found Implementation \u00b6 It's convinient to use the edge list representation 1 2 3 4 5 6 vector < pair < int u , int v >> edges ; //edge list sort ( vector . begin (), vector . end ()); for ( auto edge : edges ) { //using union find structure if ( ! same ( a , b )) unite ( a , b ); } efficiency The problem is how to efficiently implement the function same and unite . One possibility is to implement function same as a graph traversal and check if we can get from node a to node b . However, the time complexity of such a function would be $\\Omicron(n+m)$ and resulting algorithm would be slow, because the function same will be called for each edge in graph. Union find structure \u00b6 Using a Union find structure implements both $same$ and $unite$ functions in $\\Omicron(lg(n))$ time. thus the time complexity of Kruskal's algorithm will be $\\Omicron(mlg(n))$ Structure \u00b6 In a union-find structure, one element in each set is the representative of the set, and there is a chain from any other element of the set to the representative. The efficiency of the union-find structure depends on how the sets are joined. It turns out that we can follow a simple strategy: always connect the representative the smaller set to the representative of larger set . Using this strategy, the length of any chain will be $\\Omicron(lg(n))$ Implementation \u00b6 The union-find structure can be implemented using arrays. link contains for each element the next element in the chain or the element it self if it is representative. and the array size indicates for each representative the size of thecorresponding set. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 for ( int i = 1 ; i <= n ; i ++ ) link [ i ] = i ; for ( int i = 1 ; i <= n ; i ++ ) size [ i ] = 1 ; int find ( int x ) { // returns the representative for an element x. while ( x != link [ x ]) x = link [ x ]; return x ; } bool same ( int a , int b ) { // returns whether elements a and b belong to the same set return find ( a ) == find ( b ); } void unite ( int a , int b ) { // joins the set that contains elements a and b // it connects the smaller set to the larger set a = find ( a ); b = find ( b ); if ( size [ a ] < size [ b ]) swap ( a , b ); size [ a ] += size [ b ]; link [ b ] = a ; } Prim's algorithm \u00b6 The algorithm first adds an arbitrary node to the tree. After this, the algorithm always choose a minimum-weight edge that adds a new node to the tree. Finally all nodes have been added to the tree and a minimum tree has been found Prim's algorithm resembles Dijkstra's algorithm . But, Prim's algorithm simply selects the minimum weight edge that adds a new node to the tree. Implementation \u00b6 Like Dijkstra's algorithm , Prim's algorithm can be efficiently implemented using a priority queue. The priority queue should contain all nodes that can be conneted to the current component using a single edge, in increasing order of the weights of the corresponding edges. The time complexity of Prim's algorithm is $\\Omicron(n+ mlg(m))$ that equals the time complexity of Dijkstra's algorithm. most competitive programmers use Kruskal's algorithm. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 int V ; // #nodes(vertices) int E ; // #edges int const INF = 2e9 ; vector < vector < pair < int , int >>> adj ; //adjacency list using node = pair < int , int > ; // pair<key, node> representation. void Prim () { priority_queue < node , vector < node > , greater < node >> pq ; //mean-heap implementation using stl int starter = 0 ; // starting node or initial node. vector < int > key ( V , INF ); vector < int > parent ( V , - 1 ); vector < bool > inMST ( V , false ); pq . push ({ 0 , starter }); int MST_weight = 0 ; while ( ! pq . empty ()) { int w = pq . top (). first ; // minimum weight to add a new vertice to MST int u = pq . top (). second ; // new vertice pq . pop (); if ( inMST [ u ]) continue ; // if u already in MST continue; inMST [ u ] = true ; // else add u to MST; MST_weight += w ; for ( auto vw : adj [ u ]) { int v = vw . first ; int w = vw . second ; if ( inMST [ v ] == false && key [ v ] > w ) { // only when new key(weight) value of v is less than current value key [ v ] = w ; pq . push ({ key [ v ], v }); parent [ v ] = u ; } } } }","title":"SpanningTree"},{"location":"Algorithms/Tree/SpanningTree/#spanning_trees","text":"A spanning tree of a graph consists of all nodes of the graph and some of the edges of the graph so that there is a path between any two nodes. Like trees in general, spanning trees are connected and acyclic. Usually there are several ways to construct a spanning tree. Note that a graph may have several minimum and maximum spanning trees, so the trees ar not unique. It turns out that several greedy methods can be used to construct minimum and maximum spanning trees.","title":"Spanning Trees"},{"location":"Algorithms/Tree/SpanningTree/#terminologies","text":"Weight of spanning tree: sum of its edge weights. Minimum spanning tree: a spanning tree whose weight is as small as possible","title":"terminologies"},{"location":"Algorithms/Tree/SpanningTree/#kruskals_algorithms","text":"The initial spanning tree only contains the nodes of the graph and does not contain any edges. Then the algorithm goes through edges ordered by their weights, and always adds an edge to the tree if it does not create a cycle. The algorithm maintains the components of the tree. Initially each node of the graph belongs to a separate component. Always when an edge is added to the tree, two components are joined. Finally, all nodes belong to the same component and a minimum spanning tree has been found","title":"Kruskal's Algorithms"},{"location":"Algorithms/Tree/SpanningTree/#implementation","text":"It's convinient to use the edge list representation 1 2 3 4 5 6 vector < pair < int u , int v >> edges ; //edge list sort ( vector . begin (), vector . end ()); for ( auto edge : edges ) { //using union find structure if ( ! same ( a , b )) unite ( a , b ); } efficiency The problem is how to efficiently implement the function same and unite . One possibility is to implement function same as a graph traversal and check if we can get from node a to node b . However, the time complexity of such a function would be $\\Omicron(n+m)$ and resulting algorithm would be slow, because the function same will be called for each edge in graph.","title":"Implementation"},{"location":"Algorithms/Tree/SpanningTree/#union_find_structure","text":"Using a Union find structure implements both $same$ and $unite$ functions in $\\Omicron(lg(n))$ time. thus the time complexity of Kruskal's algorithm will be $\\Omicron(mlg(n))$","title":"Union find structure"},{"location":"Algorithms/Tree/SpanningTree/#structure","text":"In a union-find structure, one element in each set is the representative of the set, and there is a chain from any other element of the set to the representative. The efficiency of the union-find structure depends on how the sets are joined. It turns out that we can follow a simple strategy: always connect the representative the smaller set to the representative of larger set . Using this strategy, the length of any chain will be $\\Omicron(lg(n))$","title":"Structure"},{"location":"Algorithms/Tree/SpanningTree/#implementation_1","text":"The union-find structure can be implemented using arrays. link contains for each element the next element in the chain or the element it self if it is representative. and the array size indicates for each representative the size of thecorresponding set. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 for ( int i = 1 ; i <= n ; i ++ ) link [ i ] = i ; for ( int i = 1 ; i <= n ; i ++ ) size [ i ] = 1 ; int find ( int x ) { // returns the representative for an element x. while ( x != link [ x ]) x = link [ x ]; return x ; } bool same ( int a , int b ) { // returns whether elements a and b belong to the same set return find ( a ) == find ( b ); } void unite ( int a , int b ) { // joins the set that contains elements a and b // it connects the smaller set to the larger set a = find ( a ); b = find ( b ); if ( size [ a ] < size [ b ]) swap ( a , b ); size [ a ] += size [ b ]; link [ b ] = a ; }","title":"Implementation"},{"location":"Algorithms/Tree/SpanningTree/#prims_algorithm","text":"The algorithm first adds an arbitrary node to the tree. After this, the algorithm always choose a minimum-weight edge that adds a new node to the tree. Finally all nodes have been added to the tree and a minimum tree has been found Prim's algorithm resembles Dijkstra's algorithm . But, Prim's algorithm simply selects the minimum weight edge that adds a new node to the tree.","title":"Prim's algorithm"},{"location":"Algorithms/Tree/SpanningTree/#implementation_2","text":"Like Dijkstra's algorithm , Prim's algorithm can be efficiently implemented using a priority queue. The priority queue should contain all nodes that can be conneted to the current component using a single edge, in increasing order of the weights of the corresponding edges. The time complexity of Prim's algorithm is $\\Omicron(n+ mlg(m))$ that equals the time complexity of Dijkstra's algorithm. most competitive programmers use Kruskal's algorithm. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 int V ; // #nodes(vertices) int E ; // #edges int const INF = 2e9 ; vector < vector < pair < int , int >>> adj ; //adjacency list using node = pair < int , int > ; // pair<key, node> representation. void Prim () { priority_queue < node , vector < node > , greater < node >> pq ; //mean-heap implementation using stl int starter = 0 ; // starting node or initial node. vector < int > key ( V , INF ); vector < int > parent ( V , - 1 ); vector < bool > inMST ( V , false ); pq . push ({ 0 , starter }); int MST_weight = 0 ; while ( ! pq . empty ()) { int w = pq . top (). first ; // minimum weight to add a new vertice to MST int u = pq . top (). second ; // new vertice pq . pop (); if ( inMST [ u ]) continue ; // if u already in MST continue; inMST [ u ] = true ; // else add u to MST; MST_weight += w ; for ( auto vw : adj [ u ]) { int v = vw . first ; int w = vw . second ; if ( inMST [ v ] == false && key [ v ] > w ) { // only when new key(weight) value of v is less than current value key [ v ] = w ; pq . push ({ key [ v ], v }); parent [ v ] = u ; } } } }","title":"Implementation"},{"location":"Algorithms/Tree/Traversal/","text":"Tree Traversal \u00b6 General Graph Traversal algorithms can be used to traverse the nodes of a tree. However, the traversal of a tree is easier to implement than that of a general graph, because there are no cycles in the tree and it is not possible to reach a node from multiple directions. implementation \u00b6 The typical way to traverse a tree is to start a depth-first search at an arbitrary node . The following recursive function can be used assumes that we are maintaining adjacency list 1 2 3 4 5 6 7 8 void dfs ( int s , int e ) { // current node s and previous node e //process node s for ( auto u : adj [ s ]) { if ( u != e ) dfs ( u , s ); } } dfs ( x , x ) // initial call because there's no self loop","title":"Traversal"},{"location":"Algorithms/Tree/Traversal/#tree_traversal","text":"General Graph Traversal algorithms can be used to traverse the nodes of a tree. However, the traversal of a tree is easier to implement than that of a general graph, because there are no cycles in the tree and it is not possible to reach a node from multiple directions.","title":"Tree Traversal"},{"location":"Algorithms/Tree/Traversal/#implementation","text":"The typical way to traverse a tree is to start a depth-first search at an arbitrary node . The following recursive function can be used assumes that we are maintaining adjacency list 1 2 3 4 5 6 7 8 void dfs ( int s , int e ) { // current node s and previous node e //process node s for ( auto u : adj [ s ]) { if ( u != e ) dfs ( u , s ); } } dfs ( x , x ) // initial call because there's no self loop","title":"implementation"},{"location":"Contribute/CodeOfConduct/","text":"Code of Conduct \u00b6 Copy & go \u00b6 all codes should be working if you directly copy & paste to compiler. Compatibility with STL \u00b6 all the implementation should work with C++ STL. Example YourVector< int> v; ---snip--- sort(v.begin(), v.end()); // should be working","title":"CodeOfConduct"},{"location":"Contribute/CodeOfConduct/#code_of_conduct","text":"","title":"Code of Conduct"},{"location":"Contribute/CodeOfConduct/#copy_go","text":"all codes should be working if you directly copy & paste to compiler.","title":"Copy &amp; go"},{"location":"Contribute/CodeOfConduct/#compatibility_with_stl","text":"all the implementation should work with C++ STL. Example YourVector< int> v; ---snip--- sort(v.begin(), v.end()); // should be working","title":"Compatibility with STL"},{"location":"Contribute/Emoji/","text":"People :bowtie: :smile: :laughing: :blush: :smiley: :relaxed: :smirk: :heart_eyes: :kissing_heart: :kissing_closed_eyes: :flushed: :relieved: :satisfied: :grin: :wink: :stuck_out_tongue_winking_eye: :stuck_out_tongue_closed_eyes: :grinning: :kissing: :kissing_smiling_eyes: :stuck_out_tongue: :sleeping: :worried: :frowning: :anguished: :open_mouth: :grimacing: :confused: :hushed: :expressionless: :unamused: :sweat_smile: :sweat: :disappointed_relieved: :weary: :pensive: :disappointed: :confounded: :fearful: :cold_sweat: :persevere: :cry: :sob: :joy: :astonished: :scream: :neckbeard: :tired_face: :angry: :rage: :triumph: :sleepy: :yum: :mask: :sunglasses: :dizzy_face: :imp: :smiling_imp: :neutral_face: :no_mouth: :innocent: :alien: :yellow_heart: :blue_heart: :purple_heart: :heart: :green_heart: :broken_heart: :heartbeat: :heartpulse: :two_hearts: :revolving_hearts: :cupid: :sparkling_heart: :sparkles: :star: :star2: :dizzy: :boom: :collision: :anger: :exclamation: :question: :grey_exclamation: :grey_question: :zzz: :dash: :sweat_drops: :notes: :musical_note: :fire: :hankey: :poop: :shit: :+1: :thumbsup: :-1: :thumbsdown: :ok_hand: :punch: :facepunch: :fist: :v: :wave: :hand: :raised_hand: :open_hands: :point_up: :point_down: :point_left: :point_right: :raised_hands: :pray: :point_up_2: :clap: :muscle: :metal: :fu: :walking: :runner: :running: :couple: :family: :two_men_holding_hands: :two_women_holding_hands: :dancer: :dancers: :ok_woman: :no_good: :information_desk_person: :raising_hand: :bride_with_veil: :person_with_pouting_face: :person_frowning: :bow: :couplekiss: :couplekiss: :couple_with_heart: :massage: :haircut: :nail_care: :boy: :girl: :woman: :man: :baby: :older_woman: :older_man: :person_with_blond_hair: :man_with_gua_pi_mao: :man_with_turban: :construction_worker: :cop: :angel: :princess: :smiley_cat: :smile_cat: :heart_eyes_cat: :kissing_cat: :smirk_cat: :scream_cat: :crying_cat_face: :joy_cat: :pouting_cat: :japanese_ogre: :japanese_goblin: :see_no_evil: :hear_no_evil: :speak_no_evil: :guardsman: :skull: :feet: :lips: :kiss: :droplet: :ear: :eyes: :nose: :tongue: :love_letter: :bust_in_silhouette: :busts_in_silhouette: :speech_balloon: :thought_balloon: :feelsgood: :finnadie: :goberserk: :godmode: :hurtrealbad: :rage1: :rage2: :rage3: :rage4: :suspect: :trollface: Nature :sunny: :umbrella: :cloud: :snowflake: :snowman: :zap: :cyclone: :foggy: :ocean: :cat: :dog: :mouse: :hamster: :rabbit: :wolf: :frog: :tiger: :koala: :bear: :pig: :pig_nose: :cow: :boar: :monkey_face: :monkey: :horse: :racehorse: :camel: :sheep: :elephant: :panda_face: :snake: :bird: :baby_chick: :hatched_chick: :hatching_chick: :chicken: :penguin: :turtle: :bug: :honeybee: :ant: :beetle: :snail: :octopus: :tropical_fish: :fish: :whale: :whale2: :dolphin: :cow2: :ram: :rat: :water_buffalo: :tiger2: :rabbit2: :dragon: :goat: :rooster: :dog2: :pig2: :mouse2: :ox: :dragon_face: :blowfish: :crocodile: :dromedary_camel: :leopard: :cat2: :poodle: :paw_prints: :bouquet: :cherry_blossom: :tulip: :four_leaf_clover: :rose: :sunflower: :hibiscus: :maple_leaf: :leaves: :fallen_leaf: :herb: :mushroom: :cactus: :palm_tree: :evergreen_tree: :deciduous_tree: :chestnut: :seedling: :blossom: :ear_of_rice: :shell: :globe_with_meridians: :sun_with_face: :full_moon_with_face: :new_moon_with_face: :new_moon: :waxing_crescent_moon: :first_quarter_moon: :waxing_gibbous_moon: :full_moon: :waning_gibbous_moon: :last_quarter_moon: :waning_crescent_moon: :last_quarter_moon_with_face: :first_quarter_moon_with_face: :moon: :earth_africa: :earth_americas: :earth_asia: :volcano: :milky_way: :partly_sunny: :octocat: :squirrel: Objects :bamboo: :gift_heart: :dolls: :school_satchel: :mortar_board: :flags: :fireworks: :sparkler: :wind_chime: :rice_scene: :jack_o_lantern: :ghost: :santa: :christmas_tree: :gift: :bell: :no_bell: :tanabata_tree: :tada: :confetti_ball: :balloon: :crystal_ball: :cd: :dvd: :floppy_disk: :camera: :video_camera: :movie_camera: :computer: :tv: :iphone: :phone: :telephone: :telephone_receiver: :pager: :fax: :minidisc: :vhs: :sound: :speaker: :mute: :loudspeaker: :mega: :hourglass: :hourglass_flowing_sand: :alarm_clock: :watch: :radio: :satellite: :loop: :mag: :mag_right: :unlock: :lock: :lock_with_ink_pen: :closed_lock_with_key: :key: :bulb: :flashlight: :high_brightness: :low_brightness: :electric_plug: :battery: :calling: :email: :mailbox: :postbox: :bath: :bathtub: :shower: :toilet: :wrench: :nut_and_bolt: :hammer: :seat: :moneybag: :yen: :dollar: :pound: :euro: :credit_card: :money_with_wings: :e-mail: :inbox_tray: :outbox_tray: :envelope: :incoming_envelope: :postal_horn: :mailbox_closed: :mailbox_with_mail: :mailbox_with_no_mail: :door: :smoking: :bomb: :gun: :hocho: :pill: :syringe: :page_facing_up: :page_with_curl: :bookmark_tabs: :bar_chart: :chart_with_upwards_trend: :chart_with_downwards_trend: :scroll: :clipboard: :calendar: :date: :card_index: :file_folder: :open_file_folder: :scissors: :pushpin: :paperclip: :black_nib: :pencil2: :straight_ruler: :triangular_ruler: :closed_book: :green_book: :blue_book: :orange_book: :notebook: :notebook_with_decorative_cover: :ledger: :books: :bookmark: :name_badge: :microscope: :telescope: :newspaper: :football: :basketball: :soccer: :baseball: :tennis: :8ball: :rugby_football: :bowling: :golf: :mountain_bicyclist: :bicyclist: :horse_racing: :snowboarder: :swimmer: :surfer: :ski: :spades: :hearts: :clubs: :diamonds: :gem: :ring: :trophy: :musical_score: :musical_keyboard: :violin: :space_invader: :video_game: :black_joker: :flower_playing_cards: :game_die: :dart: :mahjong: :clapper: :memo: :pencil: :book: :art: :microphone: :headphones: :trumpet: :saxophone: :guitar: :shoe: :sandal: :high_heel: :lipstick: :boot: :shirt: :tshirt: :necktie: :womans_clothes: :dress: :running_shirt_with_sash: :jeans: :kimono: :bikini: :ribbon: :tophat: :crown: :womans_hat: :mans_shoe: :closed_umbrella: :briefcase: :handbag: :pouch: :purse: :eyeglasses: :fishing_pole_and_fish: :coffee: :tea: :sake: :baby_bottle: :beer: :beers: :cocktail: :tropical_drink: :wine_glass: :fork_and_knife: :pizza: :hamburger: :fries: :poultry_leg: :meat_on_bone: :spaghetti: :curry: :fried_shrimp: :bento: :sushi: :fish_cake: :rice_ball: :rice_cracker: :rice: :ramen: :stew: :oden: :dango: :egg: :bread: :doughnut: :custard: :icecream: :ice_cream: :shaved_ice: :birthday: :cake: :cookie: :chocolate_bar: :candy: :lollipop: :honey_pot: :apple: :green_apple: :tangerine: :lemon: :cherries: :grapes: :watermelon: :strawberry: :peach: :melon: :banana: :pear: :pineapple: :sweet_potato: :eggplant: :tomato: :corn: Places :house: :house_with_garden: :school: :office: :post_office: :hospital: :bank: :convenience_store: :love_hotel: :hotel: :wedding: :church: :department_store: :european_post_office: :city_sunrise: :city_sunset: :japanese_castle: :european_castle: :tent: :factory: :tokyo_tower: :japan: :mount_fuji: :sunrise_over_mountains: :sunrise: :stars: :statue_of_liberty: :bridge_at_night: :carousel_horse: :rainbow: :ferris_wheel: :fountain: :roller_coaster: :ship: :speedboat: :boat: :sailboat: :rowboat: :anchor: :rocket: :airplane: :helicopter: :steam_locomotive: :tram: :mountain_railway: :bike: :aerial_tramway: :suspension_railway: :mountain_cableway: :tractor: :blue_car: :oncoming_automobile: :car: :red_car: :taxi: :oncoming_taxi: :articulated_lorry: :bus: :oncoming_bus: :rotating_light: :police_car: :oncoming_police_car: :fire_engine: :ambulance: :minibus: :truck: :train: :station: :train2: :bullettrain_front: :bullettrain_side: :light_rail: :monorail: :railway_car: :trolleybus: :ticket: :fuelpump: :vertical_traffic_light: :traffic_light: :warning: :construction: :beginner: :atm: :slot_machine: :busstop: :barber: :hotsprings: :checkered_flag: :crossed_flags: :izakaya_lantern: :moyai: :circus_tent: :performing_arts: :round_pushpin: :triangular_flag_on_post: :jp: :kr: :cn: :us: :fr: :es: :it: :ru: :gb: :uk: :de: Symbols :one: :two: :three: :four: :five: :six: :seven: :eight: :nine: :keycap_ten: :1234: :zero: :hash: :symbols: :arrow_backward: :arrow_down: :arrow_forward: :arrow_left: :capital_abcd: :abcd: :abc: :arrow_lower_left: :arrow_lower_right: :arrow_right: :arrow_up: :arrow_upper_left: :arrow_upper_right: :arrow_double_down: :arrow_double_up: :arrow_down_small: :arrow_heading_down: :arrow_heading_up: :leftwards_arrow_with_hook: :arrow_right_hook: :left_right_arrow: :arrow_up_down: :arrow_up_small: :arrows_clockwise: :arrows_counterclockwise: :rewind: :fast_forward: :information_source: :ok: :twisted_rightwards_arrows: :repeat: :repeat_one: :new: :top: :up: :cool: :free: :ng: :cinema: :koko: :signal_strength: :u5272: :u5408: :u55b6: :u6307: :u6708: :u6709: :u6e80: :u7121: :u7533: :u7a7a: :u7981: :sa: :restroom: :mens: :womens: :baby_symbol: :no_smoking: :parking: :wheelchair: :metro: :baggage_claim: :accept: :wc: :potable_water: :put_litter_in_its_place: :secret: :congratulations: :m: :passport_control: :left_luggage: :customs: :ideograph_advantage: :cl: :sos: :id: :no_entry_sign: :underage: :no_mobile_phones: :do_not_litter: :non-potable_water: :no_bicycles: :no_pedestrians: :children_crossing: :no_entry: :eight_spoked_asterisk: :eight_pointed_black_star: :heart_decoration: :vs: :vibration_mode: :mobile_phone_off: :chart: :currency_exchange: :aries: :taurus: :gemini: :cancer: :leo: :virgo: :libra: :scorpius: :sagittarius: :capricorn: :aquarius: :pisces: :ophiuchus: :six_pointed_star: :negative_squared_cross_mark: :a: :b: :ab: :o2: :diamond_shape_with_a_dot_inside: :recycle: :end: :on: :soon: :clock1: :clock130: :clock10: :clock1030: :clock11: :clock1130: :clock12: :clock1230: :clock2: :clock230: :clock3: :clock330: :clock4: :clock430: :clock5: :clock530: :clock6: :clock630: :clock7: :clock730: :clock8: :clock830: :clock9: :clock930: :heavy_dollar_sign: :copyright: :registered: :tm: :x: :heavy_exclamation_mark: :bangbang: :interrobang: :o: :heavy_multiplication_x: :heavy_plus_sign: :heavy_minus_sign: :heavy_division_sign: :white_flower: :100: :heavy_check_mark: :ballot_box_with_check: :radio_button: :link: :curly_loop: :wavy_dash: :part_alternation_mark: :trident: :black_square: :black_square: :white_square: :white_square: :white_check_mark: :black_square_button: :white_square_button: :black_circle: :white_circle: :red_circle: :large_blue_circle: :large_blue_diamond: :large_orange_diamond: :small_blue_diamond: :small_orange_diamond: :small_red_triangle: :small_red_triangle_down: :shipit:","title":"Emoji"},{"location":"Contribute/HowToContribute/","text":"How To Contribute \u00b6 This is My first open source project under very active development and is also being used to ship code to everybody on codeforces , AtCoder , HackerRank , LeetCode , BaekJoonOnlineJudge and so on. I'm still working out to make contributing to this project as easy and transparent as possible, but I'm not quite there yet. Hopefully this document makes the process for contributing clear and answers some quiestions that you may have. Code of Conduct \u00b6 I adopted a Code of Conduct that i expect project participants to adhere to. You can see full document of Code Of Conduct . 1. copy & pastable \u00b6 All the codes in this site are ready-to-be-compiled that means you could just copy & paste it to see it works. 2. Compatibility \u00b6 Data Structure implementations should be compatible with C++ STL. ex) sort(Your_implementation.begin(), Your_implementation.end()) should work. Use template for your convinience \u00b6 there's a template for contribute","title":"HowToContribute"},{"location":"Contribute/HowToContribute/#how_to_contribute","text":"This is My first open source project under very active development and is also being used to ship code to everybody on codeforces , AtCoder , HackerRank , LeetCode , BaekJoonOnlineJudge and so on. I'm still working out to make contributing to this project as easy and transparent as possible, but I'm not quite there yet. Hopefully this document makes the process for contributing clear and answers some quiestions that you may have.","title":"How To Contribute"},{"location":"Contribute/HowToContribute/#code_of_conduct","text":"I adopted a Code of Conduct that i expect project participants to adhere to. You can see full document of Code Of Conduct .","title":"Code of Conduct"},{"location":"Contribute/HowToContribute/#1_copy_pastable","text":"All the codes in this site are ready-to-be-compiled that means you could just copy & paste it to see it works.","title":"1. copy &amp; pastable"},{"location":"Contribute/HowToContribute/#2_compatibility","text":"Data Structure implementations should be compatible with C++ STL. ex) sort(Your_implementation.begin(), Your_implementation.end()) should work.","title":"2. Compatibility"},{"location":"Contribute/HowToContribute/#use_template_for_your_convinience","text":"there's a template for contribute","title":"Use template for your convinience"},{"location":"Contribute/Template/","text":"Vector \u00b6 Brief explanation. Operations & time complexity \u00b6 Methods RunningTime push_back(val) O(1) pop() O(1) empty() O(1) you can use table generator Implementation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #include <cstido> #include <iostream> template < typename T > class Vector { //use TitleCase for DataStructure implementation --- snip --- } int main () { //few lines of code to test your implementation Vector < int > v ; for ( int i = 0 ; i < n ; i ++ ) { v . push_back ( rand () % 100 ); } sort ( v . begin (), v . end ()) for ( auto x : v ) { cout << x << ' ' ; } } keep your implementation self-contained. Related Problems \u00b6 title of easy problem Some hard problem Lily want a phone add difficulty information(optional) Related Topics \u00b6 Stack Analysis (Optional) \u00b6 You can add some mathematical things here using KaTex as a block tag $$ T(N) = O(N*M) $$ or as a inline tag $T(N) = O(N) $ Contributers (Optional) \u00b6 07.12.2019 contributer1 07.14.2019 typo correction contributer2 07.15.2019 add new section \"Analysis\" contributer3 07.18.2019 fix bugs in \"implementation\" contributer4 07.19.2019 improved performance \"implementation\" contributer5 07.23.2019 refactoring \"implementation\" contributer6 08.02.2019 add related problems contributer7","title":"Template"},{"location":"Contribute/Template/#vector","text":"Brief explanation.","title":"Vector"},{"location":"Contribute/Template/#operations_time_complexity","text":"Methods RunningTime push_back(val) O(1) pop() O(1) empty() O(1) you can use table generator","title":"Operations &amp; time complexity"},{"location":"Contribute/Template/#implementation","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #include <cstido> #include <iostream> template < typename T > class Vector { //use TitleCase for DataStructure implementation --- snip --- } int main () { //few lines of code to test your implementation Vector < int > v ; for ( int i = 0 ; i < n ; i ++ ) { v . push_back ( rand () % 100 ); } sort ( v . begin (), v . end ()) for ( auto x : v ) { cout << x << ' ' ; } } keep your implementation self-contained.","title":"Implementation"},{"location":"Contribute/Template/#related_problems","text":"title of easy problem Some hard problem Lily want a phone add difficulty information(optional)","title":"Related Problems"},{"location":"Contribute/Template/#related_topics","text":"Stack","title":"Related Topics"},{"location":"Contribute/Template/#analysis_optional","text":"You can add some mathematical things here using KaTex as a block tag $$ T(N) = O(N*M) $$ or as a inline tag $T(N) = O(N) $","title":"Analysis (Optional)"},{"location":"Contribute/Template/#contributers_optional","text":"07.12.2019 contributer1 07.14.2019 typo correction contributer2 07.15.2019 add new section \"Analysis\" contributer3 07.18.2019 fix bugs in \"implementation\" contributer4 07.19.2019 improved performance \"implementation\" contributer5 07.23.2019 refactoring \"implementation\" contributer6 08.02.2019 add related problems contributer7","title":"Contributers (Optional)"},{"location":"DataStructures/HashTables/HashFunctions/","text":"Hash Function \u00b6 A hash function is any function that can be used to map data of arbitrary size onto data of a fixed size. Hash Functions \u00b6 1. DJB2 \u00b6 this algorithm (k=33) was first reported by dan bernstein many years ago in comp.lang.c. another version of this algorithm (now favored by bernstein) uses xor: hash(i) = hash(i-1) * 33 ^ str[i]; the magic of number 33 (why it works better than many other constants, prime or not) has never adequately explained 1 2 3 4 5 6 7 8 9 10 unsigned long long djb2 ( char * str ) { unsigned long long hash = 5381 ; int c ; while (( c = * ( str ++ ))) { hash = ( hash << 5 ) + hash + c ; } return hash ; } 2. sdbm \u00b6 this algorithm was created for sdbm (a public-domain reimplementation of ndbm) database library. it was found to do well in scrambling bits, causing better distribution of the keys and fewer splits. it also happens to be a good general hashing function with good distribution. the actual function is hash(i) = hash(i - 1) * 65599 + str[i];; what is included below is faster version used in gawk. (there iseven a faster, duff's device version) the magic constant 65599 was picked out of thin air while experimenting with different constants, and turns out to be a prime. this is one of the algorithms used in berkeley db (see sleepy cat) and else where 1 2 3 4 5 6 7 8 9 10 unsigned long long sdbm ( char * str ) { unsigned long long hash = 5381 ; int c ; while (( c = * ( str ++ ))) { hash = c + ( hash << 6 ) + ( hash << 16 ) - hash ; } return hash ; } 3. lose lose \u00b6 This hash function appeared in K&R (1st ed) but at least the reader was warned: \"This is not the best possible algorithm, but it has the merit of extreme simplicity\". This is an understatement; It is a terrible hashing algorithm, and it could have been much better without scarificing its \"extreme simplicity.\" Many C programmers use this function without actually testing it, or checking something like Knuth's Sorting and searching, so it stuck. It is now found mixed with other respectable code, eg.cnews. Warning Don't use this algorithm, it's terrible. 1 2 3 4 5 6 7 8 9 10 unsigned long long loseLose ( char * str ) { unsigned long long hash = 0 ; int c ; while (( c = * ( str ++ ))) { hash += c ; } return hash ; }","title":"Hash Functions"},{"location":"DataStructures/HashTables/HashFunctions/#hash_function","text":"A hash function is any function that can be used to map data of arbitrary size onto data of a fixed size.","title":"Hash Function"},{"location":"DataStructures/HashTables/HashFunctions/#hash_functions","text":"","title":"Hash Functions"},{"location":"DataStructures/HashTables/HashFunctions/#1_djb2","text":"this algorithm (k=33) was first reported by dan bernstein many years ago in comp.lang.c. another version of this algorithm (now favored by bernstein) uses xor: hash(i) = hash(i-1) * 33 ^ str[i]; the magic of number 33 (why it works better than many other constants, prime or not) has never adequately explained 1 2 3 4 5 6 7 8 9 10 unsigned long long djb2 ( char * str ) { unsigned long long hash = 5381 ; int c ; while (( c = * ( str ++ ))) { hash = ( hash << 5 ) + hash + c ; } return hash ; }","title":"1. DJB2"},{"location":"DataStructures/HashTables/HashFunctions/#2_sdbm","text":"this algorithm was created for sdbm (a public-domain reimplementation of ndbm) database library. it was found to do well in scrambling bits, causing better distribution of the keys and fewer splits. it also happens to be a good general hashing function with good distribution. the actual function is hash(i) = hash(i - 1) * 65599 + str[i];; what is included below is faster version used in gawk. (there iseven a faster, duff's device version) the magic constant 65599 was picked out of thin air while experimenting with different constants, and turns out to be a prime. this is one of the algorithms used in berkeley db (see sleepy cat) and else where 1 2 3 4 5 6 7 8 9 10 unsigned long long sdbm ( char * str ) { unsigned long long hash = 5381 ; int c ; while (( c = * ( str ++ ))) { hash = c + ( hash << 6 ) + ( hash << 16 ) - hash ; } return hash ; }","title":"2. sdbm"},{"location":"DataStructures/HashTables/HashFunctions/#3_lose_lose","text":"This hash function appeared in K&R (1st ed) but at least the reader was warned: \"This is not the best possible algorithm, but it has the merit of extreme simplicity\". This is an understatement; It is a terrible hashing algorithm, and it could have been much better without scarificing its \"extreme simplicity.\" Many C programmers use this function without actually testing it, or checking something like Knuth's Sorting and searching, so it stuck. It is now found mixed with other respectable code, eg.cnews. Warning Don't use this algorithm, it's terrible. 1 2 3 4 5 6 7 8 9 10 unsigned long long loseLose ( char * str ) { unsigned long long hash = 0 ; int c ; while (( c = * ( str ++ ))) { hash += c ; } return hash ; }","title":"3. lose lose"},{"location":"DataStructures/HashTables/Preface/","text":"Preface \u00b6 Many applications require a dynamic set that supports only the dictionary operations. A Hash Table is an effective data structure for implementing dictionaries. A hash table typically uses an array of size proportional to the number of keys actually stored. Hash functions \u00b6 Instead of using the key as an array index directly, the array index is computed from the key Dealing with collisions \u00b6 Collision: two keys hash to the same slot. Since a hash table uses array of size relatively small to the number of possible keys, there is a chance to collisions in which more than one key maps to the same array index Chaining OpenAddressing PerfectHashing OPERATIONS average worst average worst average worst INSERT $O(1)$ - - SEARCH $O(n/m)$ $O(n)$ $O(1)$ $O(1)$ DELETE $O(1)$ - - 1. Chaining \u00b6 In Chaining, we place all the elements that hash to the same slot in to the same linked llist 2. Open Addressing \u00b6 Resolve Collisions with iterative hashing Perfect Hashing \u00b6 Perfect Hasing uses second level Hashtable that has no collision. perfect hashing can support searches in $O(1)\\ wosrt-case$ time, when the set is static(!= dynamic)","title":"Preface"},{"location":"DataStructures/HashTables/Preface/#preface","text":"Many applications require a dynamic set that supports only the dictionary operations. A Hash Table is an effective data structure for implementing dictionaries. A hash table typically uses an array of size proportional to the number of keys actually stored.","title":"Preface"},{"location":"DataStructures/HashTables/Preface/#hash_functions","text":"Instead of using the key as an array index directly, the array index is computed from the key","title":"Hash functions"},{"location":"DataStructures/HashTables/Preface/#dealing_with_collisions","text":"Collision: two keys hash to the same slot. Since a hash table uses array of size relatively small to the number of possible keys, there is a chance to collisions in which more than one key maps to the same array index Chaining OpenAddressing PerfectHashing OPERATIONS average worst average worst average worst INSERT $O(1)$ - - SEARCH $O(n/m)$ $O(n)$ $O(1)$ $O(1)$ DELETE $O(1)$ - -","title":"Dealing with collisions"},{"location":"DataStructures/HashTables/Preface/#1_chaining","text":"In Chaining, we place all the elements that hash to the same slot in to the same linked llist","title":"1. Chaining"},{"location":"DataStructures/HashTables/Preface/#2_open_addressing","text":"Resolve Collisions with iterative hashing","title":"2. Open Addressing"},{"location":"DataStructures/HashTables/Preface/#perfect_hashing","text":"Perfect Hasing uses second level Hashtable that has no collision. perfect hashing can support searches in $O(1)\\ wosrt-case$ time, when the set is static(!= dynamic)","title":"Perfect Hashing"},{"location":"DataStructures/Linear/LinkedList/","text":"LinkedList(Doubly Linked List) \u00b6 A linked list is a linear data structure, in which the elements are not stored at contiguous memory locations. The elements in a linked lists are linked using pointers. Operations & time complexity \u00b6 Member Function Running Time insert_front() $\\Omicron(1)$ insert_back() $\\Omicron(1)$ insert_after() $\\Omicron(1)$ erase $\\Omicron(1)$ search() $\\Omicron(n)$ Implementation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 #include <bits/stdc++.h> using namespace std ; template < typename T > class LinkedList { struct Node { Node * before ; Node * next ; T data ; Node ( T data ) : before ( 0 ), next ( 0 ), data ( data ){} ~ Node () { delete before ; delete next ; delete data ; } }; Node * tail ; unsigned int _size ; public : Node * head ; LinkedList () : head ( 0 ), tail ( 0 ) {} void insert_front ( T val ) { Node * temp = new Node ( val ); if ( head == 0 ) { head = temp ; tail = temp ; } else { temp -> next = head ; head -> before = temp ; head = temp ; } } void insert_back ( T val ) { Node * temp = new Node ( val ); if ( tail == 0 ) { head = temp ; tail = temp ; } else { temp -> before = tail ; tail -> next = temp ; tail = temp ; } } void insert_after ( Node * node , T val ) { Node * temp = new Node ( val ); if ( temp -> next == 0 ) { tail = temp ; } temp -> next = node -> next ; temp -> next -> before = temp ; node -> next = temp ; temp -> before = node ; } Node * search ( T val ) { //search_from head Node * it = head ; while ( it != 0 && it -> data != val ) it = it -> next ; return it ; } void erase ( Node * node ) { if ( node == 0 ) return ; if ( node -> next == 0 ) { tail = node -> before ; tail -> next = 0 ; } else if ( node -> before == 0 ) { head = node -> next ; head -> before = 0 ; } else { node -> before -> next = node -> next ; node -> next = node -> before ; } delete node ; } void print () { Node * it = head ; while ( it != 0 ) { cout << it -> data << ' ' ; it = it -> next ; } } }; int main () { LinkedList < int > list ; for ( int i = 0 ; i < 100 ; i ++ ) { list . insert_front ( i ); } for ( int i = 0 ; i < 100 ; i ++ ) { list . insert_back ( i ); } for ( int i = 0 ; i < 100 ; i ++ ) { list . insert_after ( list . head -> next , i ); } list . print (); return 0 ; } Related Problems \u00b6 NEED_TO_BE_ADDED Related Topics \u00b6 NOT_YET Analysis (Later..) \u00b6 You can add some mathematical things here using KaTex as a block tag $$ T(N) = O(N*M) $$ or as a inline tag $T(N) = O(N) $ Contributers \u00b6 08.15.2019 jchrys","title":"Linked List"},{"location":"DataStructures/Linear/LinkedList/#linkedlistdoubly_linked_list","text":"A linked list is a linear data structure, in which the elements are not stored at contiguous memory locations. The elements in a linked lists are linked using pointers.","title":"LinkedList(Doubly Linked List)"},{"location":"DataStructures/Linear/LinkedList/#operations_time_complexity","text":"Member Function Running Time insert_front() $\\Omicron(1)$ insert_back() $\\Omicron(1)$ insert_after() $\\Omicron(1)$ erase $\\Omicron(1)$ search() $\\Omicron(n)$","title":"Operations &amp; time complexity"},{"location":"DataStructures/Linear/LinkedList/#implementation","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 #include <bits/stdc++.h> using namespace std ; template < typename T > class LinkedList { struct Node { Node * before ; Node * next ; T data ; Node ( T data ) : before ( 0 ), next ( 0 ), data ( data ){} ~ Node () { delete before ; delete next ; delete data ; } }; Node * tail ; unsigned int _size ; public : Node * head ; LinkedList () : head ( 0 ), tail ( 0 ) {} void insert_front ( T val ) { Node * temp = new Node ( val ); if ( head == 0 ) { head = temp ; tail = temp ; } else { temp -> next = head ; head -> before = temp ; head = temp ; } } void insert_back ( T val ) { Node * temp = new Node ( val ); if ( tail == 0 ) { head = temp ; tail = temp ; } else { temp -> before = tail ; tail -> next = temp ; tail = temp ; } } void insert_after ( Node * node , T val ) { Node * temp = new Node ( val ); if ( temp -> next == 0 ) { tail = temp ; } temp -> next = node -> next ; temp -> next -> before = temp ; node -> next = temp ; temp -> before = node ; } Node * search ( T val ) { //search_from head Node * it = head ; while ( it != 0 && it -> data != val ) it = it -> next ; return it ; } void erase ( Node * node ) { if ( node == 0 ) return ; if ( node -> next == 0 ) { tail = node -> before ; tail -> next = 0 ; } else if ( node -> before == 0 ) { head = node -> next ; head -> before = 0 ; } else { node -> before -> next = node -> next ; node -> next = node -> before ; } delete node ; } void print () { Node * it = head ; while ( it != 0 ) { cout << it -> data << ' ' ; it = it -> next ; } } }; int main () { LinkedList < int > list ; for ( int i = 0 ; i < 100 ; i ++ ) { list . insert_front ( i ); } for ( int i = 0 ; i < 100 ; i ++ ) { list . insert_back ( i ); } for ( int i = 0 ; i < 100 ; i ++ ) { list . insert_after ( list . head -> next , i ); } list . print (); return 0 ; }","title":"Implementation"},{"location":"DataStructures/Linear/LinkedList/#related_problems","text":"NEED_TO_BE_ADDED","title":"Related Problems"},{"location":"DataStructures/Linear/LinkedList/#related_topics","text":"NOT_YET","title":"Related Topics"},{"location":"DataStructures/Linear/LinkedList/#analysis_later","text":"You can add some mathematical things here using KaTex as a block tag $$ T(N) = O(N*M) $$ or as a inline tag $T(N) = O(N) $","title":"Analysis (Later..)"},{"location":"DataStructures/Linear/LinkedList/#contributers","text":"08.15.2019 jchrys","title":"Contributers"},{"location":"DataStructures/Linear/Stack/","text":"Stack \u00b6 Element deleted from the set is the one most recently inserted; Stack implements last-in, first out or LIFO policy You can use array to implement Stack supported operations \u00b6 insert, delete, empty, top, size 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 template < typename T > class Stack { public : struct Node { T val ; Node * next ; Node () {}; Node ( T val ) : val ( val ), next ( 0 ){}; }; Node * head ; int _size ; Stack () { head = 0 ; _size = 0 ; } void push ( T val ) { Node * temp = new Node ( val ); if ( head == 0 ) { head = temp ; } else { temp -> next = head ; head = temp ; } _size ++ ; } void pop () { if ( empty ()) return ; Node * temp = head ; head = head -> next ; delete temp ; _size -- ; } bool empty () const { return _size == 0 ; } T top () const { return head -> val ; } int size () const { return _size ; } };","title":"Stack"},{"location":"DataStructures/Linear/Stack/#stack","text":"Element deleted from the set is the one most recently inserted; Stack implements last-in, first out or LIFO policy You can use array to implement Stack","title":"Stack"},{"location":"DataStructures/Linear/Stack/#supported_operations","text":"insert, delete, empty, top, size 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 template < typename T > class Stack { public : struct Node { T val ; Node * next ; Node () {}; Node ( T val ) : val ( val ), next ( 0 ){}; }; Node * head ; int _size ; Stack () { head = 0 ; _size = 0 ; } void push ( T val ) { Node * temp = new Node ( val ); if ( head == 0 ) { head = temp ; } else { temp -> next = head ; head = temp ; } _size ++ ; } void pop () { if ( empty ()) return ; Node * temp = head ; head = head -> next ; delete temp ; _size -- ; } bool empty () const { return _size == 0 ; } T top () const { return head -> val ; } int size () const { return _size ; } };","title":"supported operations"},{"location":"DataStructures/Linear/Vector/","text":"Vector \u00b6 Vector is Dynamic array structure in c++ Operations & time complexity \u00b6 Member Function Running Time push_back() $O(1) amortized$ pop_back() $O(1)$ empty() $O(1)$ reserve() $O(n)$ operator [] $O(1)$ Implementation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 using size_t = unsigned long ; template < typename T > class Vector { size_t _size ; size_t _capacity ; T * _buf ; public : // constructors // Vector(SomeType); //\"ordinary constructor\" Vector ( int k ) { _size = k ; _capacity = k ; _buf = new T [ _capacity ]; } // Vector(); //default constructor Vector () { _size = 0 ; _capacity = 0 ; _buf = new T [ _capacity ]; } // Vector(const &X); // copy constructor // Vector(&&X); //move constructor // &Vector operator=(const Vector&); //copy assignment: cleanup target and copy // &Vector operator=(Vector&&); // move assignment: cleanup target and move //~Vector(); //destructor: cleanup ~ Vector () { delete [] _buf ; } // capacity: size_t size () { return _size ; } void resize ( size_t n ) { _size = n ; } size_t capacity () { return _capacity ; }; bool empty () { return _size == 0 ; }; // unsigned int max_size(); void reserve ( size_t n ) { //Requests that the vector capacity be at least enough to contain n elements. if ( _size >= n ) return ; T * _temp = new T [ n ]; for ( size_t i = 0 ; i < _size ; i ++ ) { _temp [ i ] = _buf [ i ]; } _capacity = n ; delete [] _buf ; _buf = _temp ; } // shrink_to_fit() //element access: T back (); // operator[]() T & operator []( int idx ) { return _buf [ idx ]; } T operator []( int idx ) const { return _buf [ idx ]; } // at() // front() // data() //Modifiers void clear () { resize ( 0 ); }; void push_back ( T const & val ) { if ( _size == _capacity ) { if ( _capacity ) { reserve ( _capacity << 1 ); } else { reserve ( 1 ); } } _buf [ _size ++ ] = val ; }; void pop_back () { _size -- ; }; // assign() // insert() // erase() // emplace() // emplace_back //Iterators T * begin () { return & _buf [ 0 ]; } T * end () { return & _buf [ 0 ] + _size ; }; //T* rbegin(); //T* rend(); //T* const cbegin(); //T* const cend(); //T* const crbegin(); //T* const crend(); }; Related Problems \u00b6 Letters Shop Related Topics \u00b6 Stack Analysis (Later..) \u00b6 You can add some mathematical things here using KaTex as a block tag $$ T(N) = O(N*M) $$ or as a inline tag $T(N) = O(N) $ Contributers \u00b6 08.13.2019 jchrys","title":"Vector"},{"location":"DataStructures/Linear/Vector/#vector","text":"Vector is Dynamic array structure in c++","title":"Vector"},{"location":"DataStructures/Linear/Vector/#operations_time_complexity","text":"Member Function Running Time push_back() $O(1) amortized$ pop_back() $O(1)$ empty() $O(1)$ reserve() $O(n)$ operator [] $O(1)$","title":"Operations &amp; time complexity"},{"location":"DataStructures/Linear/Vector/#implementation","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 using size_t = unsigned long ; template < typename T > class Vector { size_t _size ; size_t _capacity ; T * _buf ; public : // constructors // Vector(SomeType); //\"ordinary constructor\" Vector ( int k ) { _size = k ; _capacity = k ; _buf = new T [ _capacity ]; } // Vector(); //default constructor Vector () { _size = 0 ; _capacity = 0 ; _buf = new T [ _capacity ]; } // Vector(const &X); // copy constructor // Vector(&&X); //move constructor // &Vector operator=(const Vector&); //copy assignment: cleanup target and copy // &Vector operator=(Vector&&); // move assignment: cleanup target and move //~Vector(); //destructor: cleanup ~ Vector () { delete [] _buf ; } // capacity: size_t size () { return _size ; } void resize ( size_t n ) { _size = n ; } size_t capacity () { return _capacity ; }; bool empty () { return _size == 0 ; }; // unsigned int max_size(); void reserve ( size_t n ) { //Requests that the vector capacity be at least enough to contain n elements. if ( _size >= n ) return ; T * _temp = new T [ n ]; for ( size_t i = 0 ; i < _size ; i ++ ) { _temp [ i ] = _buf [ i ]; } _capacity = n ; delete [] _buf ; _buf = _temp ; } // shrink_to_fit() //element access: T back (); // operator[]() T & operator []( int idx ) { return _buf [ idx ]; } T operator []( int idx ) const { return _buf [ idx ]; } // at() // front() // data() //Modifiers void clear () { resize ( 0 ); }; void push_back ( T const & val ) { if ( _size == _capacity ) { if ( _capacity ) { reserve ( _capacity << 1 ); } else { reserve ( 1 ); } } _buf [ _size ++ ] = val ; }; void pop_back () { _size -- ; }; // assign() // insert() // erase() // emplace() // emplace_back //Iterators T * begin () { return & _buf [ 0 ]; } T * end () { return & _buf [ 0 ] + _size ; }; //T* rbegin(); //T* rend(); //T* const cbegin(); //T* const cend(); //T* const crbegin(); //T* const crend(); };","title":"Implementation"},{"location":"DataStructures/Linear/Vector/#related_problems","text":"Letters Shop","title":"Related Problems"},{"location":"DataStructures/Linear/Vector/#related_topics","text":"Stack","title":"Related Topics"},{"location":"DataStructures/Linear/Vector/#analysis_later","text":"You can add some mathematical things here using KaTex as a block tag $$ T(N) = O(N*M) $$ or as a inline tag $T(N) = O(N) $","title":"Analysis (Later..)"},{"location":"DataStructures/Linear/Vector/#contributers","text":"08.13.2019 jchrys","title":"Contributers"},{"location":"DataStructures/Trees/BST/","text":"Binary Search Tree \u00b6 A Search tree is called Binary Search if it satisfies BST property and it's #children $\\leq$ 2. Binary Search Tree Property \u00b6 Let $x$ be a node in a binary search tree. If $y$ is a node in the left subtree of $x$, then $y.key \\leq x.key$. If $y$ is a node in the right subtree of $x$, then $y.key \\leq x.key$. Operations & timeComplexity \u00b6 $h = height(tree)$ Member Function Running Time insert() $\\Omicron(h)$ erase() $\\Omicron(h)$ inorder_tree_walk $\\Theta(n)$ find() $\\Omicron(h)$ minimum() $\\Omicron(h)$ maximum() $\\Omicron(h)$ successor() $\\Omicron(h)$ predecessor() $\\Omicron(h)$ Warning it is not guaranteed that $h = \\Omicron(log(n))$ this binary search tree is not balanced Implementation c++ \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 #include <iostream> using namespace std ; template < typename T > class Set { struct Node { T key ; Node * parent ; Node * left ; Node * right ; Node ( T key ) { this -> key = key ; this -> parent = 0 ; this -> left = 0 ; this -> right = 0 ; } }; Node * root ; unsigned int _size ; public : Set () : root ( 0 ), _size ( 0 ) { //default constructor } void insert ( T key ) { _insert ( new Node ( key )); //insert a key in to set (helper function) } void _insert ( Node * && node ) { // Node * y = 0 ; Node * x = this -> root ; while ( x != 0 ) { y = x ; if ( node -> key < x -> key ) { x = x -> left ; } else if ( node -> key == x -> key ) { return ; } else { x = x -> right ; } } node -> parent = y ; if ( y == 0 ) // when tree is empty -> you could check with _size; this -> root = node ; else if ( node -> key < y -> key ) { y -> left = node ; node -> parent = y ; } else { y -> right = node ; node -> parent = y ; } this -> _size ++ ; } Node * find ( T key ) { Node * x = this -> root ; while ( x != 0 && x -> key != key ) { if ( x -> key > key ) { x = x -> left ; } else { x = x -> right ; } } return x ; } Node * minimum () { _minimum ( this -> root ); } Node * _minimum ( Node * x ) { while ( x -> left != 0 ) { x = x -> left ; } return x ; } Node * maximum () { //returns Node* that with maximum key return _maximum ( this -> root ); } Node * _maximum ( Node * & x ) { while ( x -> right != 0 ) { x = x -> right ; } return x ; } Node * successor ( Node * x ) { if ( x -> right != 0 ) { return _minimum ( x -> right ); } Node * y = x -> parent ; while ( y != 0 && x == y -> right ) { x = y ; y = y -> parent ; } return y ; } Node * predecessor ( Node * x ) { if ( x -> left != 0 ) { return _maximum ( x -> left ); } Node * y = x -> parent ; while ( y != 0 && x == y -> left ) { x = y ; y = y -> parent ; } return y ; } unsigned int size () { return _size ; } void _inorder_tree_travel ( Node * const & node ) { if ( node == 0 ) return ; _inorder_tree_travel ( node -> left ); cout << node -> key << ' ' ; _inorder_tree_travel ( node -> right ); } void inorder_tree_travel () { _inorder_tree_travel ( this -> root ); } void transplant ( Node * u , Node * v ) { if ( u -> parent == 0 ) { this -> root = v ; } else if ( u == u -> parent -> left ) { u -> parent -> left = v ; } else { u -> parent -> right = v ; } if ( v != 0 ) { v -> parent = u -> parent ; } } void erase ( T key ) { _erase ( find ( key )); } void _erase ( Node * target ) { if ( target == 0 ) return ; if ( target -> left == 0 ) transplant ( target , target -> right ); else if ( target -> right == 0 ) transplant ( target , target -> left ); else { Node * y = _minimum ( target -> right ); if ( y -> parent != target ) { transplant ( y , y -> right ); y -> right = target -> right ; y -> right -> parent = y ; } transplant ( target , y ); y -> left = target -> left ; y -> left -> parent = y ; } delete target ; _size -- ; } unsigned int height ( Node * node ) { if ( node == 0 ) return 0 ; unsigned int lDepth = height ( node -> left ); unsigned int rDepth = height ( node -> right ); if ( lDepth > rDepth ) return lDepth + 1 ; return rDepth + 1 ; } unsigned int tree_height () { return height ( this -> root ); } }; int main () { Set < int > s ; // if input's are random; cout << \"Naive Binary Search Tree implementation\" << endl ; cout << \"-------BEST-CASE(random inputs)--------\" << endl ; cout << \"input: 10,000 random integers\" << endl ; for ( int i = 0 ; i < 10000 ; i ++ ) { s . insert ( rand () % 1000000 ); } cout << \"-----------------results----------------\" << endl ; cout << \"tree_height: \" << s . tree_height () << endl ; cout << endl << endl << endl ; cout << \"------WORST-CASE(sorted_inputs)---------\" << endl ; cout << \"input: [1, 2, 3, ..., 10000]\" << endl ; Set < int > worst ; for ( int i = 1 ; i <= 10000 ; i ++ ) { worst . insert ( i ); } cout << \"-----------------results----------------\" << endl ; cout << \"tree_height: \" << worst . tree_height () << endl ; return 0 ; } Related Problems \u00b6 Related Topics \u00b6 Analysis (Later..) \u00b6 You can add some mathematical things here using KaTex as a block tag $$ T(N) = O(N*M) $$ or as a inline tag $T(N) = O(N) $ Contributers \u00b6 08.15.2019 jchrys","title":"BST"},{"location":"DataStructures/Trees/BST/#binary_search_tree","text":"A Search tree is called Binary Search if it satisfies BST property and it's #children $\\leq$ 2.","title":"Binary Search Tree"},{"location":"DataStructures/Trees/BST/#binary_search_tree_property","text":"Let $x$ be a node in a binary search tree. If $y$ is a node in the left subtree of $x$, then $y.key \\leq x.key$. If $y$ is a node in the right subtree of $x$, then $y.key \\leq x.key$.","title":"Binary Search Tree Property"},{"location":"DataStructures/Trees/BST/#operations_timecomplexity","text":"$h = height(tree)$ Member Function Running Time insert() $\\Omicron(h)$ erase() $\\Omicron(h)$ inorder_tree_walk $\\Theta(n)$ find() $\\Omicron(h)$ minimum() $\\Omicron(h)$ maximum() $\\Omicron(h)$ successor() $\\Omicron(h)$ predecessor() $\\Omicron(h)$ Warning it is not guaranteed that $h = \\Omicron(log(n))$ this binary search tree is not balanced","title":"Operations &amp; timeComplexity"},{"location":"DataStructures/Trees/BST/#implementation_c","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 #include <iostream> using namespace std ; template < typename T > class Set { struct Node { T key ; Node * parent ; Node * left ; Node * right ; Node ( T key ) { this -> key = key ; this -> parent = 0 ; this -> left = 0 ; this -> right = 0 ; } }; Node * root ; unsigned int _size ; public : Set () : root ( 0 ), _size ( 0 ) { //default constructor } void insert ( T key ) { _insert ( new Node ( key )); //insert a key in to set (helper function) } void _insert ( Node * && node ) { // Node * y = 0 ; Node * x = this -> root ; while ( x != 0 ) { y = x ; if ( node -> key < x -> key ) { x = x -> left ; } else if ( node -> key == x -> key ) { return ; } else { x = x -> right ; } } node -> parent = y ; if ( y == 0 ) // when tree is empty -> you could check with _size; this -> root = node ; else if ( node -> key < y -> key ) { y -> left = node ; node -> parent = y ; } else { y -> right = node ; node -> parent = y ; } this -> _size ++ ; } Node * find ( T key ) { Node * x = this -> root ; while ( x != 0 && x -> key != key ) { if ( x -> key > key ) { x = x -> left ; } else { x = x -> right ; } } return x ; } Node * minimum () { _minimum ( this -> root ); } Node * _minimum ( Node * x ) { while ( x -> left != 0 ) { x = x -> left ; } return x ; } Node * maximum () { //returns Node* that with maximum key return _maximum ( this -> root ); } Node * _maximum ( Node * & x ) { while ( x -> right != 0 ) { x = x -> right ; } return x ; } Node * successor ( Node * x ) { if ( x -> right != 0 ) { return _minimum ( x -> right ); } Node * y = x -> parent ; while ( y != 0 && x == y -> right ) { x = y ; y = y -> parent ; } return y ; } Node * predecessor ( Node * x ) { if ( x -> left != 0 ) { return _maximum ( x -> left ); } Node * y = x -> parent ; while ( y != 0 && x == y -> left ) { x = y ; y = y -> parent ; } return y ; } unsigned int size () { return _size ; } void _inorder_tree_travel ( Node * const & node ) { if ( node == 0 ) return ; _inorder_tree_travel ( node -> left ); cout << node -> key << ' ' ; _inorder_tree_travel ( node -> right ); } void inorder_tree_travel () { _inorder_tree_travel ( this -> root ); } void transplant ( Node * u , Node * v ) { if ( u -> parent == 0 ) { this -> root = v ; } else if ( u == u -> parent -> left ) { u -> parent -> left = v ; } else { u -> parent -> right = v ; } if ( v != 0 ) { v -> parent = u -> parent ; } } void erase ( T key ) { _erase ( find ( key )); } void _erase ( Node * target ) { if ( target == 0 ) return ; if ( target -> left == 0 ) transplant ( target , target -> right ); else if ( target -> right == 0 ) transplant ( target , target -> left ); else { Node * y = _minimum ( target -> right ); if ( y -> parent != target ) { transplant ( y , y -> right ); y -> right = target -> right ; y -> right -> parent = y ; } transplant ( target , y ); y -> left = target -> left ; y -> left -> parent = y ; } delete target ; _size -- ; } unsigned int height ( Node * node ) { if ( node == 0 ) return 0 ; unsigned int lDepth = height ( node -> left ); unsigned int rDepth = height ( node -> right ); if ( lDepth > rDepth ) return lDepth + 1 ; return rDepth + 1 ; } unsigned int tree_height () { return height ( this -> root ); } }; int main () { Set < int > s ; // if input's are random; cout << \"Naive Binary Search Tree implementation\" << endl ; cout << \"-------BEST-CASE(random inputs)--------\" << endl ; cout << \"input: 10,000 random integers\" << endl ; for ( int i = 0 ; i < 10000 ; i ++ ) { s . insert ( rand () % 1000000 ); } cout << \"-----------------results----------------\" << endl ; cout << \"tree_height: \" << s . tree_height () << endl ; cout << endl << endl << endl ; cout << \"------WORST-CASE(sorted_inputs)---------\" << endl ; cout << \"input: [1, 2, 3, ..., 10000]\" << endl ; Set < int > worst ; for ( int i = 1 ; i <= 10000 ; i ++ ) { worst . insert ( i ); } cout << \"-----------------results----------------\" << endl ; cout << \"tree_height: \" << worst . tree_height () << endl ; return 0 ; }","title":"Implementation c++"},{"location":"DataStructures/Trees/BST/#related_problems","text":"","title":"Related Problems"},{"location":"DataStructures/Trees/BST/#related_topics","text":"","title":"Related Topics"},{"location":"DataStructures/Trees/BST/#analysis_later","text":"You can add some mathematical things here using KaTex as a block tag $$ T(N) = O(N*M) $$ or as a inline tag $T(N) = O(N) $","title":"Analysis (Later..)"},{"location":"DataStructures/Trees/BST/#contributers","text":"08.15.2019 jchrys","title":"Contributers"},{"location":"DataStructures/Trees/RedBlackTree/","text":"Red Black Tree \u00b6 Red Black Tree is balanced binary search tree with one extra bit of storage per node: color Red Black Tree satisfies the Red-Black-Properties Red-Black-Properties Every node is black or red The root is black Every leaf(NIL) is black if a node is red, then both its children are black For each node, all simple paths from the node to descendant leaves contains the same number of black nodes. Operations & time complexity \u00b6 $N$ = number of elements in Tree Member Function Running Time insert() $\\Omicron(\\lg(N))$ erase() $\\Omicron(\\lg(N))$ inorder_tree_walk $\\Theta(N)$ find() $\\Omicron(\\lg(N))$ minimum() $\\Omicron(\\lg(N))$ maximum() $\\Omicron(\\lg(N))$ successor() $\\Omicron(\\lg(N))$ predecessor() $\\Omicron(\\lg(N))$ Note Red Black Tree is Balanced Binary Search Tree It is guaranteed that height of the tree is $\\Omicron(\\lg(N))$ in worst case Implementation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 #include <bits/stdc++.h> using namespace std ; template < typename K , typename V > class Map { struct Node { K key ; V val ; bool color ; // 0: red, 1:black Node * p ; Node * left ; Node * right ; Node ( Map & out ) : key (), val (), color ( 1 ), p ( out . NIL ), left ( out . NIL ), right ( out . NIL ){} Node ( Map & out , K key , V val ) : key ( key ), val ( val ), color ( 0 ), p ( out . NIL ), left ( out . NIL ), right ( out . NIL ){} }; Node * root ; void left_rotate ( Node * x ) { Node * y = x -> right ; // y is x's right child x -> right = y -> left ; // set y's left child to x's right child; x -> right -> p = x ; // set parent y -> p = x -> p ; //set x's parent pointing to y; if ( x -> p == NIL ) { // if x is root this -> root = y ; } else if ( x == x -> p -> left ) { //if x is left child x -> p -> left = y ; } else { x -> p -> right = y ; } x -> p = y ; // y is x's parent y -> left = x ; // x is y's left child } void right_rotate ( Node * x ) { Node * y = x -> left ; x -> left = y -> right ; x -> left -> p = x ; y -> p = x -> p ; if ( x -> p == NIL ) { this -> root = y ; } else if ( x == x -> p -> left ) { x -> p -> left = y ; } else { x -> p -> right = y ; } x -> p = y ; y -> right = x ; } void transplant ( Node * u , Node * v ) { // gives u's parent relations to v if ( u -> p == NIL ) { root = v ; } else if ( u == u -> p -> right ) { u -> p -> right = v ; } else { u -> p -> left = v ; } v -> p = u -> p ; // unconditionally because NIL can have parent also; } public : Node * NIL ; Map () { NIL = new Node ( * this ); root = NIL ; } void insert_fixup ( Node * & z ) { while ( z -> p -> color == 0 ) { Node * y ; // z's uncle if ( z -> p == z -> p -> p -> left ) { // when z's parent is left child y = z -> p -> p -> right ; if ( y -> color == 0 ) { // if uncle is red, uncles parent should be black z -> p -> color = 1 ; // recoloring and goes up y -> color = 1 ; z -> p -> p -> color = 0 ; z = z -> p -> p ; } else { if ( z == z -> p -> right ) { // if uncle is black and z is right child z = z -> p ; left_rotate ( z ); } // if uncle is black and z is right child z -> p -> color = 1 ; z -> p -> p -> color = 0 ; right_rotate ( z -> p -> p ); } } else { // when z's parent is right child y = z -> p -> p -> left ; if ( y -> color == 0 ) { z -> p -> color = 1 ; y -> color = 1 ; z -> p -> p -> color = 0 ; z = z -> p -> p ; } else { // if uncle's color is black if ( z == z -> p -> left ) { //when z is right child z = z -> p ; right_rotate ( z ); } z -> p -> color = 1 ; z -> p -> p -> color = 0 ; left_rotate ( z -> p -> p ); } } } this -> root -> color = 1 ; } void insert ( K key , V val ) { Node * z = new Node ( * this , key , val ); Node * y = NIL ; Node * x = this -> root ; while ( x != NIL ) { y = x ; if ( z -> key < x -> key ) { x = x -> left ; } else { x = x -> right ; } } z -> p = y ; if ( y == NIL ) { this -> root = z ; } else if ( z -> key < y -> key ) { y -> left = z ; } else { y -> right = z ; } // z->left = NIL; // z->right = ZIL; // z->color = 0; insert_fixup ( z ); } Node * find ( K key ) { Node * x = root ; while ( x != NIL && x -> key != key ) { if ( key < x -> key ) { x = x -> left ; } else { x = x -> right ; } } return x ; } Node * minimum ( Node * x ) { while ( x -> left != NIL ) { x = x -> left ; } return x ; } void erase_fixup ( Node * x ) { Node * w ; // sibling of x; while ( x != this -> root && x -> color == 1 ) { //only if x is black and not root if ( x == x -> p -> left ) { w = x -> p -> right ; if ( w -> color == 0 ) { // turns to case2, 3 or 4; w -> color = 1 ; x -> p -> color = 0 ; left_rotate ( x -> p ); w = x -> p -> right ; } if ( w -> left -> color == 1 && w -> right -> color == 1 ) { // case2 w -> color = 0 ; x = x -> p ; } else { if ( w -> right -> color == 1 ) { //case3 -> turns to case4 w -> left -> color = 1 ; w -> color = 0 ; right_rotate ( w ); w = x -> p -> right ; } w -> color = x -> p -> color ; // case4 -> we can make legit red-black tree x -> p -> color = 1 ; w -> right -> color = 1 ; left_rotate ( x -> p ); x = root ; } } else { // x == x->p->right w = x -> p -> left ; if ( w -> color == 0 ) { w -> color = 1 ; x -> p -> color = 0 ; right_rotate ( x -> p ); w = x -> p -> left ; } if ( w -> left -> color == 1 && w -> right -> color == 1 ) { w -> color = 0 ; x = x -> p ; } else { if ( w -> left -> color == 1 ) { w -> color = 0 ; w -> right -> color = 1 ; left_rotate ( w ); w = x -> p -> left ; } w -> color = w -> p -> color ; w -> p -> color = 1 ; w -> left -> color = 1 ; right_rotate ( x -> p ); x = root ; } } } x -> color = 1 ; } void erase ( Node * z ) { Node * y = z ; bool y_original_color = y -> color ; Node * x ; if ( z -> left == NIL ) { x = z -> right ; transplant ( z , z -> right ); } else if ( z -> right == NIL ) { x = z -> left ; transplant ( z , z -> left ); } else { y = minimum ( z -> right ); y_original_color = y -> color ; x = y -> right ; if ( y -> p == z ) { x -> p = y ; // incase of x is NIL!! we need to find it's parent! } else { transplant ( y , y -> right ); y -> right = z -> right ; y -> right -> p = y ; } transplant ( z , y ); y -> left = z -> left ; y -> left -> p = y ; y -> color = z -> color ; } if ( y_original_color == 1 ) { erase_fixup ( x ); } } void rb_printer ( Node * node , int indent ) { //prints red & black tree int count = 4 ; if ( node == NIL ) return ; indent += count ; rb_printer ( node -> right , indent ); cout << endl ; for ( int i = count ; i < indent ; i ++ ) { cout << \" \" ; } cout << ( node -> color == 0 ? \" \\033 [1;31m\" : \"\" ) << ( node == node -> p -> left ? \"l\" : \"r\" ) << node -> key << ( node -> color == 0 ? \" \\033 [0m\" : \"\" ) << endl ; rb_printer ( node -> left , indent ); } void print () { rb_printer ( this -> root , 0 ); } }; int main () { Map < int , int > m ; for ( int i = 0 ; i < 20 ; i ++ ) { m . insert ( rand () % 20 , 1 ); } m . print (); cout << \"deleting ---\" << endl ;; for ( int i = 0 ; i < 20 ; i ++ ) { int key = rand () % 20 ; auto it = m . find ( key ); cout << \"delete: \" << key << endl ; if ( it != m . NIL ) { cout << \"key exist... deleting...\" ; m . erase ( it ); m . print (); } else { cout << \"key not exist\" << endl ; } } return 0 ; } Related Problems \u00b6 NOT ADDED YET Related Topics \u00b6 BinarySearchTree Analysis (Optional) \u00b6 You can add some mathematical things here using KaTex as a block tag $$ T(N) = O(N*M) $$ or as a inline tag $T(N) = O(N) $ Contributers (Optional) \u00b6 08.18.2019 JCHRYS","title":"RedBlackTree"},{"location":"DataStructures/Trees/RedBlackTree/#red_black_tree","text":"Red Black Tree is balanced binary search tree with one extra bit of storage per node: color Red Black Tree satisfies the Red-Black-Properties Red-Black-Properties Every node is black or red The root is black Every leaf(NIL) is black if a node is red, then both its children are black For each node, all simple paths from the node to descendant leaves contains the same number of black nodes.","title":"Red Black Tree"},{"location":"DataStructures/Trees/RedBlackTree/#operations_time_complexity","text":"$N$ = number of elements in Tree Member Function Running Time insert() $\\Omicron(\\lg(N))$ erase() $\\Omicron(\\lg(N))$ inorder_tree_walk $\\Theta(N)$ find() $\\Omicron(\\lg(N))$ minimum() $\\Omicron(\\lg(N))$ maximum() $\\Omicron(\\lg(N))$ successor() $\\Omicron(\\lg(N))$ predecessor() $\\Omicron(\\lg(N))$ Note Red Black Tree is Balanced Binary Search Tree It is guaranteed that height of the tree is $\\Omicron(\\lg(N))$ in worst case","title":"Operations &amp; time complexity"},{"location":"DataStructures/Trees/RedBlackTree/#implementation","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 #include <bits/stdc++.h> using namespace std ; template < typename K , typename V > class Map { struct Node { K key ; V val ; bool color ; // 0: red, 1:black Node * p ; Node * left ; Node * right ; Node ( Map & out ) : key (), val (), color ( 1 ), p ( out . NIL ), left ( out . NIL ), right ( out . NIL ){} Node ( Map & out , K key , V val ) : key ( key ), val ( val ), color ( 0 ), p ( out . NIL ), left ( out . NIL ), right ( out . NIL ){} }; Node * root ; void left_rotate ( Node * x ) { Node * y = x -> right ; // y is x's right child x -> right = y -> left ; // set y's left child to x's right child; x -> right -> p = x ; // set parent y -> p = x -> p ; //set x's parent pointing to y; if ( x -> p == NIL ) { // if x is root this -> root = y ; } else if ( x == x -> p -> left ) { //if x is left child x -> p -> left = y ; } else { x -> p -> right = y ; } x -> p = y ; // y is x's parent y -> left = x ; // x is y's left child } void right_rotate ( Node * x ) { Node * y = x -> left ; x -> left = y -> right ; x -> left -> p = x ; y -> p = x -> p ; if ( x -> p == NIL ) { this -> root = y ; } else if ( x == x -> p -> left ) { x -> p -> left = y ; } else { x -> p -> right = y ; } x -> p = y ; y -> right = x ; } void transplant ( Node * u , Node * v ) { // gives u's parent relations to v if ( u -> p == NIL ) { root = v ; } else if ( u == u -> p -> right ) { u -> p -> right = v ; } else { u -> p -> left = v ; } v -> p = u -> p ; // unconditionally because NIL can have parent also; } public : Node * NIL ; Map () { NIL = new Node ( * this ); root = NIL ; } void insert_fixup ( Node * & z ) { while ( z -> p -> color == 0 ) { Node * y ; // z's uncle if ( z -> p == z -> p -> p -> left ) { // when z's parent is left child y = z -> p -> p -> right ; if ( y -> color == 0 ) { // if uncle is red, uncles parent should be black z -> p -> color = 1 ; // recoloring and goes up y -> color = 1 ; z -> p -> p -> color = 0 ; z = z -> p -> p ; } else { if ( z == z -> p -> right ) { // if uncle is black and z is right child z = z -> p ; left_rotate ( z ); } // if uncle is black and z is right child z -> p -> color = 1 ; z -> p -> p -> color = 0 ; right_rotate ( z -> p -> p ); } } else { // when z's parent is right child y = z -> p -> p -> left ; if ( y -> color == 0 ) { z -> p -> color = 1 ; y -> color = 1 ; z -> p -> p -> color = 0 ; z = z -> p -> p ; } else { // if uncle's color is black if ( z == z -> p -> left ) { //when z is right child z = z -> p ; right_rotate ( z ); } z -> p -> color = 1 ; z -> p -> p -> color = 0 ; left_rotate ( z -> p -> p ); } } } this -> root -> color = 1 ; } void insert ( K key , V val ) { Node * z = new Node ( * this , key , val ); Node * y = NIL ; Node * x = this -> root ; while ( x != NIL ) { y = x ; if ( z -> key < x -> key ) { x = x -> left ; } else { x = x -> right ; } } z -> p = y ; if ( y == NIL ) { this -> root = z ; } else if ( z -> key < y -> key ) { y -> left = z ; } else { y -> right = z ; } // z->left = NIL; // z->right = ZIL; // z->color = 0; insert_fixup ( z ); } Node * find ( K key ) { Node * x = root ; while ( x != NIL && x -> key != key ) { if ( key < x -> key ) { x = x -> left ; } else { x = x -> right ; } } return x ; } Node * minimum ( Node * x ) { while ( x -> left != NIL ) { x = x -> left ; } return x ; } void erase_fixup ( Node * x ) { Node * w ; // sibling of x; while ( x != this -> root && x -> color == 1 ) { //only if x is black and not root if ( x == x -> p -> left ) { w = x -> p -> right ; if ( w -> color == 0 ) { // turns to case2, 3 or 4; w -> color = 1 ; x -> p -> color = 0 ; left_rotate ( x -> p ); w = x -> p -> right ; } if ( w -> left -> color == 1 && w -> right -> color == 1 ) { // case2 w -> color = 0 ; x = x -> p ; } else { if ( w -> right -> color == 1 ) { //case3 -> turns to case4 w -> left -> color = 1 ; w -> color = 0 ; right_rotate ( w ); w = x -> p -> right ; } w -> color = x -> p -> color ; // case4 -> we can make legit red-black tree x -> p -> color = 1 ; w -> right -> color = 1 ; left_rotate ( x -> p ); x = root ; } } else { // x == x->p->right w = x -> p -> left ; if ( w -> color == 0 ) { w -> color = 1 ; x -> p -> color = 0 ; right_rotate ( x -> p ); w = x -> p -> left ; } if ( w -> left -> color == 1 && w -> right -> color == 1 ) { w -> color = 0 ; x = x -> p ; } else { if ( w -> left -> color == 1 ) { w -> color = 0 ; w -> right -> color = 1 ; left_rotate ( w ); w = x -> p -> left ; } w -> color = w -> p -> color ; w -> p -> color = 1 ; w -> left -> color = 1 ; right_rotate ( x -> p ); x = root ; } } } x -> color = 1 ; } void erase ( Node * z ) { Node * y = z ; bool y_original_color = y -> color ; Node * x ; if ( z -> left == NIL ) { x = z -> right ; transplant ( z , z -> right ); } else if ( z -> right == NIL ) { x = z -> left ; transplant ( z , z -> left ); } else { y = minimum ( z -> right ); y_original_color = y -> color ; x = y -> right ; if ( y -> p == z ) { x -> p = y ; // incase of x is NIL!! we need to find it's parent! } else { transplant ( y , y -> right ); y -> right = z -> right ; y -> right -> p = y ; } transplant ( z , y ); y -> left = z -> left ; y -> left -> p = y ; y -> color = z -> color ; } if ( y_original_color == 1 ) { erase_fixup ( x ); } } void rb_printer ( Node * node , int indent ) { //prints red & black tree int count = 4 ; if ( node == NIL ) return ; indent += count ; rb_printer ( node -> right , indent ); cout << endl ; for ( int i = count ; i < indent ; i ++ ) { cout << \" \" ; } cout << ( node -> color == 0 ? \" \\033 [1;31m\" : \"\" ) << ( node == node -> p -> left ? \"l\" : \"r\" ) << node -> key << ( node -> color == 0 ? \" \\033 [0m\" : \"\" ) << endl ; rb_printer ( node -> left , indent ); } void print () { rb_printer ( this -> root , 0 ); } }; int main () { Map < int , int > m ; for ( int i = 0 ; i < 20 ; i ++ ) { m . insert ( rand () % 20 , 1 ); } m . print (); cout << \"deleting ---\" << endl ;; for ( int i = 0 ; i < 20 ; i ++ ) { int key = rand () % 20 ; auto it = m . find ( key ); cout << \"delete: \" << key << endl ; if ( it != m . NIL ) { cout << \"key exist... deleting...\" ; m . erase ( it ); m . print (); } else { cout << \"key not exist\" << endl ; } } return 0 ; }","title":"Implementation"},{"location":"DataStructures/Trees/RedBlackTree/#related_problems","text":"NOT ADDED YET","title":"Related Problems"},{"location":"DataStructures/Trees/RedBlackTree/#related_topics","text":"BinarySearchTree","title":"Related Topics"},{"location":"DataStructures/Trees/RedBlackTree/#analysis_optional","text":"You can add some mathematical things here using KaTex as a block tag $$ T(N) = O(N*M) $$ or as a inline tag $T(N) = O(N) $","title":"Analysis (Optional)"},{"location":"DataStructures/Trees/RedBlackTree/#contributers_optional","text":"08.18.2019 JCHRYS","title":"Contributers (Optional)"},{"location":"DataStructures/Trees/SearchTree/","text":"Search Tree \u00b6 The Search tree data structure supports many dynamic-set operations, including $SEARCH,\\ MINIMUM,\\ MAXIMUM,\\ PREDECESSOR,$ $ SUCCESSOR,\\ INSERT,\\ DELETE$ so we can use a search tree as a $dictionary$ and as a $priority\\ queue$ DataStructure \u00b6 We can represent it as linked objects Each Node Containing \u00b6 Each Node Contatining pointers : rightChild , leftChild , parent","title":"SearchTree"},{"location":"DataStructures/Trees/SearchTree/#search_tree","text":"The Search tree data structure supports many dynamic-set operations, including $SEARCH,\\ MINIMUM,\\ MAXIMUM,\\ PREDECESSOR,$ $ SUCCESSOR,\\ INSERT,\\ DELETE$ so we can use a search tree as a $dictionary$ and as a $priority\\ queue$","title":"Search Tree"},{"location":"DataStructures/Trees/SearchTree/#datastructure","text":"We can represent it as linked objects","title":"DataStructure"},{"location":"DataStructures/Trees/SearchTree/#each_node_containing","text":"Each Node Contatining pointers : rightChild , leftChild , parent","title":"Each Node Containing"},{"location":"Language/Class/","text":"Class \u00b6 C++ classes are a tool for creating new types that can be used conveniently as builtin types The Fundamental idea in defining a new type is to separate the details of the implementation from the properties essential to the correc use of it Brief Summary of classes \u00b6 A class is user-defined type A class consists of a set of members. The most common kinds of members are data members and member functions. Member functions can define the meaning of initialization, copy, move, and cleanup Members are accessed using . (dot) for objects and -> (arrow) for pointers. Operators, such as, + , ! , and [] , can be defined for a class A class is a namespace containing its members The public members provide the class's interface and the private members provide implementation details A struct is a class where members are by default public Class Basics \u00b6 class example \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class X { private : // the representation (implementation) is private int m ; public : // the user interface is public X ( int i = 0 ) : m { i } {} //a constructor (initialize the data member m) int mf ( int i ) { // a member function int old = m ; m = i ; //set a new value return old ; // return the old value } }; X var { 7 }; // a variable of type X, initialized to 7 int user ( X var , X * ptr ) { int x = var . mf ( 7 ); // access using . int y = ptr -> mf ( 9 ); // access using -> int z = var . m ; // error: cannot acces private member } 1. Member functions \u00b6 Functions declared within a class definition are called member functions 2. Default copying \u00b6 a class object can be initialized with a copy of an obejct of its class 1 2 UserClass c1 = c0 ; // initialization by copy UserClass c2 { d1 }; // initialization by copy 3. Access Control \u00b6 class is consist of two parts private part: can be used only by member functions , public part : interface to objects of class 4. class and struct \u00b6 a struct is a class in which members are by default public struct S{}; is simply short hand for class S{public: }; 5. Constructors \u00b6 a constructor is recognized by having the same name as the class it self. programmers can declare a function with the explicit purpose of initializing objects. 1 2 3 4 5 6 7 8 9 10 11 12 class Date { int d , m , y ; public : Date ( int dd , int mm , int yy ); // constructor } Date today = Date ( 23 , 6 , 1983 ); // OK Date xmas ( 25 , 12 , 1990 ); // OK -> abbreviated form Date my_birthday ; //error: initializer missing Date release1_0 ( 10 , 12 ) //error: third argument missing Date today = Date { 23 , 6 , 1982 } // good! I recommend the {} notation over the () notation for initializing, because it is explicit about what is being done we could use default values directly as default arguments 1 2 3 4 5 class Date { int d , m , y ; public : Date ( int dd = today . d , int mm = today . m , int yy = today . y ); // constructor }","title":"Class"},{"location":"Language/Class/#class","text":"C++ classes are a tool for creating new types that can be used conveniently as builtin types The Fundamental idea in defining a new type is to separate the details of the implementation from the properties essential to the correc use of it","title":"Class"},{"location":"Language/Class/#brief_summary_of_classes","text":"A class is user-defined type A class consists of a set of members. The most common kinds of members are data members and member functions. Member functions can define the meaning of initialization, copy, move, and cleanup Members are accessed using . (dot) for objects and -> (arrow) for pointers. Operators, such as, + , ! , and [] , can be defined for a class A class is a namespace containing its members The public members provide the class's interface and the private members provide implementation details A struct is a class where members are by default public","title":"Brief Summary of classes"},{"location":"Language/Class/#class_basics","text":"","title":"Class Basics"},{"location":"Language/Class/#class_example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class X { private : // the representation (implementation) is private int m ; public : // the user interface is public X ( int i = 0 ) : m { i } {} //a constructor (initialize the data member m) int mf ( int i ) { // a member function int old = m ; m = i ; //set a new value return old ; // return the old value } }; X var { 7 }; // a variable of type X, initialized to 7 int user ( X var , X * ptr ) { int x = var . mf ( 7 ); // access using . int y = ptr -> mf ( 9 ); // access using -> int z = var . m ; // error: cannot acces private member }","title":"class example"},{"location":"Language/Class/#1_member_functions","text":"Functions declared within a class definition are called member functions","title":"1. Member functions"},{"location":"Language/Class/#2_default_copying","text":"a class object can be initialized with a copy of an obejct of its class 1 2 UserClass c1 = c0 ; // initialization by copy UserClass c2 { d1 }; // initialization by copy","title":"2. Default copying"},{"location":"Language/Class/#3_access_control","text":"class is consist of two parts private part: can be used only by member functions , public part : interface to objects of class","title":"3. Access Control"},{"location":"Language/Class/#4_class_and_struct","text":"a struct is a class in which members are by default public struct S{}; is simply short hand for class S{public: };","title":"4. class and struct"},{"location":"Language/Class/#5_constructors","text":"a constructor is recognized by having the same name as the class it self. programmers can declare a function with the explicit purpose of initializing objects. 1 2 3 4 5 6 7 8 9 10 11 12 class Date { int d , m , y ; public : Date ( int dd , int mm , int yy ); // constructor } Date today = Date ( 23 , 6 , 1983 ); // OK Date xmas ( 25 , 12 , 1990 ); // OK -> abbreviated form Date my_birthday ; //error: initializer missing Date release1_0 ( 10 , 12 ) //error: third argument missing Date today = Date { 23 , 6 , 1982 } // good! I recommend the {} notation over the () notation for initializing, because it is explicit about what is being done we could use default values directly as default arguments 1 2 3 4 5 class Date { int d , m , y ; public : Date ( int dd = today . d , int mm = today . m , int yy = today . y ); // constructor }","title":"5. Constructors"},{"location":"Language/Keywords/","text":"Aliases \u00b6 Aliases are used when we want to insulate our code from details of the underlying machine. - note that naming a type after its representation than its purpose is not neccessarily a good idea. 1. typedef \u00b6 1 2 cpp typedef double decimal_places ; // is equivalent to \"using decimal_places = double;\" 2. using \u00b6 the using keyword can also be used to introduce a template alias. 1 2 template < typename T > using Vector = std :: vector < T , my_allocator < T >> but we cannot apply type specifiers, such as unsigned, to an alias. 1 2 3 using Char = char ; using Uchar = unsigned Char ; //error using Uchar = unsigned char ; // ok","title":"Keywords"},{"location":"Language/Keywords/#aliases","text":"Aliases are used when we want to insulate our code from details of the underlying machine. - note that naming a type after its representation than its purpose is not neccessarily a good idea.","title":"Aliases"},{"location":"Language/Keywords/#1_typedef","text":"1 2 cpp typedef double decimal_places ; // is equivalent to \"using decimal_places = double;\"","title":"1. typedef"},{"location":"Language/Keywords/#2_using","text":"the using keyword can also be used to introduce a template alias. 1 2 template < typename T > using Vector = std :: vector < T , my_allocator < T >> but we cannot apply type specifiers, such as unsigned, to an alias. 1 2 3 using Char = char ; using Uchar = unsigned Char ; //error using Uchar = unsigned char ; // ok","title":"2. using"},{"location":"Language/Preface/","text":"Why C++? \u00b6 What You Should Know Before.. \u00b6 you should be able to write C++ programs using components such as IOstreams and containers from C++ STL. You Should be also be familiar with the basic features of \"Modern C++\", such as auto, decltype, move semantics, and lambdas. c++17 modern C++ \u00b6 We will use number of these new features of modern C++ 1. C++11 \u00b6 Variadic templates Alias templates Move semantics, rvalue references, and perfect forwarding Standard type traits 2. C++14 \u00b6 Variable templates Generic Lambdas 3. C++17 \u00b6 Class template argument deduction Compile-time if Fold expressions Style Guide \u00b6 1. the order of constant qualifier. \u00b6 What is in front of const qualifier is always a constant 1 2 int const MAX_SIZE = 100 ; // the int is constant int * const P ; // the pointer cannot change, but int value can; 1 2 const int MAX_SIZE = 100 ; const int * P ; // you can not find what's constant value; reason1. easy to know what's constant. it's always what is in front of the const qualifier reason2. syntatical substitution principle. consider following example 1 2 3 4 5 6 7 typedef char * CHARS ; typedef CHARS const CPTR ; // constant pointer to chars // => typedef char * const CPTR ; using CHARS = char * : using CPTR = CHARS const ; // constant pointer to chars // => using CPTR = char * const ; The meaning of the second declaration is preseved when we textually replace CHARS with what it stands for; How ever if you write const before the type it qualifies. textually 1 2 3 typedef char * CHARS ; typedef const CHARS CTPR ; //const pointer to chars; // => typedef const char* CTPR // pointer to constant chars; footnote: note that typedef defines a \"type alias\" rather than a new type 1 2 3 4 typedef int newInt ; int i = 29 ; newInt j = 1999 ; i = j ; // OK 2. put the space between the & and the parameter name; \u00b6 by doing this, we emphasize the separation between the parameter type and the parameter name. 1 void foo ( int const & x ); 3. avoid declaring multiple entities in this way!. \u00b6 1 char * a , b ; according to the rules inherited from C, a is a pointer but b is an ordinary char ;","title":"Preface"},{"location":"Language/Preface/#why_c","text":"","title":"Why C++?"},{"location":"Language/Preface/#what_you_should_know_before","text":"you should be able to write C++ programs using components such as IOstreams and containers from C++ STL. You Should be also be familiar with the basic features of \"Modern C++\", such as auto, decltype, move semantics, and lambdas. c++17","title":"What You Should Know Before.."},{"location":"Language/Preface/#modern_c","text":"We will use number of these new features of modern C++","title":"modern C++"},{"location":"Language/Preface/#1_c11","text":"Variadic templates Alias templates Move semantics, rvalue references, and perfect forwarding Standard type traits","title":"1. C++11"},{"location":"Language/Preface/#2_c14","text":"Variable templates Generic Lambdas","title":"2. C++14"},{"location":"Language/Preface/#3_c17","text":"Class template argument deduction Compile-time if Fold expressions","title":"3. C++17"},{"location":"Language/Preface/#style_guide","text":"","title":"Style Guide"},{"location":"Language/Preface/#1_the_order_of_constant_qualifier","text":"What is in front of const qualifier is always a constant 1 2 int const MAX_SIZE = 100 ; // the int is constant int * const P ; // the pointer cannot change, but int value can; 1 2 const int MAX_SIZE = 100 ; const int * P ; // you can not find what's constant value; reason1. easy to know what's constant. it's always what is in front of the const qualifier reason2. syntatical substitution principle. consider following example 1 2 3 4 5 6 7 typedef char * CHARS ; typedef CHARS const CPTR ; // constant pointer to chars // => typedef char * const CPTR ; using CHARS = char * : using CPTR = CHARS const ; // constant pointer to chars // => using CPTR = char * const ; The meaning of the second declaration is preseved when we textually replace CHARS with what it stands for; How ever if you write const before the type it qualifies. textually 1 2 3 typedef char * CHARS ; typedef const CHARS CTPR ; //const pointer to chars; // => typedef const char* CTPR // pointer to constant chars; footnote: note that typedef defines a \"type alias\" rather than a new type 1 2 3 4 typedef int newInt ; int i = 29 ; newInt j = 1999 ; i = j ; // OK","title":"1. the order of constant qualifier."},{"location":"Language/Preface/#2_put_the_space_between_the_amp_and_the_parameter_name","text":"by doing this, we emphasize the separation between the parameter type and the parameter name. 1 void foo ( int const & x );","title":"2. put the space between the &amp; and the parameter name;"},{"location":"Language/Preface/#3_avoid_declaring_multiple_entities_in_this_way","text":"1 char * a , b ; according to the rules inherited from C, a is a pointer but b is an ordinary char ;","title":"3. avoid declaring multiple entities in this way!."},{"location":"MISC/Lemma/","text":"Lemma \u00b6 Lemma-1 \u00b6 if we run $dfs(root)$ in a rooted tree, then v is an ancestor of $u$ if and only if $st_v\\leq st_u\\leq ft_u\\leq ft_v$. Lemma-2 \u00b6","title":"Lemma"},{"location":"MISC/Lemma/#lemma","text":"","title":"Lemma"},{"location":"MISC/Lemma/#lemma-1","text":"if we run $dfs(root)$ in a rooted tree, then v is an ancestor of $u$ if and only if $st_v\\leq st_u\\leq ft_u\\leq ft_v$.","title":"Lemma-1"},{"location":"MISC/Lemma/#lemma-2","text":"","title":"Lemma-2"}]}