{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"THIS SITE IS UNDER CONSTRUCTION \u00b6 Welcome to ICPC.NINJA \u00b6 SHARE & GET useful problem solving techniques All Articles in this site is implemented on c++17 Why I created this site? \u00b6 Ipsum nemo veritatis illum nulla veniam. Aut esse blanditiis placeat omnis culpa. Odit iusto repudiandae dolorem molestiae labore. Sed a dolores iusto esse placeat officia Obcaecati corporis perspiciatis sequi consequatur. How I created this site? \u00b6 Dolor eveniet incidunt esse nostrum dolor Cumque totam veniam dolore culpa sapiente Repudiandae velit odit magni commodi quasi? Voluptates officiis ab veniam eos totam Dolorem commodi deleniti unde adipisci nihil How to contribute \u00b6 Dolor vitae soluta praesentium obcaecati minus Enim consequuntur expedita voluptatibus eius dignissimos Praesentium provident commodi natus quisquam amet! Eum illo nisi laboriosam rem veritatis Excepturi sed assumenda libero qui accusamus? emojis \u00b6","title":"Home"},{"location":"#this_site_is_under_construction","text":"","title":"THIS SITE IS UNDER CONSTRUCTION"},{"location":"#welcome_to_icpcninja","text":"SHARE & GET useful problem solving techniques All Articles in this site is implemented on c++17","title":"Welcome to ICPC.NINJA"},{"location":"#why_i_created_this_site","text":"Ipsum nemo veritatis illum nulla veniam. Aut esse blanditiis placeat omnis culpa. Odit iusto repudiandae dolorem molestiae labore. Sed a dolores iusto esse placeat officia Obcaecati corporis perspiciatis sequi consequatur.","title":"Why I created this site?"},{"location":"#how_i_created_this_site","text":"Dolor eveniet incidunt esse nostrum dolor Cumque totam veniam dolore culpa sapiente Repudiandae velit odit magni commodi quasi? Voluptates officiis ab veniam eos totam Dolorem commodi deleniti unde adipisci nihil","title":"How I created this site?"},{"location":"#how_to_contribute","text":"Dolor vitae soluta praesentium obcaecati minus Enim consequuntur expedita voluptatibus eius dignissimos Praesentium provident commodi natus quisquam amet! Eum illo nisi laboriosam rem veritatis Excepturi sed assumenda libero qui accusamus?","title":"How to contribute"},{"location":"#emojis","text":"","title":"emojis"},{"location":"Algorithms/EuclideanAlgorithm/","text":"Euclidean Algorithm \u00b6 Euclidean algorithm, or Euclid's algorithm, is an efficient method for computing the GCD(greatest common divisor) Implementation \u00b6 1 2 3 4 int gcd ( int a , int b ) { if ( ! a ) return b ; return gcd ( b % a , a ); } Proof \u00b6 This proof consists of two parts. - part 1: proves that Euclidean Algorithm gives us a common factor of two integers. - part 2: proves that the common divisor that Euclidean Algorithm produces is the largest possible Part1 \u00b6 Euclidean Algorithm gives us a common factor of two integers($a, b$). $$ \\begin{aligned} a & = q_1b + r_1, &\\text{where}(0<r<b)& \\\\ b & = q_2r_1 + r_2, &\\text{where}(0<r_2<r_1)& \\\\ r_1 & = q_3r_2 + r_3, &\\text{where}(0<r_3<r_2)&\\\\ &\\vdots \\\\ r_i & = q_{i+2}r_{i+1} + r_{i+2}, & \\text{where}( 0 < r_{i+2} < r_{i+1})& \\\\ &\\vdots \\\\ r_{k-2} & = q_{k}r_{k-1} + r_{k}\\\\ r_{k-1} & = q_{k+1}r_k \\\\ \\end{aligned} $$ From the last euqation, we know that $r_k|r_{k-1}$. So, we know that we can express $r_{k-1} = cr_k$. where $c$ is an integer. Now consider the previous equation. $$ \\begin{aligned} r_{k-2} & = q_{k} r_{k-1} + r_{k} \\\\ & = q_{k} cr_{k} + r_{k} \\\\ & = r_{k} (q_{k}c + 1) \\\\ \\end{aligned} $$ Thus, we have that $r_{k} | r_{k-2}$. In our equation previous to that one, we have $$ r_{k-3} = q_{k-1} r_{k-2} + r_{k-1} $$ From here, since $r_{k} | r_{k-1}$ and $r_{k}| r{k-2}$, using our rules of divisibility we have that $r_{k} | r_{k-3}$. As you can see, we can continue this process, considering each previous equation until we get to the last two, where we will find that $r_{k} | a$ and $r_{k} | b$. Thus, we find that Euclids algorithm gives us a common factor of a and b. Part2 \u00b6 The common divisor Euclidean Algorithm produces is the largest possible. We will start by assuming that $a$ and $b$ have a common factor $d$, and then show that $d | r_{k}$. consider an arbitrary common factor d of $a$ and $b$. If $d$ is a common factor, we can rewrite $a$ and $b$ as follows: $$ a = d a^{\\prime}, b = d b^{\\prime}, \\text{where } d, a, b \\text{ are all positive integers } $$ Now, consider the first euqation from Euclidean algorithm: $$ \\begin{aligned} a & = q_{1} b + r_{1} \\\\ r_{1} & = a - q_{1}b \\\\ & = da^{\\prime} - q_{1} d b^{\\prime} \\\\ & = d(a^{\\prime} - q_{1}b^{\\prime}) \\end{aligned} $$ Thus, we have that $d|r_{1}$. Now, consider the second equation, and repeat the steps we did on the first, this time solving for $r_{2}$ $$ \\begin{aligned} b & = q_{2}r_{1} + r_{2} \\\\ r_{2} & = b - q_{2}r_{1} \\\\ & = d b^{\\prime} - q_{2}dr_1^{\\prime} \\\\ & = d (b^{\\prime} - q_{2} r_1^{\\prime}) \\end{aligned} $$ As you can see, we can continue this process through each of the equations until we hit the second to last one, where we will have $$ \\begin{aligned} r_{k-2} & = q_{k}r_{k-1} + r_{k} \\\\ r_{k} & = q_{k}r_{k-1} - r_{k-2} \\\\ & = q_{k}dr_{k-1}^{\\prime} - d r_{k-2}^{\\prime} \\\\ & = d(q_{k}r_{k-1}^{\\prime} - r_{k-2}^{\\prime}) \\end{aligned} $$ Thus, $d| r_k$ But this says that any arbitrary common factor of $a$ and $b$ that we originally picked divides into $r_{k}$, the value that Euclidean algorithm produced. Since we know that $r_{k}$ is a common factor to both $a$ and $b$, this shows that is must be the largest possible common factor, or $gcd(a, b)$ $\\blacksquare$ Extended Euclidean Algorithm \u00b6 Given integers $a$ and $b$, there is always an integral solution to the equation $$ ax + by = gcd(a, b) $$ and we can find the values of $x$ and $y$. implementation \u00b6 not yet Proof \u00b6 Consider writing down the steps of Euclidean Algorithm $$ \\begin{aligned} a & = q_1b + r_1, &\\text{where}(0<r<b)& \\\\ b & = q_2r_1 + r_2, &\\text{where}(0<r_2<r_1)& \\\\ r_1 & = q_3r_2 + r_3, &\\text{where}(0<r_3<r_2)&\\\\ &\\vdots \\\\ r_i & = q_{i+2}r_{i+1} + r_{i+2}, & \\text{where}( 0 < r_{i+2} < r_{i+1})& \\\\ &\\vdots \\\\ r_{k-2} & = q_{k}r_{k-1} + r_{k} & \\text{where}(0 < r_k < r_{k-1}) \\\\ r_{k-1} & = q_{k+1}r_k \\\\ \\end{aligned} $$ Consider solving the second to last euqation for $r_k$. You get $$ \\begin{aligned} r_{k-2} & = q_{k}r_{k-1} + r_{k} \\\\ r_{k} & = r_{k-2} - q_{k}r_{k-1} \\\\ gcd(a, b) & = r_{k-2} - q_{k}r_{k-1} \\end{aligned} $$ Now, solve the previous equation for $r_{k-1}$ $$ \\begin{aligned} r_{k-3} & = q_{k-1}r_{k-2} + r_{k-1} \\\\ r_{k-1} & = r_{k-3} - q_{k-1}r_{k-2} \\\\ \\end{aligned} $$ and we substitute this value in to the previous derived equation $$ \\begin{aligned} gcd(a, b) & = r_{k-2} - q_{k}r_{k-1} \\\\ & = r_{k-2} - q_k(r_{k-3} - q_{k-1}r_{k-2}) \\\\ & = r_{k-2}(1 - q_{k-1}) - q_kr_{k-3} \\end{aligned} $$ Notice that now we have expressed $gcd(a, b)$ as a linear combination of $r_{k-2}$ and $r_{k-3}$. Next we can substitute for of $r_{k-2}$ in terms of $r_{k-3}$ and $r_{k-4}$, so that the $gcd(a, b)$ can be expressed as the linear combination of $r_{k-3}$ and $r_{k-4}$. Eventually, by continuing this process, $gcd(a, b)$ will be expressed as a linear combination of $a$ and $b$ as desired. This process will be much easier to see with examples: Find integers $x$ and $y$ such that $$ 135x + 50y = 5 $$ Use Euclidean Algorithm to compute $gcd(135, 50)$ $$ \\begin{aligned} 135 & = 2 \\sdot 50 + 35 \\cdots\\text{\\textcircled 1}\\\\ 50 & = 1 \\sdot 35 + 15 \\cdots\\text{\\textcircled 2}\\\\ 35 & = 2 \\sdot 15 + 5 \\cdots\\text{\\textcircled 3}\\\\ 15 & = 3 \\sdot 5 \\end{aligned} $$ Now, let's use the Extended Euclidean algorithm to solve the problem $ 5 = 35 - 2 \\sdot 15 $ from equation 3 But, we have that $ 15 = 50 - 35 $ from euqation $\\text{\\textcircled 2}$ Now we, substitute this value into the previously derived equation: $$ \\begin{aligned} 5 & = 35 - 2 \\sdot 15 \\\\ 5 & = 35 - 2 \\sdot (50 - 35) \\\\ 5 & = 3 \\sdot 35 - 2 \\sdot 50 \\end{aligned} $$ Now, finally use the first equation to determine an expression for $35$ as a linear combination of $135$ and $50$ $$ 35 = 135 - 2 \\sdot 50 \\text{ from equation}\\text{\\textcircled 1} $$ Plug this into our last euqation: $$ \\begin{aligned} 5 & = 3 \\sdot 35 - 2 \\sdot 50 \\\\ 5 & = 3 \\sdot (135 - 2 \\sdot 50) - 2 \\sdot 50 \\\\ 5 & = 3 \\sdot 135 - 8 \\sdot 50 \\end{aligned} $$ So, a set of solutions to the equation is $x=3, y = -8$ This article is from 'COT3100Euclid01' \u00b6","title":"Euclidean"},{"location":"Algorithms/EuclideanAlgorithm/#euclidean_algorithm","text":"Euclidean algorithm, or Euclid's algorithm, is an efficient method for computing the GCD(greatest common divisor)","title":"Euclidean Algorithm"},{"location":"Algorithms/EuclideanAlgorithm/#implementation","text":"1 2 3 4 int gcd ( int a , int b ) { if ( ! a ) return b ; return gcd ( b % a , a ); }","title":"Implementation"},{"location":"Algorithms/EuclideanAlgorithm/#proof","text":"This proof consists of two parts. - part 1: proves that Euclidean Algorithm gives us a common factor of two integers. - part 2: proves that the common divisor that Euclidean Algorithm produces is the largest possible","title":"Proof"},{"location":"Algorithms/EuclideanAlgorithm/#part1","text":"Euclidean Algorithm gives us a common factor of two integers($a, b$). $$ \\begin{aligned} a & = q_1b + r_1, &\\text{where}(0<r<b)& \\\\ b & = q_2r_1 + r_2, &\\text{where}(0<r_2<r_1)& \\\\ r_1 & = q_3r_2 + r_3, &\\text{where}(0<r_3<r_2)&\\\\ &\\vdots \\\\ r_i & = q_{i+2}r_{i+1} + r_{i+2}, & \\text{where}( 0 < r_{i+2} < r_{i+1})& \\\\ &\\vdots \\\\ r_{k-2} & = q_{k}r_{k-1} + r_{k}\\\\ r_{k-1} & = q_{k+1}r_k \\\\ \\end{aligned} $$ From the last euqation, we know that $r_k|r_{k-1}$. So, we know that we can express $r_{k-1} = cr_k$. where $c$ is an integer. Now consider the previous equation. $$ \\begin{aligned} r_{k-2} & = q_{k} r_{k-1} + r_{k} \\\\ & = q_{k} cr_{k} + r_{k} \\\\ & = r_{k} (q_{k}c + 1) \\\\ \\end{aligned} $$ Thus, we have that $r_{k} | r_{k-2}$. In our equation previous to that one, we have $$ r_{k-3} = q_{k-1} r_{k-2} + r_{k-1} $$ From here, since $r_{k} | r_{k-1}$ and $r_{k}| r{k-2}$, using our rules of divisibility we have that $r_{k} | r_{k-3}$. As you can see, we can continue this process, considering each previous equation until we get to the last two, where we will find that $r_{k} | a$ and $r_{k} | b$. Thus, we find that Euclids algorithm gives us a common factor of a and b.","title":"Part1"},{"location":"Algorithms/EuclideanAlgorithm/#part2","text":"The common divisor Euclidean Algorithm produces is the largest possible. We will start by assuming that $a$ and $b$ have a common factor $d$, and then show that $d | r_{k}$. consider an arbitrary common factor d of $a$ and $b$. If $d$ is a common factor, we can rewrite $a$ and $b$ as follows: $$ a = d a^{\\prime}, b = d b^{\\prime}, \\text{where } d, a, b \\text{ are all positive integers } $$ Now, consider the first euqation from Euclidean algorithm: $$ \\begin{aligned} a & = q_{1} b + r_{1} \\\\ r_{1} & = a - q_{1}b \\\\ & = da^{\\prime} - q_{1} d b^{\\prime} \\\\ & = d(a^{\\prime} - q_{1}b^{\\prime}) \\end{aligned} $$ Thus, we have that $d|r_{1}$. Now, consider the second equation, and repeat the steps we did on the first, this time solving for $r_{2}$ $$ \\begin{aligned} b & = q_{2}r_{1} + r_{2} \\\\ r_{2} & = b - q_{2}r_{1} \\\\ & = d b^{\\prime} - q_{2}dr_1^{\\prime} \\\\ & = d (b^{\\prime} - q_{2} r_1^{\\prime}) \\end{aligned} $$ As you can see, we can continue this process through each of the equations until we hit the second to last one, where we will have $$ \\begin{aligned} r_{k-2} & = q_{k}r_{k-1} + r_{k} \\\\ r_{k} & = q_{k}r_{k-1} - r_{k-2} \\\\ & = q_{k}dr_{k-1}^{\\prime} - d r_{k-2}^{\\prime} \\\\ & = d(q_{k}r_{k-1}^{\\prime} - r_{k-2}^{\\prime}) \\end{aligned} $$ Thus, $d| r_k$ But this says that any arbitrary common factor of $a$ and $b$ that we originally picked divides into $r_{k}$, the value that Euclidean algorithm produced. Since we know that $r_{k}$ is a common factor to both $a$ and $b$, this shows that is must be the largest possible common factor, or $gcd(a, b)$ $\\blacksquare$","title":"Part2"},{"location":"Algorithms/EuclideanAlgorithm/#extended_euclidean_algorithm","text":"Given integers $a$ and $b$, there is always an integral solution to the equation $$ ax + by = gcd(a, b) $$ and we can find the values of $x$ and $y$.","title":"Extended Euclidean Algorithm"},{"location":"Algorithms/EuclideanAlgorithm/#implementation_1","text":"not yet","title":"implementation"},{"location":"Algorithms/EuclideanAlgorithm/#proof_1","text":"Consider writing down the steps of Euclidean Algorithm $$ \\begin{aligned} a & = q_1b + r_1, &\\text{where}(0<r<b)& \\\\ b & = q_2r_1 + r_2, &\\text{where}(0<r_2<r_1)& \\\\ r_1 & = q_3r_2 + r_3, &\\text{where}(0<r_3<r_2)&\\\\ &\\vdots \\\\ r_i & = q_{i+2}r_{i+1} + r_{i+2}, & \\text{where}( 0 < r_{i+2} < r_{i+1})& \\\\ &\\vdots \\\\ r_{k-2} & = q_{k}r_{k-1} + r_{k} & \\text{where}(0 < r_k < r_{k-1}) \\\\ r_{k-1} & = q_{k+1}r_k \\\\ \\end{aligned} $$ Consider solving the second to last euqation for $r_k$. You get $$ \\begin{aligned} r_{k-2} & = q_{k}r_{k-1} + r_{k} \\\\ r_{k} & = r_{k-2} - q_{k}r_{k-1} \\\\ gcd(a, b) & = r_{k-2} - q_{k}r_{k-1} \\end{aligned} $$ Now, solve the previous equation for $r_{k-1}$ $$ \\begin{aligned} r_{k-3} & = q_{k-1}r_{k-2} + r_{k-1} \\\\ r_{k-1} & = r_{k-3} - q_{k-1}r_{k-2} \\\\ \\end{aligned} $$ and we substitute this value in to the previous derived equation $$ \\begin{aligned} gcd(a, b) & = r_{k-2} - q_{k}r_{k-1} \\\\ & = r_{k-2} - q_k(r_{k-3} - q_{k-1}r_{k-2}) \\\\ & = r_{k-2}(1 - q_{k-1}) - q_kr_{k-3} \\end{aligned} $$ Notice that now we have expressed $gcd(a, b)$ as a linear combination of $r_{k-2}$ and $r_{k-3}$. Next we can substitute for of $r_{k-2}$ in terms of $r_{k-3}$ and $r_{k-4}$, so that the $gcd(a, b)$ can be expressed as the linear combination of $r_{k-3}$ and $r_{k-4}$. Eventually, by continuing this process, $gcd(a, b)$ will be expressed as a linear combination of $a$ and $b$ as desired. This process will be much easier to see with examples: Find integers $x$ and $y$ such that $$ 135x + 50y = 5 $$ Use Euclidean Algorithm to compute $gcd(135, 50)$ $$ \\begin{aligned} 135 & = 2 \\sdot 50 + 35 \\cdots\\text{\\textcircled 1}\\\\ 50 & = 1 \\sdot 35 + 15 \\cdots\\text{\\textcircled 2}\\\\ 35 & = 2 \\sdot 15 + 5 \\cdots\\text{\\textcircled 3}\\\\ 15 & = 3 \\sdot 5 \\end{aligned} $$ Now, let's use the Extended Euclidean algorithm to solve the problem $ 5 = 35 - 2 \\sdot 15 $ from equation 3 But, we have that $ 15 = 50 - 35 $ from euqation $\\text{\\textcircled 2}$ Now we, substitute this value into the previously derived equation: $$ \\begin{aligned} 5 & = 35 - 2 \\sdot 15 \\\\ 5 & = 35 - 2 \\sdot (50 - 35) \\\\ 5 & = 3 \\sdot 35 - 2 \\sdot 50 \\end{aligned} $$ Now, finally use the first equation to determine an expression for $35$ as a linear combination of $135$ and $50$ $$ 35 = 135 - 2 \\sdot 50 \\text{ from equation}\\text{\\textcircled 1} $$ Plug this into our last euqation: $$ \\begin{aligned} 5 & = 3 \\sdot 35 - 2 \\sdot 50 \\\\ 5 & = 3 \\sdot (135 - 2 \\sdot 50) - 2 \\sdot 50 \\\\ 5 & = 3 \\sdot 135 - 8 \\sdot 50 \\end{aligned} $$ So, a set of solutions to the equation is $x=3, y = -8$","title":"Proof"},{"location":"Algorithms/EuclideanAlgorithm/#this_article_is_from_cot3100euclid01","text":"","title":"This article is from 'COT3100Euclid01'"},{"location":"Algorithms/FastPower/","text":"Fast Power Algorithm \u00b6 FAST! time complexity $\\Omicron(pow)$ Implementation \u00b6 Recursive \u00b6 1 2 3 4 5 6 7 int power ( int base , int pow ) { if ( ! pow ) return 1 ; if ( n & 1 ) // if n is odd return base * power ( base , ( pow - 1 ) >> 1 ) * power ( base , ( pow - 1 ) >> 1 ); return power ( base , pow >> 1 ) * power ( base , pow >> 1 ); } Iterative \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 int power ( int base , int pow ) { if ( ! pow ) return 1 ; int result = 1 ; while ( pow ) { if ( pow & 1 ) { result *= base ; pow -- ; // not necessary } pow >>= 1 ; base *= base ; } return result ; } How it works? \u00b6","title":"FastPower"},{"location":"Algorithms/FastPower/#fast_power_algorithm","text":"FAST! time complexity $\\Omicron(pow)$","title":"Fast Power Algorithm"},{"location":"Algorithms/FastPower/#implementation","text":"","title":"Implementation"},{"location":"Algorithms/FastPower/#recursive","text":"1 2 3 4 5 6 7 int power ( int base , int pow ) { if ( ! pow ) return 1 ; if ( n & 1 ) // if n is odd return base * power ( base , ( pow - 1 ) >> 1 ) * power ( base , ( pow - 1 ) >> 1 ); return power ( base , pow >> 1 ) * power ( base , pow >> 1 ); }","title":"Recursive"},{"location":"Algorithms/FastPower/#iterative","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 int power ( int base , int pow ) { if ( ! pow ) return 1 ; int result = 1 ; while ( pow ) { if ( pow & 1 ) { result *= base ; pow -- ; // not necessary } pow >>= 1 ; base *= base ; } return result ; }","title":"Iterative"},{"location":"Algorithms/FastPower/#how_it_works","text":"","title":"How it works?"},{"location":"Algorithms/Graph/Basics/","text":"Graph \u00b6 A graph is consists of nodes and edges Path: A path leads from node a to node b through edges of the graph. Length: The length of a path is the number of edges in it. Cycle: A path is a cycle if the first and last node is the same. Simple: A path is simple if each node appears at most once in the path. Terminologies \u00b6 1. Connectivity \u00b6 A graph is connected if there is a path between any two nodes. Components: Connected parts of its grpah. Tree: a tree is a connected graph that consists of $n$ nodes and $n-1$ edges. 2. Edge directions \u00b6 A graph is directed if the edges can be traversed in one direction only 3. Edge weights \u00b6 In a weighted graph, each edge is assigned a weight. often interpreted as edge length 4. Neighbors and degrees \u00b6 Two nodes are $neighbors$ or $adjacent$ if there is an edge between them Degree: The degrege of a node is number of its neighbors Indegree: The indegree of a node is the number of edges that end at the node Outdegree: The outdegree of a node is the number of edges that start at the node Regular graph: A graph is regular if the degree of every node is a constant d Complete graph: A graph is complete if the degree of every node is $n-1$ Colorings \u00b6 In a coloring of a graph, each node is assigned a color so that no adjacent nodes have the same color Note It turns out that a graph is bipartite exactly when it does not contain a cycle with an odd number of edges Simplicity \u00b6 A graph is simple if no edge starts and ends at the same node(loop), and there are no multiple edges between two nodes. Graph representation \u00b6 There are several ways to represent graphs in algorithms. The choice of a data structure depends on the size of graph and the way the algorithm processes it 1. Adjacency list representation \u00b6 In the adjacency list representation, each node x in the graph is assigned an adjacency list that consists of nodes to which there is an edge from x we can efficiently find the nodes to which we can move from a given node through an edge 2. Adjacency matrix representation \u00b6 An adjacency matrix is two dimensional array that indicates which edges the graph contains. we can efficiently check from an adjacency matrix if there is an edge between two nodes. 3. Edge list representation \u00b6 An edge list contains all edges of a graph in some order. This is convenient way to represent a graph if the algorithm proesses all edges of the graph and it is not needed to find edges that start at a given node.","title":"Basics"},{"location":"Algorithms/Graph/Basics/#graph","text":"A graph is consists of nodes and edges Path: A path leads from node a to node b through edges of the graph. Length: The length of a path is the number of edges in it. Cycle: A path is a cycle if the first and last node is the same. Simple: A path is simple if each node appears at most once in the path.","title":"Graph"},{"location":"Algorithms/Graph/Basics/#terminologies","text":"","title":"Terminologies"},{"location":"Algorithms/Graph/Basics/#1_connectivity","text":"A graph is connected if there is a path between any two nodes. Components: Connected parts of its grpah. Tree: a tree is a connected graph that consists of $n$ nodes and $n-1$ edges.","title":"1. Connectivity"},{"location":"Algorithms/Graph/Basics/#2_edge_directions","text":"A graph is directed if the edges can be traversed in one direction only","title":"2. Edge directions"},{"location":"Algorithms/Graph/Basics/#3_edge_weights","text":"In a weighted graph, each edge is assigned a weight. often interpreted as edge length","title":"3. Edge weights"},{"location":"Algorithms/Graph/Basics/#4_neighbors_and_degrees","text":"Two nodes are $neighbors$ or $adjacent$ if there is an edge between them Degree: The degrege of a node is number of its neighbors Indegree: The indegree of a node is the number of edges that end at the node Outdegree: The outdegree of a node is the number of edges that start at the node Regular graph: A graph is regular if the degree of every node is a constant d Complete graph: A graph is complete if the degree of every node is $n-1$","title":"4. Neighbors and degrees"},{"location":"Algorithms/Graph/Basics/#colorings","text":"In a coloring of a graph, each node is assigned a color so that no adjacent nodes have the same color Note It turns out that a graph is bipartite exactly when it does not contain a cycle with an odd number of edges","title":"Colorings"},{"location":"Algorithms/Graph/Basics/#simplicity","text":"A graph is simple if no edge starts and ends at the same node(loop), and there are no multiple edges between two nodes.","title":"Simplicity"},{"location":"Algorithms/Graph/Basics/#graph_representation","text":"There are several ways to represent graphs in algorithms. The choice of a data structure depends on the size of graph and the way the algorithm processes it","title":"Graph representation"},{"location":"Algorithms/Graph/Basics/#1_adjacency_list_representation","text":"In the adjacency list representation, each node x in the graph is assigned an adjacency list that consists of nodes to which there is an edge from x we can efficiently find the nodes to which we can move from a given node through an edge","title":"1. Adjacency list representation"},{"location":"Algorithms/Graph/Basics/#2_adjacency_matrix_representation","text":"An adjacency matrix is two dimensional array that indicates which edges the graph contains. we can efficiently check from an adjacency matrix if there is an edge between two nodes.","title":"2. Adjacency matrix representation"},{"location":"Algorithms/Graph/Basics/#3_edge_list_representation","text":"An edge list contains all edges of a graph in some order. This is convenient way to represent a graph if the algorithm proesses all edges of the graph and it is not needed to find edges that start at a given node.","title":"3. Edge list representation"},{"location":"Algorithms/Graph/DeBruijnSequences/","text":"De Bruijn sequences \u00b6 A De Bruijn sequence is a string that contains every string of length $n$ exactly once as a substring, for fixed alphabet of $k$ characters. The length of such a string is $k^n + n - 1$ characters. It turns out that each De Bruijn sequence corresponds to an Eulerian path in a graph. The idea is to construct a graph where each node contains a string of $n-1$ characters and each edge adds one character to the string. An Eulerian path in this graph corresponds to a string that contains all strings of length $n$. The string contains the characters of the starting node and all characters of the edges. The starting node has $n-1$ characters and there are $k^n$ characters in the edges, so the length of the string is $k^n + n - 1$.","title":"De Bruijn sequences"},{"location":"Algorithms/Graph/DeBruijnSequences/#de_bruijn_sequences","text":"A De Bruijn sequence is a string that contains every string of length $n$ exactly once as a substring, for fixed alphabet of $k$ characters. The length of such a string is $k^n + n - 1$ characters. It turns out that each De Bruijn sequence corresponds to an Eulerian path in a graph. The idea is to construct a graph where each node contains a string of $n-1$ characters and each edge adds one character to the string. An Eulerian path in this graph corresponds to a string that contains all strings of length $n$. The string contains the characters of the starting node and all characters of the edges. The starting node has $n-1$ characters and there are $k^n$ characters in the edges, so the length of the string is $k^n + n - 1$.","title":"De Bruijn sequences"},{"location":"Algorithms/Graph/DirectedGraphs/","text":"Directed graphs \u00b6 Acyclic graphs($\\text{DAGs}$): There are no cycles in the graph, so there is no path from any node to itself. Successor graphs: The out degree of each node is 1, so each node has a unique successor. Topological sorting \u00b6 A Topological sort is an ordering of the nodes of a directed graph such that if there is a path from node $a$ to node $b$, then node $a$ appears before node $b$ in the ordering. An acyclic graph always has a topological sort. Note It turns out that depth-first search($\\text{DFS}$) can be used to both check if a directed graph contains a cycle and, if it does not contain a cycle, to construct a topological sort. Implementation(Using DFS) \u00b6 The idea is to go through the nodes of the graph and always begin a depth-first search at the current node if it has not been processed yet. during the the searches the node have three possible states: state 0: the node has not been processed state 1: the node is under processing state 2: the node has been processed Initially, the state of each node is 0. When a search reaches a node for the first time, its state becomes1. Finally, after all successors of the node have been processed, its state becomes 2. If the graph contains a cycle, we will find this out during the search, because sooner or later we wil arrive at a node whose state is 1. In this case, it is not possible to construct a topological sort. If the graph does not contain a cycle, we can construct a topological sort by adding each node to a list when the state of the node becomes 2. This list in reverse order is topological sort. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 vector < int > adj []; // adjancency list int color []; // array: color of nodes vector < int > topsort bool cycle = false ; void dfs ( int u ) { if ( color [ u ] == 2 ) return ; //already processed if ( color [ u ] == 1 ) { //cycle detected cycle = true ; return ; } color [ u ] = 1 ; // color as under processing for ( auto v : adj [ u ]) dfs ( v ); color [ u ] == 2 topsort . push_back ( u ); } for ( int i = 0 ; i < # edges ; i ++ ) { if ( color [ i ] == 0 ) // if node is not processed yet dfs ( i ); } if ( cycle ) { cout << \"cycle detected\" << endl ; } else { for ( auto it = topsort . rbegin (); it != topsort . rend (); it ++ ) { cout << * it << ' ' ; } cout << endl ; } Related Problems \u00b6 \uc791\uc5c5\uc21c\uc11c Dynamic Programming \u00b6 If a directed graph is acyclic, dynamic programming can be applied to it. For example, we can efficiently solve the following problems concerning paths from a starting node to and ending node 1. Counting the number of paths \u00b6 Let paths(x) denote the number of paths from node $1$ to node $x$. As a basecase, paths(1) 1. Then to calculate other values of paths(x) , we may use recursion 1 paths ( x ) = paths ( a1 ) + paths ( a2 ) + ... + paths ( ak ) where a1, a2, ..., ak are the nodes from which there is an edge to x. Since the graph is acyclic the values of paths(x) can be calculated in the order of topological sort. 2. Extending Dijkstra's algorithm \u00b6 A by-product of Dijkstra's algorithm is a directed, acyclic graph that indicates for each node of the original graph the possible ways to reach the node using a shortest path from the starting node. Dynamic programming can be applied to that graph. thus we can find number of shortest paths from node $a$ to node $b$ 3. Representing problems as graphs \u00b6 Actually, any dynamic programming problem can be represented as a directed acyclic graph. In such a graph, each node coressponds to a dynamic programming state and the edges indicate how the states depend on each other. Successor paths \u00b6 Successor graphs: the outdegree of each node is 1. A successor graph consists of one or more components, each of which contains one cycle and some paths that lead to it. Successor graphs are sometimes called functional graphs. The reason for that is that any successor graph corresponds to a function that defines the edges of the graph. The parameter of the function is a node of the graph, and the function gives the successor of that node. succ(x, k) \u00b6 Since each node of a successor graph has a unique successor, we can also define a function succ(x, k) that gives the node that we will reach if we begin at node x and k step forward. Using preprocessing, any value of succ(x, k) can be calculated only $\\Omicron(lgk)$ time. The idea is to precalculate all values of succ(x, k) where k is a power of two and at most u , where u is the maximum number of steps we will ever walk. This can be efficiently done, because we can use the following recursion 1 2 3 4 5 6 int succ ( x , k ) { if ( k == 1 ) { return succ ( x ) ; } succ ( succ ( x , k / 2 ) , k / 2 ) ; } Precalculating the values takes $\\Omicron(nlgu)$ time because $\\Omicron(lgu)$ values are calculated for each node. After precalculating, any value of succ(x, k) can be calculating presenting the number of steps k as a sum of pwoers of two. $$ succ(x, 11) = succ(succ(succ(x, 8), 2), 1); $$ Such a representation always consists of $\\Omicron(lgk)$ parts, so calculating a value of succ(x, k) takes $\\Omicron(lgk)$ time. Cycle Detection \u00b6 Consider a successor graph that only contains a path that ends in a cycle. A simple way to detect the cycle is to walk in the graph and keep track of all nodes that have been visited. Once a node is visited for the second time we can conclude that the node is the first node in the cycle. This method works in $\\Omicron(n)$ time and also uses $\\Omciron(n)$ memory. There are better algorithms for cycle detection. The time complexity of such algorithm is still $\\Omicron(n)$, but they only use $\\Omicron(1)$ memory. This is an important improvement if $n$ is large. Floyd's algorithm \u00b6 Floyd's algorithm walks forward in the graph using two pointers $a$ and $b$. Both pointers begin at node $x$ that is the starting node of the graph. Then on each turn, the pointer $a$ walks one step forward and the pointer $b$ walks to steps forward. The process continues until the pointer meet each other. 1 2 3 4 5 6 a = succ ( x ); b = succ ( succ ( x )); while ( a != b ) { a = succ ( a ); b = succ ( succ ( b )); } At this point, the pointer $a$ has walked $k$ steps and the pointer $b$ has walked $2k$ steps, so the length of the cycle divides $k$. Thus, the first node that belongs to the cycle can be found by moving the pointer $a$ to node $x$ and advancing the pointers step by step until they meet again. 1 2 3 4 5 6 a = x ; while ( a != b ) { a = succ ( a ); b = succ ( b ); } first = a ; After this, the length of the cycle can be calculated as follows 1 2 3 4 5 6 b = succ ( a ); length = 1 ; while ( a != b ) { b = succ ( b ); length ++ ; }","title":"Directed Graphs"},{"location":"Algorithms/Graph/DirectedGraphs/#directed_graphs","text":"Acyclic graphs($\\text{DAGs}$): There are no cycles in the graph, so there is no path from any node to itself. Successor graphs: The out degree of each node is 1, so each node has a unique successor.","title":"Directed graphs"},{"location":"Algorithms/Graph/DirectedGraphs/#topological_sorting","text":"A Topological sort is an ordering of the nodes of a directed graph such that if there is a path from node $a$ to node $b$, then node $a$ appears before node $b$ in the ordering. An acyclic graph always has a topological sort. Note It turns out that depth-first search($\\text{DFS}$) can be used to both check if a directed graph contains a cycle and, if it does not contain a cycle, to construct a topological sort.","title":"Topological sorting"},{"location":"Algorithms/Graph/DirectedGraphs/#implementationusing_dfs","text":"The idea is to go through the nodes of the graph and always begin a depth-first search at the current node if it has not been processed yet. during the the searches the node have three possible states: state 0: the node has not been processed state 1: the node is under processing state 2: the node has been processed Initially, the state of each node is 0. When a search reaches a node for the first time, its state becomes1. Finally, after all successors of the node have been processed, its state becomes 2. If the graph contains a cycle, we will find this out during the search, because sooner or later we wil arrive at a node whose state is 1. In this case, it is not possible to construct a topological sort. If the graph does not contain a cycle, we can construct a topological sort by adding each node to a list when the state of the node becomes 2. This list in reverse order is topological sort. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 vector < int > adj []; // adjancency list int color []; // array: color of nodes vector < int > topsort bool cycle = false ; void dfs ( int u ) { if ( color [ u ] == 2 ) return ; //already processed if ( color [ u ] == 1 ) { //cycle detected cycle = true ; return ; } color [ u ] = 1 ; // color as under processing for ( auto v : adj [ u ]) dfs ( v ); color [ u ] == 2 topsort . push_back ( u ); } for ( int i = 0 ; i < # edges ; i ++ ) { if ( color [ i ] == 0 ) // if node is not processed yet dfs ( i ); } if ( cycle ) { cout << \"cycle detected\" << endl ; } else { for ( auto it = topsort . rbegin (); it != topsort . rend (); it ++ ) { cout << * it << ' ' ; } cout << endl ; }","title":"Implementation(Using DFS)"},{"location":"Algorithms/Graph/DirectedGraphs/#related_problems","text":"\uc791\uc5c5\uc21c\uc11c","title":"Related Problems"},{"location":"Algorithms/Graph/DirectedGraphs/#dynamic_programming","text":"If a directed graph is acyclic, dynamic programming can be applied to it. For example, we can efficiently solve the following problems concerning paths from a starting node to and ending node","title":"Dynamic Programming"},{"location":"Algorithms/Graph/DirectedGraphs/#1_counting_the_number_of_paths","text":"Let paths(x) denote the number of paths from node $1$ to node $x$. As a basecase, paths(1) 1. Then to calculate other values of paths(x) , we may use recursion 1 paths ( x ) = paths ( a1 ) + paths ( a2 ) + ... + paths ( ak ) where a1, a2, ..., ak are the nodes from which there is an edge to x. Since the graph is acyclic the values of paths(x) can be calculated in the order of topological sort.","title":"1. Counting the number of paths"},{"location":"Algorithms/Graph/DirectedGraphs/#2_extending_dijkstras_algorithm","text":"A by-product of Dijkstra's algorithm is a directed, acyclic graph that indicates for each node of the original graph the possible ways to reach the node using a shortest path from the starting node. Dynamic programming can be applied to that graph. thus we can find number of shortest paths from node $a$ to node $b$","title":"2. Extending Dijkstra's algorithm"},{"location":"Algorithms/Graph/DirectedGraphs/#3_representing_problems_as_graphs","text":"Actually, any dynamic programming problem can be represented as a directed acyclic graph. In such a graph, each node coressponds to a dynamic programming state and the edges indicate how the states depend on each other.","title":"3. Representing problems as graphs"},{"location":"Algorithms/Graph/DirectedGraphs/#successor_paths","text":"Successor graphs: the outdegree of each node is 1. A successor graph consists of one or more components, each of which contains one cycle and some paths that lead to it. Successor graphs are sometimes called functional graphs. The reason for that is that any successor graph corresponds to a function that defines the edges of the graph. The parameter of the function is a node of the graph, and the function gives the successor of that node.","title":"Successor paths"},{"location":"Algorithms/Graph/DirectedGraphs/#succx_k","text":"Since each node of a successor graph has a unique successor, we can also define a function succ(x, k) that gives the node that we will reach if we begin at node x and k step forward. Using preprocessing, any value of succ(x, k) can be calculated only $\\Omicron(lgk)$ time. The idea is to precalculate all values of succ(x, k) where k is a power of two and at most u , where u is the maximum number of steps we will ever walk. This can be efficiently done, because we can use the following recursion 1 2 3 4 5 6 int succ ( x , k ) { if ( k == 1 ) { return succ ( x ) ; } succ ( succ ( x , k / 2 ) , k / 2 ) ; } Precalculating the values takes $\\Omicron(nlgu)$ time because $\\Omicron(lgu)$ values are calculated for each node. After precalculating, any value of succ(x, k) can be calculating presenting the number of steps k as a sum of pwoers of two. $$ succ(x, 11) = succ(succ(succ(x, 8), 2), 1); $$ Such a representation always consists of $\\Omicron(lgk)$ parts, so calculating a value of succ(x, k) takes $\\Omicron(lgk)$ time.","title":"succ(x, k)"},{"location":"Algorithms/Graph/DirectedGraphs/#cycle_detection","text":"Consider a successor graph that only contains a path that ends in a cycle. A simple way to detect the cycle is to walk in the graph and keep track of all nodes that have been visited. Once a node is visited for the second time we can conclude that the node is the first node in the cycle. This method works in $\\Omicron(n)$ time and also uses $\\Omciron(n)$ memory. There are better algorithms for cycle detection. The time complexity of such algorithm is still $\\Omicron(n)$, but they only use $\\Omicron(1)$ memory. This is an important improvement if $n$ is large.","title":"Cycle Detection"},{"location":"Algorithms/Graph/DirectedGraphs/#floyds_algorithm","text":"Floyd's algorithm walks forward in the graph using two pointers $a$ and $b$. Both pointers begin at node $x$ that is the starting node of the graph. Then on each turn, the pointer $a$ walks one step forward and the pointer $b$ walks to steps forward. The process continues until the pointer meet each other. 1 2 3 4 5 6 a = succ ( x ); b = succ ( succ ( x )); while ( a != b ) { a = succ ( a ); b = succ ( succ ( b )); } At this point, the pointer $a$ has walked $k$ steps and the pointer $b$ has walked $2k$ steps, so the length of the cycle divides $k$. Thus, the first node that belongs to the cycle can be found by moving the pointer $a$ to node $x$ and advancing the pointers step by step until they meet again. 1 2 3 4 5 6 a = x ; while ( a != b ) { a = succ ( a ); b = succ ( b ); } first = a ; After this, the length of the cycle can be calculated as follows 1 2 3 4 5 6 b = succ ( a ); length = 1 ; while ( a != b ) { b = succ ( b ); length ++ ; }","title":"Floyd's algorithm"},{"location":"Algorithms/Graph/EulerianPaths/","text":"Eulerian Path \u00b6 An Eulerian Path is a path that goes through each edge exactly one. It turns out that there is a simple rule that determines whether a graph contains an Eulerian path, and there is also an efficient algorithm to find a path if it exists. Existence \u00b6 The existence of Eulerian paths and circuits depends on the degrees of the nodes. In undireccted graph \u00b6 an undirected graph has an Eulerian path exactly when \"all the edges belong to the same connected component\" and Case 1: the degree of each node is even Case 2: the degree of exactly two nodes is odd, and the degree of all other nodes is even In the Case 1, each Eulerian path is also Eulerian circuit. In the Case2, the odd-degree nodes are the starting and ending nodes of an eulerian path which is not an Eulerian circuit. Directed graph \u00b6 In a directed graph, we focus on indegrees and outdegrees of the nodes. A directed graph contains an Eulerian path exactly when \"all the edges belong to the same connected component\" and Case 1: in each node, the indegree equals the outdegree Case 2: in one node, the indegree is one larger than the outdegree, in another node, the outdegree is one larger than the indegree, and in all other nodes, the indegree equals the out degree. In Case 1, each Eulerian path is also an Eulerian circuit. In case 2, the graph contains an Eulerian path that begins at the node whose out degree is larger and ends at the node whose indegree is larger. Hierholzer's Algorithm \u00b6 Hierholzer's algorithm is an efficient method for constructing an Eulerian circuit. The algorithm consists of several rounds, each of which adds new edges to the circuit. Of course, we assume that the graph contains an Eulerian circuit; otherwise Hierholzer's algorithm cannot find it First, the algorithm constructs a circuit that contains some of the edges of the graph. After this, the algorithm extends the circuit step by step by adding subcircuits to it. The process continues until all edges have been added to the circuit. The algorithm extends the circuit by always finding a node $x$ that belong to the circuit but has an outgoing edge that is not included in the circuit. The algorithm constructs a new path from node $x$ that only contains edges that are not yet in the circuit. Sooner or later, the path will return to node $x$, which creates a subcircuit If the graph only contains an Eulerian path, we can still use Hierholzer's algorithm to find it by adding an extra edge to the graph and removing the edge after the circuit has been constructed. For example, in an undirected graph, we add the extra edge between the two odd-degree nodes. Implementation \u00b6 --- will be added---","title":"Eulerian Paths"},{"location":"Algorithms/Graph/EulerianPaths/#eulerian_path","text":"An Eulerian Path is a path that goes through each edge exactly one. It turns out that there is a simple rule that determines whether a graph contains an Eulerian path, and there is also an efficient algorithm to find a path if it exists.","title":"Eulerian Path"},{"location":"Algorithms/Graph/EulerianPaths/#existence","text":"The existence of Eulerian paths and circuits depends on the degrees of the nodes.","title":"Existence"},{"location":"Algorithms/Graph/EulerianPaths/#in_undireccted_graph","text":"an undirected graph has an Eulerian path exactly when \"all the edges belong to the same connected component\" and Case 1: the degree of each node is even Case 2: the degree of exactly two nodes is odd, and the degree of all other nodes is even In the Case 1, each Eulerian path is also Eulerian circuit. In the Case2, the odd-degree nodes are the starting and ending nodes of an eulerian path which is not an Eulerian circuit.","title":"In undireccted graph"},{"location":"Algorithms/Graph/EulerianPaths/#directed_graph","text":"In a directed graph, we focus on indegrees and outdegrees of the nodes. A directed graph contains an Eulerian path exactly when \"all the edges belong to the same connected component\" and Case 1: in each node, the indegree equals the outdegree Case 2: in one node, the indegree is one larger than the outdegree, in another node, the outdegree is one larger than the indegree, and in all other nodes, the indegree equals the out degree. In Case 1, each Eulerian path is also an Eulerian circuit. In case 2, the graph contains an Eulerian path that begins at the node whose out degree is larger and ends at the node whose indegree is larger.","title":"Directed graph"},{"location":"Algorithms/Graph/EulerianPaths/#hierholzers_algorithm","text":"Hierholzer's algorithm is an efficient method for constructing an Eulerian circuit. The algorithm consists of several rounds, each of which adds new edges to the circuit. Of course, we assume that the graph contains an Eulerian circuit; otherwise Hierholzer's algorithm cannot find it First, the algorithm constructs a circuit that contains some of the edges of the graph. After this, the algorithm extends the circuit step by step by adding subcircuits to it. The process continues until all edges have been added to the circuit. The algorithm extends the circuit by always finding a node $x$ that belong to the circuit but has an outgoing edge that is not included in the circuit. The algorithm constructs a new path from node $x$ that only contains edges that are not yet in the circuit. Sooner or later, the path will return to node $x$, which creates a subcircuit If the graph only contains an Eulerian path, we can still use Hierholzer's algorithm to find it by adding an extra edge to the graph and removing the edge after the circuit has been constructed. For example, in an undirected graph, we add the extra edge between the two odd-degree nodes.","title":"Hierholzer's Algorithm"},{"location":"Algorithms/Graph/EulerianPaths/#implementation","text":"--- will be added---","title":"Implementation"},{"location":"Algorithms/Graph/GraphTraversal/","text":"Graph Traversal \u00b6 We will cover two fundamental graph algorithms. depth-first search & breadth-first search. BFS vs DFS Both algorithms are given starting node in the graph and they visit all nodes that can be reached from the starting node. The difference in algorithms is the order in which they visit the nodes. Depth-first search($\\text{DFS}$) \u00b6 Depth-first search always follows a single path in the graph as long as it find new nodes. After this, it returns to previous nodes and begin to explore other parts of the graph. The algorithm keeps track of visited nodes, so that it processes each node only once Implementation \u00b6 Using adjacency lists in an array maintain an array visited[N] 1 2 3 4 5 6 7 8 9 10 11 vector < int > adj [ N ]; //adjacency lists bool visited [ N ]; void dfs ( int s ) { //starting node s if ( visited [ s ]) return ; visited [ s ] = true ; // process node for ( auto u : adj [ s ]) { dfs ( u ); } } Breadth-first search($\\text{BFS}$) \u00b6 Breadth-first search visits the nodes in increasing order of their distance from the starting node. Thus, we can calculate the distance from the starting node to all other nodes using $BFS$. Implementation \u00b6 Typical implementation is based on a queue that contains nodes. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 queue < int > q ; bool visited [ N ]; int distance [ N ]; visited [ s ] = true ; // starting node s distance [ s ] = 0 ; q . push ( s ); while ( ! q . empty ()) { int cur = q . front (); q . pop (); // process node for ( auto next : adj [ cur ]) { if ( visited [ next ]) continue ; visited [ next ] = true ; distance [ next ] = distance [ cur ] + 1 ; q . push ( next ); } } Applications \u00b6 You can use any of both to check properties of graph but in practice, depth-first search is a better choice, because of ease of implementation 1. Connectivity check \u00b6 A graph is connected if there is a path between any two nodes of the graph Implementation \u00b6 Connected Graph: If a search did not visit all the nodes, we can conclude that the graph is not connected Find all components of Graph: iterating through the nodes and always starting a new depth-first search if the current node does not belong to any component yet 2. Finding cycles \u00b6 Implementation \u00b6 Way1: A graph contains a cycle if during a graph traversal, we find a node whose neighbor (other than the previous node in the current path) has already been visited Way2(math): if a component contains c nodes and no cycle, it must contain exactly c-1 edges. if there are c or more edges, the component surely contains a cycle 3. Bipartiteness check \u00b6 A graph is bipartite if its nodes can be colored using two colors so that there are no adjacent nodes with the same color Implementation \u00b6 The idea is to color the starting node blue, all its neighbors red, all their neighbors blue, and so on. If at some point of the search we notice that two adjacent nodes have the same color, this means that the graph is not bipartite. Otherwise, the graph is bipartite. Why it works? This algorithm always works, because when there are only two colors available, the color of the starting node in a component determines the colors of all other nodes in the component NP-hard Note that in the general case, it is difficult to find out if the nodes in a graph can be colored using $k$ colors so that no adjacent nodes have the same color. Even when $k=3$, no efficient algorithm is known but the problem is NP-hard","title":"Graph Traversal"},{"location":"Algorithms/Graph/GraphTraversal/#graph_traversal","text":"We will cover two fundamental graph algorithms. depth-first search & breadth-first search. BFS vs DFS Both algorithms are given starting node in the graph and they visit all nodes that can be reached from the starting node. The difference in algorithms is the order in which they visit the nodes.","title":"Graph Traversal"},{"location":"Algorithms/Graph/GraphTraversal/#depth-first_searchtextdfs","text":"Depth-first search always follows a single path in the graph as long as it find new nodes. After this, it returns to previous nodes and begin to explore other parts of the graph. The algorithm keeps track of visited nodes, so that it processes each node only once","title":"Depth-first search($\\text{DFS}$)"},{"location":"Algorithms/Graph/GraphTraversal/#implementation","text":"Using adjacency lists in an array maintain an array visited[N] 1 2 3 4 5 6 7 8 9 10 11 vector < int > adj [ N ]; //adjacency lists bool visited [ N ]; void dfs ( int s ) { //starting node s if ( visited [ s ]) return ; visited [ s ] = true ; // process node for ( auto u : adj [ s ]) { dfs ( u ); } }","title":"Implementation"},{"location":"Algorithms/Graph/GraphTraversal/#breadth-first_searchtextbfs","text":"Breadth-first search visits the nodes in increasing order of their distance from the starting node. Thus, we can calculate the distance from the starting node to all other nodes using $BFS$.","title":"Breadth-first search($\\text{BFS}$)"},{"location":"Algorithms/Graph/GraphTraversal/#implementation_1","text":"Typical implementation is based on a queue that contains nodes. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 queue < int > q ; bool visited [ N ]; int distance [ N ]; visited [ s ] = true ; // starting node s distance [ s ] = 0 ; q . push ( s ); while ( ! q . empty ()) { int cur = q . front (); q . pop (); // process node for ( auto next : adj [ cur ]) { if ( visited [ next ]) continue ; visited [ next ] = true ; distance [ next ] = distance [ cur ] + 1 ; q . push ( next ); } }","title":"Implementation"},{"location":"Algorithms/Graph/GraphTraversal/#applications","text":"You can use any of both to check properties of graph but in practice, depth-first search is a better choice, because of ease of implementation","title":"Applications"},{"location":"Algorithms/Graph/GraphTraversal/#1_connectivity_check","text":"A graph is connected if there is a path between any two nodes of the graph","title":"1. Connectivity check"},{"location":"Algorithms/Graph/GraphTraversal/#implementation_2","text":"Connected Graph: If a search did not visit all the nodes, we can conclude that the graph is not connected Find all components of Graph: iterating through the nodes and always starting a new depth-first search if the current node does not belong to any component yet","title":"Implementation"},{"location":"Algorithms/Graph/GraphTraversal/#2_finding_cycles","text":"","title":"2. Finding cycles"},{"location":"Algorithms/Graph/GraphTraversal/#implementation_3","text":"Way1: A graph contains a cycle if during a graph traversal, we find a node whose neighbor (other than the previous node in the current path) has already been visited Way2(math): if a component contains c nodes and no cycle, it must contain exactly c-1 edges. if there are c or more edges, the component surely contains a cycle","title":"Implementation"},{"location":"Algorithms/Graph/GraphTraversal/#3_bipartiteness_check","text":"A graph is bipartite if its nodes can be colored using two colors so that there are no adjacent nodes with the same color","title":"3. Bipartiteness check"},{"location":"Algorithms/Graph/GraphTraversal/#implementation_4","text":"The idea is to color the starting node blue, all its neighbors red, all their neighbors blue, and so on. If at some point of the search we notice that two adjacent nodes have the same color, this means that the graph is not bipartite. Otherwise, the graph is bipartite. Why it works? This algorithm always works, because when there are only two colors available, the color of the starting node in a component determines the colors of all other nodes in the component NP-hard Note that in the general case, it is difficult to find out if the nodes in a graph can be colored using $k$ colors so that no adjacent nodes have the same color. Even when $k=3$, no efficient algorithm is known but the problem is NP-hard","title":"Implementation"},{"location":"Algorithms/Graph/HamiltonianPaths/","text":"Hamiltonian Paths \u00b6 A Hamiltonian path is a path that visits each node of the graph exactly once. If a Hamiltonian path begins and ends at the same node, it is called Hamiltonian circuit. Existence \u00b6 No efficient method is known for testing if a graph contains Hamiltonian path, and the problem is NP-hard Still, in some special cases, we can be certain that a graph contains a Hamiltonian path. A simple observation is that if the graph is complete. It also contains a Hamiltonian path. Dirac's theorem: If a degree of each node is at least $n/2$, the graph contains a Hamiltonian path Ore's theorem: If the sum of degrees of each non-adjacent pair of nodes is at least $n$, the graph contains a Hamiltonian path. A common property in these theorems and other results is that they guarantee the existence of a Hamiltonian path if the graph has a large number of edges. Construction \u00b6 Since there is no efficient way to check if a Hamiltonian path exists, it is clear that there is also no method to efficiently construct the path, because otherwise we could just try to construct the path and see whether it exists. A simple way to search for a Hamiltonian path is use backtracking algorithm that goes through all possible ways to construct the path. The time complexity of such an algorithm is at least $\\Omicron(n!)$, because there are $n!$ different ways to choose the order of $n$ nodes. A more efficient solution is based on dynamic programming. The idea is to calculate values of a function possible(S, x) , where $S$ is a subset of nodes and $x$ is one of the nodes. The function indicates whether there is a hamiltonian path that visits the nodes of $S$ and ends at node $x$. It is possible to implement this solution in $\\Omicron(2^nn^2)$ time. Implementation \u00b6 Related subjects \u00b6 Kinght's tours","title":"Hamiltonian Paths"},{"location":"Algorithms/Graph/HamiltonianPaths/#hamiltonian_paths","text":"A Hamiltonian path is a path that visits each node of the graph exactly once. If a Hamiltonian path begins and ends at the same node, it is called Hamiltonian circuit.","title":"Hamiltonian Paths"},{"location":"Algorithms/Graph/HamiltonianPaths/#existence","text":"No efficient method is known for testing if a graph contains Hamiltonian path, and the problem is NP-hard Still, in some special cases, we can be certain that a graph contains a Hamiltonian path. A simple observation is that if the graph is complete. It also contains a Hamiltonian path. Dirac's theorem: If a degree of each node is at least $n/2$, the graph contains a Hamiltonian path Ore's theorem: If the sum of degrees of each non-adjacent pair of nodes is at least $n$, the graph contains a Hamiltonian path. A common property in these theorems and other results is that they guarantee the existence of a Hamiltonian path if the graph has a large number of edges.","title":"Existence"},{"location":"Algorithms/Graph/HamiltonianPaths/#construction","text":"Since there is no efficient way to check if a Hamiltonian path exists, it is clear that there is also no method to efficiently construct the path, because otherwise we could just try to construct the path and see whether it exists. A simple way to search for a Hamiltonian path is use backtracking algorithm that goes through all possible ways to construct the path. The time complexity of such an algorithm is at least $\\Omicron(n!)$, because there are $n!$ different ways to choose the order of $n$ nodes. A more efficient solution is based on dynamic programming. The idea is to calculate values of a function possible(S, x) , where $S$ is a subset of nodes and $x$ is one of the nodes. The function indicates whether there is a hamiltonian path that visits the nodes of $S$ and ends at node $x$. It is possible to implement this solution in $\\Omicron(2^nn^2)$ time.","title":"Construction"},{"location":"Algorithms/Graph/HamiltonianPaths/#implementation","text":"","title":"Implementation"},{"location":"Algorithms/Graph/HamiltonianPaths/#related_subjects","text":"Kinght's tours","title":"Related subjects"},{"location":"Algorithms/Graph/KnightsTours/","text":"Knight's tours \u00b6 A knight's tour is sequence of moves of a knight on an $n \\times n$ chess board following the rules of chess such that the knight visits each square exactly once. A knight's tour is closed if the knight finally returns to the starting square and otherwise it is called open tour. A knight's tour corresponds to a Hamiltonian path in a graph whose nodes represent the suqare of the board, and two nodes are connected with an edge if a knight can move between the squares according to the rules of chess. A natural way to construct a knight's tour is to use backtracking. The search can be made more fficient by using heuristics that attempt to guide the knight so that a complete tour will be found quickly Polynomial algorithms There are also polynomial algorithms for finding knight's tours, but they are more complicated. Warnsdorf's rule \u00b6 Warnsdorf's rule is a simple and effective heruistic for finding a knight's tour. Using the rule, it is possible to efficiently construct a tour even on a large board . The idea is to always move the knight so that it ends up in a square where the number of possible moves is as small as possible. Related Topic \u00b6 [Hamiltonian Paths] Related Problems \u00b6 \ub098\uc774\ud2b8 \ud22c\uc5b4 \ub098\uc774\ud2b8 \ud22c\uc5b4","title":"Knight's tours"},{"location":"Algorithms/Graph/KnightsTours/#knights_tours","text":"A knight's tour is sequence of moves of a knight on an $n \\times n$ chess board following the rules of chess such that the knight visits each square exactly once. A knight's tour is closed if the knight finally returns to the starting square and otherwise it is called open tour. A knight's tour corresponds to a Hamiltonian path in a graph whose nodes represent the suqare of the board, and two nodes are connected with an edge if a knight can move between the squares according to the rules of chess. A natural way to construct a knight's tour is to use backtracking. The search can be made more fficient by using heuristics that attempt to guide the knight so that a complete tour will be found quickly Polynomial algorithms There are also polynomial algorithms for finding knight's tours, but they are more complicated.","title":"Knight's tours"},{"location":"Algorithms/Graph/KnightsTours/#warnsdorfs_rule","text":"Warnsdorf's rule is a simple and effective heruistic for finding a knight's tour. Using the rule, it is possible to efficiently construct a tour even on a large board . The idea is to always move the knight so that it ends up in a square where the number of possible moves is as small as possible.","title":"Warnsdorf's rule"},{"location":"Algorithms/Graph/KnightsTours/#related_topic","text":"[Hamiltonian Paths]","title":"Related Topic"},{"location":"Algorithms/Graph/KnightsTours/#related_problems","text":"\ub098\uc774\ud2b8 \ud22c\uc5b4 \ub098\uc774\ud2b8 \ud22c\uc5b4","title":"Related Problems"},{"location":"Algorithms/Graph/StrongConnectivity/","text":"Strong Connectivity(DAGs) \u00b6 A graph is strongly connected if there is a path from any node to all other nodes in the graph The strongly connected components of a graph divide the graph into strongly connected parts that are as large as possible. The strongly connected components form an acyclic component graph that represents deep structure of original graph. A component graph is a cyclic, directed graph, so it is easior to process than the original graph. Since the graph does not contain cycles, we can always construct a topological sort and use dynamic programming techniques. Kosaraju's algorithm \u00b6 Kosaraju's algorithm is an efficient method for finding the strongly connected components of a directed graph. The algorithm performs two depth-first search the first search constructs a list of nodes according to the structure of the graph the second search forms the strongly connected components. 1. Search 1 \u00b6 The first phase of Kosaraju's algorithm constructs a list of nodes in the order in which a depth-first search processes them. The algorithm goes through the nodes, and begins a depth-first search at each unprocessed node. Each node will be added to the list after it has been processed. 2. Search 2 \u00b6 The second phase of the algorithm forms the strongly connected components of the graph. First, the algorithm reverses every edge in the graph. This guarantees that during second search, we will always find strongly connected components that do not have extra nodes. After this, the algorithm goes through the list of nodes created by the first search, in reverse order. if a node does not belong to a component, the algorithm creates a new component and starts a depth-first search that adds all new nodes found durin g the search to the new component leak Note that since all edges are reversed, the component does not leak \"leak\" to other pars in the graph. Implementation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 vector < int > adj [ V ]; //adjancecy lists vector < int > radj [ V ]; // reversed adjacency list (directions are opposite to original) vector < int > order ; vector < vector < int >> result ; bool processed [ V ]; void search1 ( int u ) { if ( processed [ u ]) return ; processed [ u ] = true ; for ( auto v : adj [ u ]) search1 ( v ); order . push_back ( u ); } void search2 ( int u ) { if ( processed [ u ]) return ; processed [ u ] = true ; for ( auto v : radj [ u ]) search2 ( v ); result [ result . size () - 1 ]. push_back ( u ); //add current node to the last component } int main () { for ( int i = 0 ; i < V ; i ++ ) { // V = number of nodes if ( ! processed [ i ]) search1 ( i ); } reverse ( order . begin (), order . end ()) // reverse the order that created by the first search for ( int i = 0 ; i < V ; i ++ ) // initialized processed array processed [ i ] = false ; for ( auto u : order ) if ( ! processed [ u ]) result . push_back ( vector < int > ()); // add new component search2 ( i ); } Time complexity \u00b6 $\\Omicron(n+m)$ because the algorithm performs two depth-first searches. 2SAT problem \u00b6 Strong connectivity is also linked with the 2SAT problem . In this problem, we are given a logical formula for example $$ L_1 = (x_2 \\lor \\neg x_1) \\land (\\neg x_1 \\lor \\neg x_2) \\land (x_1 \\lor x_3) \\land (\\neg x_2 \\lor \\neg x_3) \\land (x_1 \\lor x_4) $$ is true when the variables are assigned as follows $$ \\begin{aligned} x_1 & = false \\\\ x_2 & = false \\\\ x_3 & = true \\\\ x_4 & = true \\\\ \\end{aligned} $$ The 2SAT problem can be represented as a graph whose nodes correspond to variable $x_i$ and negations $\\neg x_i$, and edges determine the connections between the variables. Each pair $(a_i \\lor b_i)$ generates two edges: $\\neg a_i \\rarr b_i$ and $\\neg b_i \\rarr a_i$. This means that if $a_i$ does not hold, $b_i$ must hold, and vice versa. The structure of the graph tells us whether it is possible to assign the values of the variables so that the formula is true. It turns out that this can be done exactly when there are no nodes $x_i$ and $\\neg x_i$ such that both nodes belong to the same strongly connected component. If there are such nodes, the graph contains a path from $x_i$ to $\\neg x_i$ and also a path from $\\neg x_i$ to $x_i$, so both $x_i$ and $\\neg x_i$ should be true which is not possible. In the graph of the formula $L_1$ there are no nodes $x_i$ and $\\neg x_i$ such that both nodes belong to the same strongly connected component, so solution exists. If a solution exists, the values for the variables can be found by going through the nodes of the component graph in a reverse topological sort order. at each step, we process a component that does not contain edges that lead to an unprocessed component. If the variables in the component have not been assigned values, their values will be determined accoring to the values in the component, and if they already have values, they remain unchanged. The process continues until each variable has been assigned a value. this method works because the graph has a special structure: if there are paths from node $x_i$ to node $x_j$ and from node $x_j$ to node $\\neg x_j$, then node $x_i$ never becomes true. The reason for this is that there is also a path from node $\\neg x_j$ to node $\\neg x_j$, and both $x_i$ and $x_j$ become false. Info A more difficult problem is 3SAT problem , where each part of the formula is form $(a_i \\lor b_i \\lor c_i)$. This problem is NP-hard, so no efficient algorithm for solving the problem is known. Implementation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 int const MAXN ; // maximum number of variables int N ; // number of variables int M ; // number of conditions vector < vector < int >> adj ; // adjancecy list vector < vector < int >> radj ; //inver adjancency list vector < bool > processed ( 2 * MAXN ); vector < bool > rprocessed ( 2 * MAXN ); vector < int > sccnum ( 2 * MAXN , - 1 ); // initialize -1 vector < vector < int >> scc ; vector < int > order ; // maintain order created by search 1 vector < int > answer ( 2 * MAXN ); void search1 ( int u ) { if ( processed [ u ]) return ; processed [ u ] = true ; for ( auto v : adj [ u ]) search1 ( v ); order . push_back ( u ); } void search2 ( int u ) { if ( rprocessed [ u ]) return ; rprocessed [ u ] = true ; for ( auto v : radj [ u ]) search2 ( v ); scc [ scc . size () - 1 ]. push_back ( u ); sccnum [ u ] = scc . size () - 1 ; // write which SCC node 'u' is in } int neg ( int i ) { // negate of i return ( i + N ) % ( 2 * N ); } int main () { for ( int i = 0 ; i < 2 * N ; i ++ ) { // to 2 * N because we have to consider neg. if ( ! processed [ i ]) search1 ( i ); } reverse ( order . begin (), order . end ()); for ( auto u : order ) { if ( ! rprocessed [ u ]) { scc . push_back ( vector < int > ()); search2 ( u ); } } for ( int i = 0 ; i < N ; i ++ ) { if ( sccnum [ i ] == sccnum [ neg ( i )]) { cout << \"WE FOUND CONTRADICTION!!!\" ; return 0 ; } } reverse ( scc . begin (), scc . end ()); // reverse order of topological order of SCC for ( auto component : scc ) { for ( auto x : component ) { if ( answer [ x ] == - 1 ) { // if not assigned yet answer [ x ] = true ; answer [ neg ( x )] = false ; } } } for ( int i = N ; i < 2 * N ; i ++ ) { cout << i << \": \" << answer [ i ] << endl ; } } Related Problems \u00b6 2-SAT 1 2-SAT 2 2-SAT 3 2-SAT 4","title":"Strong Connectivity"},{"location":"Algorithms/Graph/StrongConnectivity/#strong_connectivitydags","text":"A graph is strongly connected if there is a path from any node to all other nodes in the graph The strongly connected components of a graph divide the graph into strongly connected parts that are as large as possible. The strongly connected components form an acyclic component graph that represents deep structure of original graph. A component graph is a cyclic, directed graph, so it is easior to process than the original graph. Since the graph does not contain cycles, we can always construct a topological sort and use dynamic programming techniques.","title":"Strong Connectivity(DAGs)"},{"location":"Algorithms/Graph/StrongConnectivity/#kosarajus_algorithm","text":"Kosaraju's algorithm is an efficient method for finding the strongly connected components of a directed graph. The algorithm performs two depth-first search the first search constructs a list of nodes according to the structure of the graph the second search forms the strongly connected components.","title":"Kosaraju's algorithm"},{"location":"Algorithms/Graph/StrongConnectivity/#1_search_1","text":"The first phase of Kosaraju's algorithm constructs a list of nodes in the order in which a depth-first search processes them. The algorithm goes through the nodes, and begins a depth-first search at each unprocessed node. Each node will be added to the list after it has been processed.","title":"1. Search 1"},{"location":"Algorithms/Graph/StrongConnectivity/#2_search_2","text":"The second phase of the algorithm forms the strongly connected components of the graph. First, the algorithm reverses every edge in the graph. This guarantees that during second search, we will always find strongly connected components that do not have extra nodes. After this, the algorithm goes through the list of nodes created by the first search, in reverse order. if a node does not belong to a component, the algorithm creates a new component and starts a depth-first search that adds all new nodes found durin g the search to the new component leak Note that since all edges are reversed, the component does not leak \"leak\" to other pars in the graph.","title":"2. Search 2"},{"location":"Algorithms/Graph/StrongConnectivity/#implementation","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 vector < int > adj [ V ]; //adjancecy lists vector < int > radj [ V ]; // reversed adjacency list (directions are opposite to original) vector < int > order ; vector < vector < int >> result ; bool processed [ V ]; void search1 ( int u ) { if ( processed [ u ]) return ; processed [ u ] = true ; for ( auto v : adj [ u ]) search1 ( v ); order . push_back ( u ); } void search2 ( int u ) { if ( processed [ u ]) return ; processed [ u ] = true ; for ( auto v : radj [ u ]) search2 ( v ); result [ result . size () - 1 ]. push_back ( u ); //add current node to the last component } int main () { for ( int i = 0 ; i < V ; i ++ ) { // V = number of nodes if ( ! processed [ i ]) search1 ( i ); } reverse ( order . begin (), order . end ()) // reverse the order that created by the first search for ( int i = 0 ; i < V ; i ++ ) // initialized processed array processed [ i ] = false ; for ( auto u : order ) if ( ! processed [ u ]) result . push_back ( vector < int > ()); // add new component search2 ( i ); }","title":"Implementation"},{"location":"Algorithms/Graph/StrongConnectivity/#time_complexity","text":"$\\Omicron(n+m)$ because the algorithm performs two depth-first searches.","title":"Time complexity"},{"location":"Algorithms/Graph/StrongConnectivity/#2sat_problem","text":"Strong connectivity is also linked with the 2SAT problem . In this problem, we are given a logical formula for example $$ L_1 = (x_2 \\lor \\neg x_1) \\land (\\neg x_1 \\lor \\neg x_2) \\land (x_1 \\lor x_3) \\land (\\neg x_2 \\lor \\neg x_3) \\land (x_1 \\lor x_4) $$ is true when the variables are assigned as follows $$ \\begin{aligned} x_1 & = false \\\\ x_2 & = false \\\\ x_3 & = true \\\\ x_4 & = true \\\\ \\end{aligned} $$ The 2SAT problem can be represented as a graph whose nodes correspond to variable $x_i$ and negations $\\neg x_i$, and edges determine the connections between the variables. Each pair $(a_i \\lor b_i)$ generates two edges: $\\neg a_i \\rarr b_i$ and $\\neg b_i \\rarr a_i$. This means that if $a_i$ does not hold, $b_i$ must hold, and vice versa. The structure of the graph tells us whether it is possible to assign the values of the variables so that the formula is true. It turns out that this can be done exactly when there are no nodes $x_i$ and $\\neg x_i$ such that both nodes belong to the same strongly connected component. If there are such nodes, the graph contains a path from $x_i$ to $\\neg x_i$ and also a path from $\\neg x_i$ to $x_i$, so both $x_i$ and $\\neg x_i$ should be true which is not possible. In the graph of the formula $L_1$ there are no nodes $x_i$ and $\\neg x_i$ such that both nodes belong to the same strongly connected component, so solution exists. If a solution exists, the values for the variables can be found by going through the nodes of the component graph in a reverse topological sort order. at each step, we process a component that does not contain edges that lead to an unprocessed component. If the variables in the component have not been assigned values, their values will be determined accoring to the values in the component, and if they already have values, they remain unchanged. The process continues until each variable has been assigned a value. this method works because the graph has a special structure: if there are paths from node $x_i$ to node $x_j$ and from node $x_j$ to node $\\neg x_j$, then node $x_i$ never becomes true. The reason for this is that there is also a path from node $\\neg x_j$ to node $\\neg x_j$, and both $x_i$ and $x_j$ become false. Info A more difficult problem is 3SAT problem , where each part of the formula is form $(a_i \\lor b_i \\lor c_i)$. This problem is NP-hard, so no efficient algorithm for solving the problem is known.","title":"2SAT problem"},{"location":"Algorithms/Graph/StrongConnectivity/#implementation_1","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 int const MAXN ; // maximum number of variables int N ; // number of variables int M ; // number of conditions vector < vector < int >> adj ; // adjancecy list vector < vector < int >> radj ; //inver adjancency list vector < bool > processed ( 2 * MAXN ); vector < bool > rprocessed ( 2 * MAXN ); vector < int > sccnum ( 2 * MAXN , - 1 ); // initialize -1 vector < vector < int >> scc ; vector < int > order ; // maintain order created by search 1 vector < int > answer ( 2 * MAXN ); void search1 ( int u ) { if ( processed [ u ]) return ; processed [ u ] = true ; for ( auto v : adj [ u ]) search1 ( v ); order . push_back ( u ); } void search2 ( int u ) { if ( rprocessed [ u ]) return ; rprocessed [ u ] = true ; for ( auto v : radj [ u ]) search2 ( v ); scc [ scc . size () - 1 ]. push_back ( u ); sccnum [ u ] = scc . size () - 1 ; // write which SCC node 'u' is in } int neg ( int i ) { // negate of i return ( i + N ) % ( 2 * N ); } int main () { for ( int i = 0 ; i < 2 * N ; i ++ ) { // to 2 * N because we have to consider neg. if ( ! processed [ i ]) search1 ( i ); } reverse ( order . begin (), order . end ()); for ( auto u : order ) { if ( ! rprocessed [ u ]) { scc . push_back ( vector < int > ()); search2 ( u ); } } for ( int i = 0 ; i < N ; i ++ ) { if ( sccnum [ i ] == sccnum [ neg ( i )]) { cout << \"WE FOUND CONTRADICTION!!!\" ; return 0 ; } } reverse ( scc . begin (), scc . end ()); // reverse order of topological order of SCC for ( auto component : scc ) { for ( auto x : component ) { if ( answer [ x ] == - 1 ) { // if not assigned yet answer [ x ] = true ; answer [ neg ( x )] = false ; } } } for ( int i = N ; i < 2 * N ; i ++ ) { cout << i << \": \" << answer [ i ] << endl ; } }","title":"Implementation"},{"location":"Algorithms/Graph/StrongConnectivity/#related_problems","text":"2-SAT 1 2-SAT 2 2-SAT 3 2-SAT 4","title":"Related Problems"},{"location":"Algorithms/Graph/DFS/FindingCutEdges/","text":"Finding Cut Edges \u00b6 The code below works properly because of the lemma above(first lemma) 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 h [ root ] = 0 par [ v ] = - 1 dfs ( v ): d [ v ] = h [ v ] color [ v ] = gray for u in adj [ v ]: if color [ u ] == white : then par [ u ] = v and dfs ( u ) and d [ v ] = min ( d [ v ], d [ u ]) if d [ u ] > h [ v ] then the edge v - u is a cut edge else if u != par [ v ]: then d [ v ] = min ( d [ v ], h [ u ]) color [ v ] = black in this code, h[v] = height of vertex v in the DFS tree and d[v] = min(h[w] where there is at least vertex u in subtree of v in the DFS tree where there is an edge between $u$ and $w$) First lemma will be placed here \u21a9","title":"FindingCutEdges"},{"location":"Algorithms/Graph/DFS/FindingCutEdges/#finding_cut_edges","text":"The code below works properly because of the lemma above(first lemma) 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 h [ root ] = 0 par [ v ] = - 1 dfs ( v ): d [ v ] = h [ v ] color [ v ] = gray for u in adj [ v ]: if color [ u ] == white : then par [ u ] = v and dfs ( u ) and d [ v ] = min ( d [ v ], d [ u ]) if d [ u ] > h [ v ] then the edge v - u is a cut edge else if u != par [ v ]: then d [ v ] = min ( d [ v ], h [ u ]) color [ v ] = black in this code, h[v] = height of vertex v in the DFS tree and d[v] = min(h[w] where there is at least vertex u in subtree of v in the DFS tree where there is an edge between $u$ and $w$) First lemma will be placed here \u21a9","title":"Finding Cut Edges"},{"location":"Algorithms/Graph/DFS/Preface/","text":"DFS \u00b6 The most useful graph algorithms are search algorithms. DFS(Depth First Search) is one of them. While running DFS, we assign colors to the vertices (initially white) Algorithm itself is really simple 1 2 3 4 5 6 dfs ( v ): color [ v ] = gray for u in adj [ v ]: if color [ u ] == white : then dfs ( u ) color [ v ] = black Black color here is not used, but you can use it sometimes. Time complexity: $O(n + m)$","title":"Preface"},{"location":"Algorithms/Graph/DFS/Preface/#dfs","text":"The most useful graph algorithms are search algorithms. DFS(Depth First Search) is one of them. While running DFS, we assign colors to the vertices (initially white) Algorithm itself is really simple 1 2 3 4 5 6 dfs ( v ): color [ v ] = gray for u in adj [ v ]: if color [ u ] == white : then dfs ( u ) color [ v ] = black Black color here is not used, but you can use it sometimes. Time complexity: $O(n + m)$","title":"DFS"},{"location":"Algorithms/Graph/DFS/StartingFinishingTime/","text":"Starting time, finishing time \u00b6 Starting time of a vertex is the time we enter it (the order we enter it) and its finishing time is the time we leave it. Calculating these are easy 1 2 3 4 5 6 7 8 9 TIME = 0 dfs ( v ): st [ v ] = TIME ++ color [ v ] = gray for u in adj [ v ]: if color [ u ] == white : then dfs ( u ) color [ v ] = black ft [ v ] = TIME # or we can use TIME ++ It is useable in specially data structure problems (convert the tree into an array). Lemma-1 : if we run $dfs(root)$ in a rooted tree, then v is an ancestor of $u$ if and only if $st_v\\leq st_u\\leq ft_u\\leq ft_v$. So, given arrays $st$ and $ft$ we can rebuild the tree.","title":"StartingFinishingTime"},{"location":"Algorithms/Graph/DFS/StartingFinishingTime/#starting_time_finishing_time","text":"Starting time of a vertex is the time we enter it (the order we enter it) and its finishing time is the time we leave it. Calculating these are easy 1 2 3 4 5 6 7 8 9 TIME = 0 dfs ( v ): st [ v ] = TIME ++ color [ v ] = gray for u in adj [ v ]: if color [ u ] == white : then dfs ( u ) color [ v ] = black ft [ v ] = TIME # or we can use TIME ++ It is useable in specially data structure problems (convert the tree into an array). Lemma-1 : if we run $dfs(root)$ in a rooted tree, then v is an ancestor of $u$ if and only if $st_v\\leq st_u\\leq ft_u\\leq ft_v$. So, given arrays $st$ and $ft$ we can rebuild the tree.","title":"Starting time, finishing time"},{"location":"Algorithms/Graph/DFS/Tree/","text":"DFS tree \u00b6 DFS Tree is a rooted tree that is built like this 1 2 3 4 5 6 7 let T be a new tree dfs ( v ): color [ v ] = gray for u in adj [ v ]: if color [ u ] == white : then dfs ( u ) and par [ u ] = v ( in T ) color [ v ] = black Lemma : There is no cross edges, it means if there is an edge between $V$ and $u$, then $v=par[u]$ or $u=par[v]$","title":"DFSTree"},{"location":"Algorithms/Graph/DFS/Tree/#dfs_tree","text":"DFS Tree is a rooted tree that is built like this 1 2 3 4 5 6 7 let T be a new tree dfs ( v ): color [ v ] = gray for u in adj [ v ]: if color [ u ] == white : then dfs ( u ) and par [ u ] = v ( in T ) color [ v ] = black Lemma : There is no cross edges, it means if there is an edge between $V$ and $u$, then $v=par[u]$ or $u=par[v]$","title":"DFS tree"},{"location":"Algorithms/Graph/ShortestPaths/Bellman-Ford/","text":"Bellman-Ford Algorithm \u00b6 The Bellman-Ford algorithm finds shortest paths from a starting node to all nodes of the graph. The algorithm reduces the distance by finding edges that shorten the paths until it is not possible to reduce any distances. Bellman-Ford can process all kinds of graphs The algorithm can process all kinds of graphs, provided that the graph does not contain a cycle with negative length. If the graph contains a negative cycle, the algorithm can detect this. Implementation \u00b6 Assume that the graph is stored as an edge list edge that consists of tuples of the form$(a, b, w)$, meaing that there is an edge from node $a$ to node $b$ with weight $w$. The algorithm consists of $n-1$ rounds, and on each round the round the algorithm goes through all edges of the graph and tries to reduce the distances. The algorithm constructs an array $\\text{distance}$ that will contain the distance from x to all nodes of the graph. The constant INF denotes an infinite distance. $n = \\text{number of vertices(nodes)}$, $m = \\text{number of edges}$ 1 2 3 4 5 6 7 8 9 10 11 12 int const INF = 2e9 ; tuple < int , int , int > edges [ m ]; //edge list for ( int i = 1 ; i <= n ; i ++ ) distance [ i ] = INF ; distance [ s ] = 0 ; // starting node s for ( int i = 1 ; i <= n - 1 ; i ++ ) { for ( auto e : edges ) { int a , b , w ; tie ( a , b , w ) = e ; distance [ b ] = min ( distance [ b ], distance [ a ] + w ); } } Time Complexity \u00b6 $\\Omicron(nm)$ Negative Cycles \u00b6 The algorithm can also be used to check if the graph contains a cycle with negative length. A negative cycle can be detected using the Bellman-Ford algorithm by running the algorithm for $n$ rounds If the n-th round reduces any distance, the graph contains a negative cycle. Negative cycle in the whole graph Note that this algorithm can be used to search for a negative cycle in the whole graph regardless of the starting node","title":"Bellman-Ford"},{"location":"Algorithms/Graph/ShortestPaths/Bellman-Ford/#bellman-ford_algorithm","text":"The Bellman-Ford algorithm finds shortest paths from a starting node to all nodes of the graph. The algorithm reduces the distance by finding edges that shorten the paths until it is not possible to reduce any distances. Bellman-Ford can process all kinds of graphs The algorithm can process all kinds of graphs, provided that the graph does not contain a cycle with negative length. If the graph contains a negative cycle, the algorithm can detect this.","title":"Bellman-Ford Algorithm"},{"location":"Algorithms/Graph/ShortestPaths/Bellman-Ford/#implementation","text":"Assume that the graph is stored as an edge list edge that consists of tuples of the form$(a, b, w)$, meaing that there is an edge from node $a$ to node $b$ with weight $w$. The algorithm consists of $n-1$ rounds, and on each round the round the algorithm goes through all edges of the graph and tries to reduce the distances. The algorithm constructs an array $\\text{distance}$ that will contain the distance from x to all nodes of the graph. The constant INF denotes an infinite distance. $n = \\text{number of vertices(nodes)}$, $m = \\text{number of edges}$ 1 2 3 4 5 6 7 8 9 10 11 12 int const INF = 2e9 ; tuple < int , int , int > edges [ m ]; //edge list for ( int i = 1 ; i <= n ; i ++ ) distance [ i ] = INF ; distance [ s ] = 0 ; // starting node s for ( int i = 1 ; i <= n - 1 ; i ++ ) { for ( auto e : edges ) { int a , b , w ; tie ( a , b , w ) = e ; distance [ b ] = min ( distance [ b ], distance [ a ] + w ); } }","title":"Implementation"},{"location":"Algorithms/Graph/ShortestPaths/Bellman-Ford/#time_complexity","text":"$\\Omicron(nm)$","title":"Time Complexity"},{"location":"Algorithms/Graph/ShortestPaths/Bellman-Ford/#negative_cycles","text":"The algorithm can also be used to check if the graph contains a cycle with negative length. A negative cycle can be detected using the Bellman-Ford algorithm by running the algorithm for $n$ rounds If the n-th round reduces any distance, the graph contains a negative cycle. Negative cycle in the whole graph Note that this algorithm can be used to search for a negative cycle in the whole graph regardless of the starting node","title":"Negative Cycles"},{"location":"Algorithms/Graph/ShortestPaths/Dijkstra/","text":"Dijkstra's Algorithm \u00b6 The algorithm finds shortest paths from the starting node to all nodes of the graph, Like Bellman-Ford algorithm . The Benefit of Dijkstra's algorithm is that it is more efficent and can be used for processing large graphs. Dijkstra's algorithm is efficient, because it only process each edge in the graph once, using the fact that there are no negative edges. Negative edges Dijkstra's algorithm requires that there are no negative weight edges in the graph Implementation \u00b6 Assume that the graph is stored as an adjacency lists so that $adj[a]$ contains a pair $(b, w)$ always when there is an edge from node $a$ to node $b$ with weight $w$ Use priority queue that contains nodes the nodes ordered by their distances. Using priority queue, the next node to be processed can be retrieved in logarithmic time 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 int distance []; bool processed []; priority_queue < int , int > q ; // (-dis, node) for ( int i = 1 ; i <= n ; i ++ ) distance [ i ] = INF ; distance [ s ] = 0 ; // starting node s q . push ({ 0 , x }); while ( ! q . empty ()) { int a = q . top (). second ; q . pop (); if ( processed [ a ]) continue ; processed [ a ] = true ; for ( auto u : adj [ a ]) { int b = u . first , w = u . second ; if ( distance [ a ] + w < distance [ b ]) { distance [ b ] = distance [ a ] + w ; q . push ( - distance [ b ], b ); } } } Time Complexity \u00b6 Not yet","title":"Dijkstra"},{"location":"Algorithms/Graph/ShortestPaths/Dijkstra/#dijkstras_algorithm","text":"The algorithm finds shortest paths from the starting node to all nodes of the graph, Like Bellman-Ford algorithm . The Benefit of Dijkstra's algorithm is that it is more efficent and can be used for processing large graphs. Dijkstra's algorithm is efficient, because it only process each edge in the graph once, using the fact that there are no negative edges. Negative edges Dijkstra's algorithm requires that there are no negative weight edges in the graph","title":"Dijkstra's Algorithm"},{"location":"Algorithms/Graph/ShortestPaths/Dijkstra/#implementation","text":"Assume that the graph is stored as an adjacency lists so that $adj[a]$ contains a pair $(b, w)$ always when there is an edge from node $a$ to node $b$ with weight $w$ Use priority queue that contains nodes the nodes ordered by their distances. Using priority queue, the next node to be processed can be retrieved in logarithmic time 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 int distance []; bool processed []; priority_queue < int , int > q ; // (-dis, node) for ( int i = 1 ; i <= n ; i ++ ) distance [ i ] = INF ; distance [ s ] = 0 ; // starting node s q . push ({ 0 , x }); while ( ! q . empty ()) { int a = q . top (). second ; q . pop (); if ( processed [ a ]) continue ; processed [ a ] = true ; for ( auto u : adj [ a ]) { int b = u . first , w = u . second ; if ( distance [ a ] + w < distance [ b ]) { distance [ b ] = distance [ a ] + w ; q . push ( - distance [ b ], b ); } } }","title":"Implementation"},{"location":"Algorithms/Graph/ShortestPaths/Dijkstra/#time_complexity","text":"Not yet","title":"Time Complexity"},{"location":"Algorithms/Graph/ShortestPaths/Floyd-Warshall/","text":"Floyd-Warshall Algorithm \u00b6 Floyd-Warshall algorithm provides an alternative way to approach th problem of finding shortest paths. Floyd-Warhsall algorithm finds all shortests paths between the nodes in a single run. Floyd-Warhsall algorithm maintains a two-dimensional array that contains distances between the nodes. Floyd-warshall algorithm is easy to implement. Floyd-warhsall algorithm reduces distance by intermediate nodes in paths. Implementation \u00b6 Assume using adjacency matrix 1 2 3 4 5 6 7 8 9 10 11 12 // first build distance (2-dimensional array) int const INF = 2e9 ; int adj [][]; int distance [][]; for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 1 ; j <= n ; j ++ ) { if ( i == j ) distance [ i ][ j ] = 0 ; else if ( adj [ i ][ j ]) distance [ i ][ j ] = adj [ i ][ j ]; else distance [ i ][ j ] = INF ; } } 1 2 3 4 5 6 7 8 9 // process for ( int k = 1 ; k <= n ; k ++ ) { // k == intermediate node for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 1 ; j <= n ; j ++ ) { distance [ i ][ j ] = min ( distance [ i ][ j ], distance [ i ][ k ] + distance [ k ][ j ]) } } } Time Complexity \u00b6 $\\Omicron(n^3)$ Related Problems \u00b6 K-th Path","title":"Floyd-Warshall"},{"location":"Algorithms/Graph/ShortestPaths/Floyd-Warshall/#floyd-warshall_algorithm","text":"Floyd-Warshall algorithm provides an alternative way to approach th problem of finding shortest paths. Floyd-Warhsall algorithm finds all shortests paths between the nodes in a single run. Floyd-Warhsall algorithm maintains a two-dimensional array that contains distances between the nodes. Floyd-warshall algorithm is easy to implement. Floyd-warhsall algorithm reduces distance by intermediate nodes in paths.","title":"Floyd-Warshall Algorithm"},{"location":"Algorithms/Graph/ShortestPaths/Floyd-Warshall/#implementation","text":"Assume using adjacency matrix 1 2 3 4 5 6 7 8 9 10 11 12 // first build distance (2-dimensional array) int const INF = 2e9 ; int adj [][]; int distance [][]; for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 1 ; j <= n ; j ++ ) { if ( i == j ) distance [ i ][ j ] = 0 ; else if ( adj [ i ][ j ]) distance [ i ][ j ] = adj [ i ][ j ]; else distance [ i ][ j ] = INF ; } } 1 2 3 4 5 6 7 8 9 // process for ( int k = 1 ; k <= n ; k ++ ) { // k == intermediate node for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 1 ; j <= n ; j ++ ) { distance [ i ][ j ] = min ( distance [ i ][ j ], distance [ i ][ k ] + distance [ k ][ j ]) } } }","title":"Implementation"},{"location":"Algorithms/Graph/ShortestPaths/Floyd-Warshall/#time_complexity","text":"$\\Omicron(n^3)$","title":"Time Complexity"},{"location":"Algorithms/Graph/ShortestPaths/Floyd-Warshall/#related_problems","text":"K-th Path","title":"Related Problems"},{"location":"Algorithms/Graph/ShortestPaths/Preface/","text":"Shortest Paths \u00b6 Finding a shortest path between two nodes of a graph is an important problem that has many practical applications. In an uweighted graph, the length of a path equals the number of its edges, and we can simply use breath-first search to find a shortest path. However, in this chapter we focus on weighted graphs where more sophisticated algorithms are needed for shortest paths. Diff \u00b6 $n = \\text{number of nodes}$, $m = \\text{number of edges}$ TimeComplexity DS - Bellman-Ford $\\Omicron(nm)$ edge list neg-cycle detection SPFA $\\Omicron(nm)$ Dijkstra $\\Omicron(n + m\\lg(m))$ adjacency lists no neg edges Floyd-Warshall $\\Omicron(n^3)$ adjacency matrix finds all shortest paths between the nodes","title":"Preface"},{"location":"Algorithms/Graph/ShortestPaths/Preface/#shortest_paths","text":"Finding a shortest path between two nodes of a graph is an important problem that has many practical applications. In an uweighted graph, the length of a path equals the number of its edges, and we can simply use breath-first search to find a shortest path. However, in this chapter we focus on weighted graphs where more sophisticated algorithms are needed for shortest paths.","title":"Shortest Paths"},{"location":"Algorithms/Graph/ShortestPaths/Preface/#diff","text":"$n = \\text{number of nodes}$, $m = \\text{number of edges}$ TimeComplexity DS - Bellman-Ford $\\Omicron(nm)$ edge list neg-cycle detection SPFA $\\Omicron(nm)$ Dijkstra $\\Omicron(n + m\\lg(m))$ adjacency lists no neg edges Floyd-Warshall $\\Omicron(n^3)$ adjacency matrix finds all shortest paths between the nodes","title":"Diff"},{"location":"Algorithms/Sort/BubbleSort/","text":"Bubble Sort \u00b6 Bubble Sort C++ \u00b6 1 2 3 4 5 6 7 8 9 template < typename It > void BubbleSort ( It begin , It end ) { if ( begin == end ) return ; //return if container is empty for ( It i = end - 1 ; i != begin ; i -- ) { for ( It j = begin ; j != i ; j ++ ) { if ( * j > * ( j + 1 )) swap ( * j , * ( j + 1 )); } } }","title":"BubbleSort"},{"location":"Algorithms/Sort/BubbleSort/#bubble_sort","text":"Bubble Sort","title":"Bubble Sort"},{"location":"Algorithms/Sort/BubbleSort/#c","text":"1 2 3 4 5 6 7 8 9 template < typename It > void BubbleSort ( It begin , It end ) { if ( begin == end ) return ; //return if container is empty for ( It i = end - 1 ; i != begin ; i -- ) { for ( It j = begin ; j != i ; j ++ ) { if ( * j > * ( j + 1 )) swap ( * j , * ( j + 1 )); } } }","title":"C++"},{"location":"Algorithms/Sort/HeapSort/","text":"Heap Sort \u00b6 Heap is a nearly complete binary tree. we can easily implement on basic array object. Heap Structure satisfies Heap Property Heap Property max-heap-property: Parent's Key $\\geq$ Children's key min-heap-property: Parent's Key $\\leq$ Children's key Implementation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 #include <iostream> using namespace std ; int arr [ 1000000 ]; int heap_size ; template < typename T > void _swap ( T & a , T & b ) { T temp = a ; a = b ; b = temp ; } inline int parent ( int i ) { return ( i - 1 ) >> 1 ; } inline int left ( int i ) { return ( i << 1 ) + 1 ; } inline int right ( int i ) { return ( i << 1 ) + 2 ; } void max_heapify ( int i ) { int largest ; int l = left ( i ); int r = right ( i ); if ( l < heap_size && arr [ l ] > arr [ i ]) { largest = l ; } else { largest = i ; } if ( r < heap_size && arr [ r ] > arr [ largest ]) largest = r ; if ( largest != i ) { _swap ( arr [ largest ], arr [ i ]); max_heapify ( largest ); } } void build_maxheap () { for ( int i = parent ( heap_size - 1 ); i >= 0 ; i -- ) { max_heapify ( i ); } } void heapsort () { build_maxheap (); for ( int i = ( heap_size - 1 ); i >= 0 ; i -- ) { _swap ( arr [ i ], arr [ 0 ]); heap_size -- ; // reduce heap_size at here; max_heapify ( 0 ); } } int main () { ios_base :: sync_with_stdio ( 0 ); cin . tie ( 0 ); cout . tie ( 0 ); int n = 5000 ; //cin >> n; heap_size = n ; for ( int i = 0 ; i < n ; i ++ ) { arr [ i ] = rand () % 5000 ; } heapsort (); for ( int i = 0 ; i < n ; i ++ ) { cout << arr [ i ] << '\\n' ; } return 0 ; } Related Problems \u00b6 Sort","title":"HeapSort"},{"location":"Algorithms/Sort/HeapSort/#heap_sort","text":"Heap is a nearly complete binary tree. we can easily implement on basic array object. Heap Structure satisfies Heap Property Heap Property max-heap-property: Parent's Key $\\geq$ Children's key min-heap-property: Parent's Key $\\leq$ Children's key","title":"Heap Sort"},{"location":"Algorithms/Sort/HeapSort/#implementation","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 #include <iostream> using namespace std ; int arr [ 1000000 ]; int heap_size ; template < typename T > void _swap ( T & a , T & b ) { T temp = a ; a = b ; b = temp ; } inline int parent ( int i ) { return ( i - 1 ) >> 1 ; } inline int left ( int i ) { return ( i << 1 ) + 1 ; } inline int right ( int i ) { return ( i << 1 ) + 2 ; } void max_heapify ( int i ) { int largest ; int l = left ( i ); int r = right ( i ); if ( l < heap_size && arr [ l ] > arr [ i ]) { largest = l ; } else { largest = i ; } if ( r < heap_size && arr [ r ] > arr [ largest ]) largest = r ; if ( largest != i ) { _swap ( arr [ largest ], arr [ i ]); max_heapify ( largest ); } } void build_maxheap () { for ( int i = parent ( heap_size - 1 ); i >= 0 ; i -- ) { max_heapify ( i ); } } void heapsort () { build_maxheap (); for ( int i = ( heap_size - 1 ); i >= 0 ; i -- ) { _swap ( arr [ i ], arr [ 0 ]); heap_size -- ; // reduce heap_size at here; max_heapify ( 0 ); } } int main () { ios_base :: sync_with_stdio ( 0 ); cin . tie ( 0 ); cout . tie ( 0 ); int n = 5000 ; //cin >> n; heap_size = n ; for ( int i = 0 ; i < n ; i ++ ) { arr [ i ] = rand () % 5000 ; } heapsort (); for ( int i = 0 ; i < n ; i ++ ) { cout << arr [ i ] << '\\n' ; } return 0 ; }","title":"Implementation"},{"location":"Algorithms/Sort/HeapSort/#related_problems","text":"Sort","title":"Related Problems"},{"location":"Algorithms/Sort/InsertionSort/","text":"Insertion Sort \u00b6 Insertion Sort is very simple algorithm it works exactly like the way you sort a deck of card C++ \u00b6 1 2 3 4 5 6 7 8 9 10 template < typenme It > // Iterator void insertionSort ( It begin , It end ) { //TODO add comparator if ( begin == end ) return ; // return if container is empty for ( It i = begin ; i != end ; i ++ ) { for ( It j = i ; j != begin ; j -- ) { if ( * ( j - 1 ) < * j ) break ; swap ( * ( j - 1 ), * j ); } } }","title":"InsertionSort"},{"location":"Algorithms/Sort/InsertionSort/#insertion_sort","text":"Insertion Sort is very simple algorithm it works exactly like the way you sort a deck of card","title":"Insertion Sort"},{"location":"Algorithms/Sort/InsertionSort/#c","text":"1 2 3 4 5 6 7 8 9 10 template < typenme It > // Iterator void insertionSort ( It begin , It end ) { //TODO add comparator if ( begin == end ) return ; // return if container is empty for ( It i = begin ; i != end ; i ++ ) { for ( It j = i ; j != begin ; j -- ) { if ( * ( j - 1 ) < * j ) break ; swap ( * ( j - 1 ), * j ); } } }","title":"C++"},{"location":"Algorithms/Tree/Basics/","text":"Tree \u00b6 A tree is a connected, acyclic graph that consist of $n$ nodes and $n-1$ edges. Removing any edges from a tree divides it into two components , and adding any edge to a tree creates a cycle . Moreover, there is always a unique path between any two nodes of a tree. Leaves \u00b6 leaves of a tree are the nodes with degree 1, i.e., with only one neighbor Rooted tree \u00b6 In a rooted tree, one of the nodes is appointed the root of the tree, and all other nodes are placed underneath the root. Children & Parent of a node \u00b6 In a rooted tree, the children of a node are its lower negibors, and the parent of a node is its upper neighbor. Recursive structure \u00b6 The structure of rooted tree is recursive each node of the tree ats as the root of subtree that contains the node itself and all nodes that are in the subtrees of its children","title":"Basics"},{"location":"Algorithms/Tree/Basics/#tree","text":"A tree is a connected, acyclic graph that consist of $n$ nodes and $n-1$ edges. Removing any edges from a tree divides it into two components , and adding any edge to a tree creates a cycle . Moreover, there is always a unique path between any two nodes of a tree.","title":"Tree"},{"location":"Algorithms/Tree/Basics/#leaves","text":"leaves of a tree are the nodes with degree 1, i.e., with only one neighbor","title":"Leaves"},{"location":"Algorithms/Tree/Basics/#rooted_tree","text":"In a rooted tree, one of the nodes is appointed the root of the tree, and all other nodes are placed underneath the root.","title":"Rooted tree"},{"location":"Algorithms/Tree/Basics/#children_parent_of_a_node","text":"In a rooted tree, the children of a node are its lower negibors, and the parent of a node is its upper neighbor.","title":"Children &amp; Parent of a node"},{"location":"Algorithms/Tree/Basics/#recursive_structure","text":"The structure of rooted tree is recursive each node of the tree ats as the root of subtree that contains the node itself and all nodes that are in the subtrees of its children","title":"Recursive structure"},{"location":"Algorithms/Tree/DP/","text":"Tree Dynamic Programming \u00b6 Dynamic programming can be used to calculate some information during a tree traversal. Time Complexities \u00b6 Not yet added The number of nodes in its subtree \u00b6 The subtree contains the node itself and all nodes in the subtrees of its children. so we can calculate the number of nodes recursively using the following code Time complexity: $\\Omicron(n)$ 1 2 3 4 5 6 7 8 9 10 int count [ # nodes ]; void dfs ( int s , int e ) { //current node s, previous node e; count [ s ] = 1 ; for ( auto u : adj [ s ]) { if ( u == e ) continue ; dfs ( u , s ); count [ s ] += count [ u ]; } } Diameter \u00b6 The Diameter of a tree is the maximum length of a path between two nodes. Algorithm 1 (based on DP) \u00b6 A general way to approach many tree problems is to first root the tree arbitrarily . After this, we can try to solve the problem separately for each subtree. Our first algorithm for calculating the diameter is based on this idea. An important observation is that every path in a rooted tree has a highest point : the highest node that belongs to the path. Thus we can calculate for each node the length of the longest path whose heighest point is the node. One of those path corresponds to the diameter of the tree. We calculate for each node $x$ two values: - toLeaf(x): the maximum length of a path from x to any leaf - maxLength(x): the maximum length of a path whose highest point is $x$ $f(x)$: Longest path starts from node $x$ and goes into its subtree. $g(x)$: Longest path starts in subtree of $x$, passes through $x$ and ends in subtree of $x$ If for all nodes $x$, we take maximum of $f(x), g(x)$, then we can get the diameter. Dynamic programming can be used to calculate the above values for all nodes in $\\Omicron(n)$ time. Implementation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 //adjacency list //adj[i] contains all neighbors of i vector < int > adj [ N ]; //functions as defined above int f [ N ], g [ N ], diameter ; // pV is parent of node V void dfs ( int V , int pV ) { //this vector will store f for all children of V vector < int > fValues ; //traverse over all children for ( auto v : adj [ V ]) { if ( v == pV ) continue ; dfs ( v , V ); fValues . push_back ( f [ v ]); } //sort to get top two values // you can also get top two values without sorting in O(N) // current complexity is n lg n sort ( fValues . begin (), fValues . end ()); f [ V ] = 1 ; if ( ! fValues . empty ()) f [ V ] += fValues . back (); if ( fValues . size () >= 2 ) g [ V ] = 2 + fValues . back () + fValues [ fValues . size () - 2 ]; diameter = max ( diameter , max ( f [ V ], g [ V ])); } More General Implementation \u00b6 with weighted edges 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 //adjacency list //adj[i] contains all neighbors of i, and weights to i -> its neighbor vector < vector < pair < int , int >>> adj [ N ]; int diameter = 0 ; void dfs ( int V , int pV ) { vector < int > fValues ; for ( auto v : adj [ V ]) { if ( v . first == pV ) continue ; dfs ( v . first , V ); fValues . push_back ( f [ v . first ] + v . second ); // fvalue of child + weight of edge; } int a = - 1 , b = - 1 ; // a is biggest, b is second to biggest for ( auto x : fValues ) { if ( x > a ) { b = a ; a = x ; } else if ( x > b ) { b = x ; } } f [ V ] = 0 ; if ( a > 0 ) f [ V ] = a ; if ( a > 0 && b > 0 ) g [ V ] = a + b ; diameter = max ( diameter , max ( f [ V ], g [ V ])); } Algorithm 2 (based on DFS) \u00b6 Another efficient way to calculate the diameter of a tree is based on two depth-first searches. First, we choose an arbitrary node $a$ in the tree and find the farthest node $b$ from $a$. Then, we find the farthest node $c$ from $b$. The diameter of the tree is the distance between $b$ and $c$. How this works? \u00b6 Not yet added implementation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 int firstFar ; int diameter = 0 ; void dfs ( int V , int pV , int dis ) { if ( diameter < dis ) { diameter = dis ; firstFar = V ; } for ( auto v : adj [ V ]) { if ( v . first == pV ) continue ; dfs ( v . first , V , dis + v . second ); // v.first == neighbor node, v.second weight of edge } } dfs ( 0 , 0 , 0 ) //initial call // diameter = 0; // not necessary; dfs ( firstFar , 0 , 0 ); // now diameter is diameter of tree resources \u00b6 https://codeforces.com/blog/entry/20935","title":"DP"},{"location":"Algorithms/Tree/DP/#tree_dynamic_programming","text":"Dynamic programming can be used to calculate some information during a tree traversal.","title":"Tree Dynamic Programming"},{"location":"Algorithms/Tree/DP/#time_complexities","text":"Not yet added","title":"Time Complexities"},{"location":"Algorithms/Tree/DP/#the_number_of_nodes_in_its_subtree","text":"The subtree contains the node itself and all nodes in the subtrees of its children. so we can calculate the number of nodes recursively using the following code Time complexity: $\\Omicron(n)$ 1 2 3 4 5 6 7 8 9 10 int count [ # nodes ]; void dfs ( int s , int e ) { //current node s, previous node e; count [ s ] = 1 ; for ( auto u : adj [ s ]) { if ( u == e ) continue ; dfs ( u , s ); count [ s ] += count [ u ]; } }","title":"The number of nodes in its subtree"},{"location":"Algorithms/Tree/DP/#diameter","text":"The Diameter of a tree is the maximum length of a path between two nodes.","title":"Diameter"},{"location":"Algorithms/Tree/DP/#algorithm_1_based_on_dp","text":"A general way to approach many tree problems is to first root the tree arbitrarily . After this, we can try to solve the problem separately for each subtree. Our first algorithm for calculating the diameter is based on this idea. An important observation is that every path in a rooted tree has a highest point : the highest node that belongs to the path. Thus we can calculate for each node the length of the longest path whose heighest point is the node. One of those path corresponds to the diameter of the tree. We calculate for each node $x$ two values: - toLeaf(x): the maximum length of a path from x to any leaf - maxLength(x): the maximum length of a path whose highest point is $x$ $f(x)$: Longest path starts from node $x$ and goes into its subtree. $g(x)$: Longest path starts in subtree of $x$, passes through $x$ and ends in subtree of $x$ If for all nodes $x$, we take maximum of $f(x), g(x)$, then we can get the diameter. Dynamic programming can be used to calculate the above values for all nodes in $\\Omicron(n)$ time.","title":"Algorithm 1 (based on DP)"},{"location":"Algorithms/Tree/DP/#implementation","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 //adjacency list //adj[i] contains all neighbors of i vector < int > adj [ N ]; //functions as defined above int f [ N ], g [ N ], diameter ; // pV is parent of node V void dfs ( int V , int pV ) { //this vector will store f for all children of V vector < int > fValues ; //traverse over all children for ( auto v : adj [ V ]) { if ( v == pV ) continue ; dfs ( v , V ); fValues . push_back ( f [ v ]); } //sort to get top two values // you can also get top two values without sorting in O(N) // current complexity is n lg n sort ( fValues . begin (), fValues . end ()); f [ V ] = 1 ; if ( ! fValues . empty ()) f [ V ] += fValues . back (); if ( fValues . size () >= 2 ) g [ V ] = 2 + fValues . back () + fValues [ fValues . size () - 2 ]; diameter = max ( diameter , max ( f [ V ], g [ V ])); }","title":"Implementation"},{"location":"Algorithms/Tree/DP/#more_general_implementation","text":"with weighted edges 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 //adjacency list //adj[i] contains all neighbors of i, and weights to i -> its neighbor vector < vector < pair < int , int >>> adj [ N ]; int diameter = 0 ; void dfs ( int V , int pV ) { vector < int > fValues ; for ( auto v : adj [ V ]) { if ( v . first == pV ) continue ; dfs ( v . first , V ); fValues . push_back ( f [ v . first ] + v . second ); // fvalue of child + weight of edge; } int a = - 1 , b = - 1 ; // a is biggest, b is second to biggest for ( auto x : fValues ) { if ( x > a ) { b = a ; a = x ; } else if ( x > b ) { b = x ; } } f [ V ] = 0 ; if ( a > 0 ) f [ V ] = a ; if ( a > 0 && b > 0 ) g [ V ] = a + b ; diameter = max ( diameter , max ( f [ V ], g [ V ])); }","title":"More General Implementation"},{"location":"Algorithms/Tree/DP/#algorithm_2_based_on_dfs","text":"Another efficient way to calculate the diameter of a tree is based on two depth-first searches. First, we choose an arbitrary node $a$ in the tree and find the farthest node $b$ from $a$. Then, we find the farthest node $c$ from $b$. The diameter of the tree is the distance between $b$ and $c$.","title":"Algorithm 2 (based on DFS)"},{"location":"Algorithms/Tree/DP/#how_this_works","text":"Not yet added","title":"How this works?"},{"location":"Algorithms/Tree/DP/#implementation_1","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 int firstFar ; int diameter = 0 ; void dfs ( int V , int pV , int dis ) { if ( diameter < dis ) { diameter = dis ; firstFar = V ; } for ( auto v : adj [ V ]) { if ( v . first == pV ) continue ; dfs ( v . first , V , dis + v . second ); // v.first == neighbor node, v.second weight of edge } } dfs ( 0 , 0 , 0 ) //initial call // diameter = 0; // not necessary; dfs ( firstFar , 0 , 0 ); // now diameter is diameter of tree","title":"implementation"},{"location":"Algorithms/Tree/DP/#resources","text":"https://codeforces.com/blog/entry/20935","title":"resources"},{"location":"Algorithms/Tree/SpanningTree/","text":"Spanning Trees \u00b6 A spanning tree of a graph consists of all nodes of the graph and some of the edges of the graph so that there is a path between any two nodes. Like trees in general, spanning trees are connected and acyclic. Usually there are several ways to construct a spanning tree. Note that a graph may have several minimum and maximum spanning trees, so the trees ar not unique. It turns out that several greedy methods can be used to construct minimum and maximum spanning trees. terminologies \u00b6 Weight of spanning tree: sum of its edge weights. Minimum spanning tree: a spanning tree whose weight is as small as possible Kruskal's Algorithms \u00b6 The initial spanning tree only contains the nodes of the graph and does not contain any edges. Then the algorithm goes through edges ordered by their weights, and always adds an edge to the tree if it does not create a cycle. The algorithm maintains the components of the tree. Initially each node of the graph belongs to a separate component. Always when an edge is added to the tree, two components are joined. Finally, all nodes belong to the same component and a minimum spanning tree has been found Implementation \u00b6 It's convinient to use the edge list representation 1 2 3 4 5 6 vector < pair < int u , int v >> edges ; //edge list sort ( vector . begin (), vector . end ()); for ( auto edge : edges ) { //using union find structure if ( ! same ( a , b )) unite ( a , b ); } efficiency The problem is how to efficiently implement the function same and unite . One possibility is to implement function same as a graph traversal and check if we can get from node a to node b . However, the time complexity of such a function would be $\\Omicron(n+m)$ and resulting algorithm would be slow, because the function same will be called for each edge in graph. Union find structure \u00b6 Using a Union find structure implements both $same$ and $unite$ functions in $\\Omicron(lg(n))$ time. thus the time complexity of Kruskal's algorithm will be $\\Omicron(mlg(n))$ Structure \u00b6 In a union-find structure, one element in each set is the representative of the set, and there is a chain from any other element of the set to the representative. The efficiency of the union-find structure depends on how the sets are joined. It turns out that we can follow a simple strategy: always connect the representative the smaller set to the representative of larger set . Using this strategy, the length of any chain will be $\\Omicron(lg(n))$ Implementation \u00b6 The union-find structure can be implemented using arrays. link contains for each element the next element in the chain or the element it self if it is representative. and the array size indicates for each representative the size of thecorresponding set. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 for ( int i = 1 ; i <= n ; i ++ ) link [ i ] = i ; for ( int i = 1 ; i <= n ; i ++ ) size [ i ] = 1 ; int find ( int x ) { // returns the representative for an element x. while ( x != link [ x ]) x = link [ x ]; return x ; } bool same ( int a , int b ) { // returns whether elements a and b belong to the same set return find ( a ) == find ( b ); } void unite ( int a , int b ) { // joins the set that contains elements a and b // it connects the smaller set to the larger set a = find ( a ); b = find ( b ); if ( size [ a ] < size [ b ]) swap ( a , b ); size [ a ] += size [ b ]; link [ b ] = a ; } Prim's algorithm \u00b6 The algorithm first adds an arbitrary node to the tree. After this, the algorithm always choose a minimum-weight edge that adds a new node to the tree. Finally all nodes have been added to the tree and a minimum tree has been found Prim's algorithm resembles Dijkstra's algorithm . But, Prim's algorithm simply selects the minimum weight edge that adds a new node to the tree. Implementation \u00b6 Like Dijkstra's algorithm , Prim's algorithm can be efficiently implemented using a priority queue. The priority queue should contain all nodes that can be conneted to the current component using a single edge, in increasing order of the weights of the corresponding edges. The time complexity of Prim's algorithm is $\\Omicron(n+ mlg(m))$ that equals the time complexity of Dijkstra's algorithm. most competitive programmers use Kruskal's algorithm. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 int V ; // #nodes(vertices) int E ; // #edges int const INF = 2e9 ; vector < vector < pair < int , int >>> adj ; //adjacency list using node = pair < int , int > ; // pair<key, node> representation. void Prim () { priority_queue < node , vector < node > , greater < node >> pq ; //mean-heap implementation using stl int starter = 0 ; // starting node or initial node. vector < int > key ( V , INF ); vector < int > parent ( V , - 1 ); vector < bool > inMST ( V , false ); pq . push ({ 0 , starter }); int MST_weight = 0 ; while ( ! pq . empty ()) { int w = pq . top (). first ; // minimum weight to add a new vertice to MST int u = pq . top (). second ; // new vertice pq . pop (); if ( inMST [ u ]) continue ; // if u already in MST continue; inMST [ u ] = true ; // else add u to MST; MST_weight += w ; for ( auto vw : adj [ u ]) { int v = vw . first ; int w = vw . second ; if ( inMST [ v ] == false && key [ v ] > w ) { // only when new key(weight) value of v is less than current value key [ v ] = w ; pq . push ({ key [ v ], v }); parent [ v ] = u ; } } } }","title":"SpanningTree"},{"location":"Algorithms/Tree/SpanningTree/#spanning_trees","text":"A spanning tree of a graph consists of all nodes of the graph and some of the edges of the graph so that there is a path between any two nodes. Like trees in general, spanning trees are connected and acyclic. Usually there are several ways to construct a spanning tree. Note that a graph may have several minimum and maximum spanning trees, so the trees ar not unique. It turns out that several greedy methods can be used to construct minimum and maximum spanning trees.","title":"Spanning Trees"},{"location":"Algorithms/Tree/SpanningTree/#terminologies","text":"Weight of spanning tree: sum of its edge weights. Minimum spanning tree: a spanning tree whose weight is as small as possible","title":"terminologies"},{"location":"Algorithms/Tree/SpanningTree/#kruskals_algorithms","text":"The initial spanning tree only contains the nodes of the graph and does not contain any edges. Then the algorithm goes through edges ordered by their weights, and always adds an edge to the tree if it does not create a cycle. The algorithm maintains the components of the tree. Initially each node of the graph belongs to a separate component. Always when an edge is added to the tree, two components are joined. Finally, all nodes belong to the same component and a minimum spanning tree has been found","title":"Kruskal's Algorithms"},{"location":"Algorithms/Tree/SpanningTree/#implementation","text":"It's convinient to use the edge list representation 1 2 3 4 5 6 vector < pair < int u , int v >> edges ; //edge list sort ( vector . begin (), vector . end ()); for ( auto edge : edges ) { //using union find structure if ( ! same ( a , b )) unite ( a , b ); } efficiency The problem is how to efficiently implement the function same and unite . One possibility is to implement function same as a graph traversal and check if we can get from node a to node b . However, the time complexity of such a function would be $\\Omicron(n+m)$ and resulting algorithm would be slow, because the function same will be called for each edge in graph.","title":"Implementation"},{"location":"Algorithms/Tree/SpanningTree/#union_find_structure","text":"Using a Union find structure implements both $same$ and $unite$ functions in $\\Omicron(lg(n))$ time. thus the time complexity of Kruskal's algorithm will be $\\Omicron(mlg(n))$","title":"Union find structure"},{"location":"Algorithms/Tree/SpanningTree/#structure","text":"In a union-find structure, one element in each set is the representative of the set, and there is a chain from any other element of the set to the representative. The efficiency of the union-find structure depends on how the sets are joined. It turns out that we can follow a simple strategy: always connect the representative the smaller set to the representative of larger set . Using this strategy, the length of any chain will be $\\Omicron(lg(n))$","title":"Structure"},{"location":"Algorithms/Tree/SpanningTree/#implementation_1","text":"The union-find structure can be implemented using arrays. link contains for each element the next element in the chain or the element it self if it is representative. and the array size indicates for each representative the size of thecorresponding set. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 for ( int i = 1 ; i <= n ; i ++ ) link [ i ] = i ; for ( int i = 1 ; i <= n ; i ++ ) size [ i ] = 1 ; int find ( int x ) { // returns the representative for an element x. while ( x != link [ x ]) x = link [ x ]; return x ; } bool same ( int a , int b ) { // returns whether elements a and b belong to the same set return find ( a ) == find ( b ); } void unite ( int a , int b ) { // joins the set that contains elements a and b // it connects the smaller set to the larger set a = find ( a ); b = find ( b ); if ( size [ a ] < size [ b ]) swap ( a , b ); size [ a ] += size [ b ]; link [ b ] = a ; }","title":"Implementation"},{"location":"Algorithms/Tree/SpanningTree/#prims_algorithm","text":"The algorithm first adds an arbitrary node to the tree. After this, the algorithm always choose a minimum-weight edge that adds a new node to the tree. Finally all nodes have been added to the tree and a minimum tree has been found Prim's algorithm resembles Dijkstra's algorithm . But, Prim's algorithm simply selects the minimum weight edge that adds a new node to the tree.","title":"Prim's algorithm"},{"location":"Algorithms/Tree/SpanningTree/#implementation_2","text":"Like Dijkstra's algorithm , Prim's algorithm can be efficiently implemented using a priority queue. The priority queue should contain all nodes that can be conneted to the current component using a single edge, in increasing order of the weights of the corresponding edges. The time complexity of Prim's algorithm is $\\Omicron(n+ mlg(m))$ that equals the time complexity of Dijkstra's algorithm. most competitive programmers use Kruskal's algorithm. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 int V ; // #nodes(vertices) int E ; // #edges int const INF = 2e9 ; vector < vector < pair < int , int >>> adj ; //adjacency list using node = pair < int , int > ; // pair<key, node> representation. void Prim () { priority_queue < node , vector < node > , greater < node >> pq ; //mean-heap implementation using stl int starter = 0 ; // starting node or initial node. vector < int > key ( V , INF ); vector < int > parent ( V , - 1 ); vector < bool > inMST ( V , false ); pq . push ({ 0 , starter }); int MST_weight = 0 ; while ( ! pq . empty ()) { int w = pq . top (). first ; // minimum weight to add a new vertice to MST int u = pq . top (). second ; // new vertice pq . pop (); if ( inMST [ u ]) continue ; // if u already in MST continue; inMST [ u ] = true ; // else add u to MST; MST_weight += w ; for ( auto vw : adj [ u ]) { int v = vw . first ; int w = vw . second ; if ( inMST [ v ] == false && key [ v ] > w ) { // only when new key(weight) value of v is less than current value key [ v ] = w ; pq . push ({ key [ v ], v }); parent [ v ] = u ; } } } }","title":"Implementation"},{"location":"Algorithms/Tree/Traversal/","text":"Tree Traversal \u00b6 General Graph Traversal algorithms can be used to traverse the nodes of a tree. However, the traversal of a tree is easier to implement than that of a general graph, because there are no cycles in the tree and it is not possible to reach a node from multiple directions. implementation \u00b6 The typical way to traverse a tree is to start a depth-first search at an arbitrary node . The following recursive function can be used assumes that we are maintaining adjacency list 1 2 3 4 5 6 7 8 void dfs ( int s , int e ) { // current node s and previous node e //process node s for ( auto u : adj [ s ]) { if ( u != e ) dfs ( u , s ); } } dfs ( x , x ) // initial call because there's no self loop","title":"Traversal"},{"location":"Algorithms/Tree/Traversal/#tree_traversal","text":"General Graph Traversal algorithms can be used to traverse the nodes of a tree. However, the traversal of a tree is easier to implement than that of a general graph, because there are no cycles in the tree and it is not possible to reach a node from multiple directions.","title":"Tree Traversal"},{"location":"Algorithms/Tree/Traversal/#implementation","text":"The typical way to traverse a tree is to start a depth-first search at an arbitrary node . The following recursive function can be used assumes that we are maintaining adjacency list 1 2 3 4 5 6 7 8 void dfs ( int s , int e ) { // current node s and previous node e //process node s for ( auto u : adj [ s ]) { if ( u != e ) dfs ( u , s ); } } dfs ( x , x ) // initial call because there's no self loop","title":"implementation"},{"location":"Algorithms/Tree/TreeQueries/","text":"Tree Queries \u00b6 What is the $k$th ancester of a node? What is the sum of values in the subtree of a node? what is the sum of values on a path between two nodes? what is the lowest common ancester of two nodes? Finding ancestors \u00b6 The $k$th ancestor of a node $x$ in a rooted tree is the node that we will reach if we move $k$ levels up from $x$. Let ancestor(x, k) denote the $k$th ancestor of node $x$. an easy way to calculate any value of ancestor(x, k) is to perform a sequence of $k$ moves in the tree. However, the time complexity of the method is $\\Omicron(k)$, which may be slow, because tree of $n$ nodes may have chain of $n$ nodes. Fortunately, using a technique similar to that used in here , any value of ancestor(x, k) can be efficiently calculated in $\\Omicron(lgk)$ time after pre processing. The idea is to precalculate all values ancestor(x, k) where $k\\leq n$ is a power of two. The preprocessing takes $\\Omicron(nlgn)$ time, because $\\Omicron(lgn)$ values are calculated for each node. After this, any value of ancestor(x, k) can be calculated in $\\Omicron(lgk)$ time by representing $k$ as a sum where each term is a power of two. Implementation \u00b6 1 not yet added Subtree and paths \u00b6 A tree traveral array contains the nodes of a rooted tree in the order in which a depth-first search from the root node visit them. example will be added we're working on it Subtree queries \u00b6 Each subtree of a tree corresponds to a subarray of the tree traversal array such that the first element of the subarray is the root node. Using this fact, we can efficiently process queries that are related to subtree of a tree. for example: update the value of a node, calculate the sum of values in the subtree of a node. The idea is to construct a tree traversal array that contains three values for each node: the identifier of the node, the size of the subtree, and the value of the node. Using this array we can calculate the sum of the values in any subtree by first finding out the size of the subtree and then the values of the corresponding nodes. To answer the queries efficiently, it suffices to store the values of the nodes in a binary indexed or segment tree .<< link will be added. After this, we can both update a value and calculate the sum of values in $\\Omicron(lgn)$ time. Implementation \u00b6 1 not yet added path queries \u00b6 Using a tree traversal array, we can also efficiently calculate sums of values on paths from the root node to any node of the tree. Consider a problem where our task is to support the following queries: change the value of a node calculate the sum of values on a path from the root to node We can solve this problem like before, but now each value in the last row of the array is the sum of values on a path from the root to the node. When the value of a node increase by $x$, the sums of all nodes in its subtree increase by $x$. Thus, to support both the operation, we should be able to increase all values in a range and retrieve a single value. This can be done in $\\Omicron(lgn)$ using a binary indexed or segment tree. Implementation \u00b6 1 not yet added Lowest Common Ancestor \u00b6 The lowest common ancestor of two nodes of a rooted tree is lowest node whose subtree contains both the nodes. A typical problem is to efficiently process queries that ask to find the lowest common ancestor of two nodes. Problems \u00b6 LCA LCA 2 LCA\uc640 \ucffc\ub9ac Method 1 \u00b6 One way to solve the problem is to use the fact that we can efficiently find the $k$th ancestor of any node in the tree. Using this, we can divide the problem of finding the lowest common ancestor into two parts. We use two pointers that initially point to the two nodes whose lowest common ancestor we should find. First, we move one of the pointers upwards so that both pointers point to nodes at the same level. After this, we determine the minimum number of steps needed to move both pointers upwards so that they will point to the same node. The node to which the pointers point after this is the lowest common ancestor. Since both parts of the algorithm can be performed in $\\Omicron(lgn)$ time using precomputed information, we can find the lowest common ancestor of any two nodes in $\\Omicron(lgn)$ time. Implementation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 //preprocess int log ( int x ) { int ans = 0 ; while ( x ) { x >>= 1 ; ans ++ ; } return ans ; } int MAXLOG = log ( MAXN ); int par [ MAXN ][ MAXLOG ]; // initially all -1 int h [ MAXN ]; void dfs ( int v , int p = - 1 ) { if ( p + 1 ) h [ v ] = h [ p ] + 1 ; par [ v ][ 0 ] = p ; for ( int i = 1 ; i < MAXLOG ; i ++ ) { if ( par [ v ][ i - 1 ] + 1 ) par [ v ][ i ] = par [ par [ v ][ i - 1 ]][ i - 1 ]; } for ( auto u : adj [ v ]) if ( p - u ) // equivalent to u != v dfs ( u , v ); } //query int LCA ( int v , int u ) { if ( h [ v ] < h [ u ]) swap ( v , u ) //v is bigger for ( int i = MAXLOG - 1 ; i >= 0 ; i -- ) if ( par [ v ][ i ] + 1 && h [ par [ v ][ i ]] >= h [ u ]) v = par [ v ][ i ]; //now h[v] == h[u] if ( v == u ) return v ; for ( int i = MAXLOG - 1 ; i >= 0 ; i -- ) if ( par [ v ][ i ] - par [ u ][ i ]) // par != par v = par [ v ][ i ], u = par [ u ][ i ]; return par [ v ][ 0 ]; } Method 2 \u00b6 Another way to solve the problem is based on a tree traversal array 1 . Once again, the idea is to traverse the nodes using depth-first search. However, we use a different tree traversal array: We add each node to th e array always the depth-first search walks through the node, and not only at the first visit. Hence, a node that has $k$ children appears $k+1$ times in the array and there are a total $2n-1$ nodes in the array. We store two values in the array: the identifier of the node and the depth of the node in the tree. Now we can find the lowest common ancestor of nodes $a$ and $b$ by finding the node with $\\text{minimum}$ depth between nodes $a$ and $b$ in the array Thus to find the lowest common ancestor of two nodes it suffices to process [range minimum query]. Since the array is static, we can process such queries in $\\Omicron(1)$ time after an $\\Omicron(nlgn)$ time preprocessing. Implementation \u00b6 1 2 will be added after RMQ is added https : //codeforces.com/blog/entry/16221 Distance of nodes \u00b6 The distance between nodes $a$ and $b$ equals the length of the path from $a$ to $b$. It turns out that the problem of calculating the distance between nodes reduces to finding their lowest common ancestor. First, we root the tree arbitrarily. After this, the distance of nodes $a$ and $b$ can be calculated using the formula $$ depth(a) + depth(b) - 2 \\sdot depth(c) $$ where $c$ is the lowest common ancestor of $a$ and $b$ and $depth(s)$ denotes the depth of node $s$ Offline algorithms \u00b6 Offline algorithms. those algorithms are given a set of queries which can be answered in any order. It is often easier to design an offline algorithm compared to an online algorithm Merging data structures \u00b6 One method to construct an offline algorithm is to perform depth-first tree traversal and maintain data structures in nodes. At each node $s$, we create a data structure d[s] that is based on the data structures of the children of $s$. Then using this data structure, all queries related to $s$ are processed. As an example, consider the following problem: We are given a tree where each node has some value. Our task is to process queries of the form \"calculate the number of nodes with value $x$ in the subtree of node $s$\". In this problem, we can use map structures to answer the queries. If we create such a data structure for each node, we can easily process all given queries, because we can handle all queries related to a node immediately after creating its data structure. However it would be too slow to create all data structure from scratch. Instead, at each node $s$, we create an initial data structure d[s] that only contains the values of $s$. After this, we go through the children of $s$ and merge d[s] and all data structures d[u] where $u$ is a child of $s$. The merging at node $s$ can be done as follows We go through the children of $s$ and at each child $u$ merge d[s] and d[u] . We always copy the contents from d[u] to d[s] . However, before this, we swap the contents of d[s] and d[u] if d[s] is smaller than d[u] . By doing this, each value is copied only $\\Omicron(lgn)$ times during the tree traversal, which ensures that the algorithm is efficient. To swap the contents of two data structres $a$ and $b$ efficiently, we can just use following code: 1 swap ( a , b ); It is guaranteed that the above code works in constant time when $a$ and $b$ are C++ standard library data structures. Lowest common ancestors \u00b6 There is also an offline algorithm for processing a set of lowest common ancestor queries. The algorithm is based on the union-find data structure , and the benefit of the algorithm is that it is easier to implement than the algorithm we discussed earlier. The algorithm is given as input a set of pairs of nodes, and it determines for each such pair the lowest common ancestor of the nodes. The algorithm performs a depth-first tree traversal and maintains disjoint sets of nodes. Initially, each node belongs to a separate set. For each set, we also store the highest node in the tree that belongs to the set. When the algorithm visits a node $x$, it goes through all nodes $y$ such that the lowest common ancestor of $x$ and $y$ has to be found. If $y$ has already been visited, the algorithm reports that the lowest common ancestor of $x$ and $y$ is the highest node in the set of $y$. Then, after processing node $x$, the algorithm joins the sets of $x$ and its parent. This technique is sometimes called the Euler tour technique. \u21a9","title":"TreeQueries"},{"location":"Algorithms/Tree/TreeQueries/#tree_queries","text":"What is the $k$th ancester of a node? What is the sum of values in the subtree of a node? what is the sum of values on a path between two nodes? what is the lowest common ancester of two nodes?","title":"Tree Queries"},{"location":"Algorithms/Tree/TreeQueries/#finding_ancestors","text":"The $k$th ancestor of a node $x$ in a rooted tree is the node that we will reach if we move $k$ levels up from $x$. Let ancestor(x, k) denote the $k$th ancestor of node $x$. an easy way to calculate any value of ancestor(x, k) is to perform a sequence of $k$ moves in the tree. However, the time complexity of the method is $\\Omicron(k)$, which may be slow, because tree of $n$ nodes may have chain of $n$ nodes. Fortunately, using a technique similar to that used in here , any value of ancestor(x, k) can be efficiently calculated in $\\Omicron(lgk)$ time after pre processing. The idea is to precalculate all values ancestor(x, k) where $k\\leq n$ is a power of two. The preprocessing takes $\\Omicron(nlgn)$ time, because $\\Omicron(lgn)$ values are calculated for each node. After this, any value of ancestor(x, k) can be calculated in $\\Omicron(lgk)$ time by representing $k$ as a sum where each term is a power of two.","title":"Finding ancestors"},{"location":"Algorithms/Tree/TreeQueries/#implementation","text":"1 not yet added","title":"Implementation"},{"location":"Algorithms/Tree/TreeQueries/#subtree_and_paths","text":"A tree traveral array contains the nodes of a rooted tree in the order in which a depth-first search from the root node visit them. example will be added we're working on it","title":"Subtree and paths"},{"location":"Algorithms/Tree/TreeQueries/#subtree_queries","text":"Each subtree of a tree corresponds to a subarray of the tree traversal array such that the first element of the subarray is the root node. Using this fact, we can efficiently process queries that are related to subtree of a tree. for example: update the value of a node, calculate the sum of values in the subtree of a node. The idea is to construct a tree traversal array that contains three values for each node: the identifier of the node, the size of the subtree, and the value of the node. Using this array we can calculate the sum of the values in any subtree by first finding out the size of the subtree and then the values of the corresponding nodes. To answer the queries efficiently, it suffices to store the values of the nodes in a binary indexed or segment tree .<< link will be added. After this, we can both update a value and calculate the sum of values in $\\Omicron(lgn)$ time.","title":"Subtree queries"},{"location":"Algorithms/Tree/TreeQueries/#implementation_1","text":"1 not yet added","title":"Implementation"},{"location":"Algorithms/Tree/TreeQueries/#path_queries","text":"Using a tree traversal array, we can also efficiently calculate sums of values on paths from the root node to any node of the tree. Consider a problem where our task is to support the following queries: change the value of a node calculate the sum of values on a path from the root to node We can solve this problem like before, but now each value in the last row of the array is the sum of values on a path from the root to the node. When the value of a node increase by $x$, the sums of all nodes in its subtree increase by $x$. Thus, to support both the operation, we should be able to increase all values in a range and retrieve a single value. This can be done in $\\Omicron(lgn)$ using a binary indexed or segment tree.","title":"path queries"},{"location":"Algorithms/Tree/TreeQueries/#implementation_2","text":"1 not yet added","title":"Implementation"},{"location":"Algorithms/Tree/TreeQueries/#lowest_common_ancestor","text":"The lowest common ancestor of two nodes of a rooted tree is lowest node whose subtree contains both the nodes. A typical problem is to efficiently process queries that ask to find the lowest common ancestor of two nodes.","title":"Lowest Common Ancestor"},{"location":"Algorithms/Tree/TreeQueries/#problems","text":"LCA LCA 2 LCA\uc640 \ucffc\ub9ac","title":"Problems"},{"location":"Algorithms/Tree/TreeQueries/#method_1","text":"One way to solve the problem is to use the fact that we can efficiently find the $k$th ancestor of any node in the tree. Using this, we can divide the problem of finding the lowest common ancestor into two parts. We use two pointers that initially point to the two nodes whose lowest common ancestor we should find. First, we move one of the pointers upwards so that both pointers point to nodes at the same level. After this, we determine the minimum number of steps needed to move both pointers upwards so that they will point to the same node. The node to which the pointers point after this is the lowest common ancestor. Since both parts of the algorithm can be performed in $\\Omicron(lgn)$ time using precomputed information, we can find the lowest common ancestor of any two nodes in $\\Omicron(lgn)$ time.","title":"Method 1"},{"location":"Algorithms/Tree/TreeQueries/#implementation_3","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 //preprocess int log ( int x ) { int ans = 0 ; while ( x ) { x >>= 1 ; ans ++ ; } return ans ; } int MAXLOG = log ( MAXN ); int par [ MAXN ][ MAXLOG ]; // initially all -1 int h [ MAXN ]; void dfs ( int v , int p = - 1 ) { if ( p + 1 ) h [ v ] = h [ p ] + 1 ; par [ v ][ 0 ] = p ; for ( int i = 1 ; i < MAXLOG ; i ++ ) { if ( par [ v ][ i - 1 ] + 1 ) par [ v ][ i ] = par [ par [ v ][ i - 1 ]][ i - 1 ]; } for ( auto u : adj [ v ]) if ( p - u ) // equivalent to u != v dfs ( u , v ); } //query int LCA ( int v , int u ) { if ( h [ v ] < h [ u ]) swap ( v , u ) //v is bigger for ( int i = MAXLOG - 1 ; i >= 0 ; i -- ) if ( par [ v ][ i ] + 1 && h [ par [ v ][ i ]] >= h [ u ]) v = par [ v ][ i ]; //now h[v] == h[u] if ( v == u ) return v ; for ( int i = MAXLOG - 1 ; i >= 0 ; i -- ) if ( par [ v ][ i ] - par [ u ][ i ]) // par != par v = par [ v ][ i ], u = par [ u ][ i ]; return par [ v ][ 0 ]; }","title":"Implementation"},{"location":"Algorithms/Tree/TreeQueries/#method_2","text":"Another way to solve the problem is based on a tree traversal array 1 . Once again, the idea is to traverse the nodes using depth-first search. However, we use a different tree traversal array: We add each node to th e array always the depth-first search walks through the node, and not only at the first visit. Hence, a node that has $k$ children appears $k+1$ times in the array and there are a total $2n-1$ nodes in the array. We store two values in the array: the identifier of the node and the depth of the node in the tree. Now we can find the lowest common ancestor of nodes $a$ and $b$ by finding the node with $\\text{minimum}$ depth between nodes $a$ and $b$ in the array Thus to find the lowest common ancestor of two nodes it suffices to process [range minimum query]. Since the array is static, we can process such queries in $\\Omicron(1)$ time after an $\\Omicron(nlgn)$ time preprocessing.","title":"Method 2"},{"location":"Algorithms/Tree/TreeQueries/#implementation_4","text":"1 2 will be added after RMQ is added https : //codeforces.com/blog/entry/16221","title":"Implementation"},{"location":"Algorithms/Tree/TreeQueries/#distance_of_nodes","text":"The distance between nodes $a$ and $b$ equals the length of the path from $a$ to $b$. It turns out that the problem of calculating the distance between nodes reduces to finding their lowest common ancestor. First, we root the tree arbitrarily. After this, the distance of nodes $a$ and $b$ can be calculated using the formula $$ depth(a) + depth(b) - 2 \\sdot depth(c) $$ where $c$ is the lowest common ancestor of $a$ and $b$ and $depth(s)$ denotes the depth of node $s$","title":"Distance of nodes"},{"location":"Algorithms/Tree/TreeQueries/#offline_algorithms","text":"Offline algorithms. those algorithms are given a set of queries which can be answered in any order. It is often easier to design an offline algorithm compared to an online algorithm","title":"Offline algorithms"},{"location":"Algorithms/Tree/TreeQueries/#merging_data_structures","text":"One method to construct an offline algorithm is to perform depth-first tree traversal and maintain data structures in nodes. At each node $s$, we create a data structure d[s] that is based on the data structures of the children of $s$. Then using this data structure, all queries related to $s$ are processed. As an example, consider the following problem: We are given a tree where each node has some value. Our task is to process queries of the form \"calculate the number of nodes with value $x$ in the subtree of node $s$\". In this problem, we can use map structures to answer the queries. If we create such a data structure for each node, we can easily process all given queries, because we can handle all queries related to a node immediately after creating its data structure. However it would be too slow to create all data structure from scratch. Instead, at each node $s$, we create an initial data structure d[s] that only contains the values of $s$. After this, we go through the children of $s$ and merge d[s] and all data structures d[u] where $u$ is a child of $s$. The merging at node $s$ can be done as follows We go through the children of $s$ and at each child $u$ merge d[s] and d[u] . We always copy the contents from d[u] to d[s] . However, before this, we swap the contents of d[s] and d[u] if d[s] is smaller than d[u] . By doing this, each value is copied only $\\Omicron(lgn)$ times during the tree traversal, which ensures that the algorithm is efficient. To swap the contents of two data structres $a$ and $b$ efficiently, we can just use following code: 1 swap ( a , b ); It is guaranteed that the above code works in constant time when $a$ and $b$ are C++ standard library data structures.","title":"Merging data structures"},{"location":"Algorithms/Tree/TreeQueries/#lowest_common_ancestors","text":"There is also an offline algorithm for processing a set of lowest common ancestor queries. The algorithm is based on the union-find data structure , and the benefit of the algorithm is that it is easier to implement than the algorithm we discussed earlier. The algorithm is given as input a set of pairs of nodes, and it determines for each such pair the lowest common ancestor of the nodes. The algorithm performs a depth-first tree traversal and maintains disjoint sets of nodes. Initially, each node belongs to a separate set. For each set, we also store the highest node in the tree that belongs to the set. When the algorithm visits a node $x$, it goes through all nodes $y$ such that the lowest common ancestor of $x$ and $y$ has to be found. If $y$ has already been visited, the algorithm reports that the lowest common ancestor of $x$ and $y$ is the highest node in the set of $y$. Then, after processing node $x$, the algorithm joins the sets of $x$ and its parent. This technique is sometimes called the Euler tour technique. \u21a9","title":"Lowest common ancestors"},{"location":"Contribute/CodeOfConduct/","text":"Code of Conduct \u00b6 Copy & go \u00b6 all codes should be working if you directly copy & paste to compiler. Compatibility with STL \u00b6 all the implementation should work with C++ STL. Example YourVector< int> v; ---snip--- sort(v.begin(), v.end()); // should be working","title":"CodeOfConduct"},{"location":"Contribute/CodeOfConduct/#code_of_conduct","text":"","title":"Code of Conduct"},{"location":"Contribute/CodeOfConduct/#copy_go","text":"all codes should be working if you directly copy & paste to compiler.","title":"Copy &amp; go"},{"location":"Contribute/CodeOfConduct/#compatibility_with_stl","text":"all the implementation should work with C++ STL. Example YourVector< int> v; ---snip--- sort(v.begin(), v.end()); // should be working","title":"Compatibility with STL"},{"location":"Contribute/Emoji/","text":"People :bowtie: :smile: :laughing: :blush: :smiley: :relaxed: :smirk: :heart_eyes: :kissing_heart: :kissing_closed_eyes: :flushed: :relieved: :satisfied: :grin: :wink: :stuck_out_tongue_winking_eye: :stuck_out_tongue_closed_eyes: :grinning: :kissing: :kissing_smiling_eyes: :stuck_out_tongue: :sleeping: :worried: :frowning: :anguished: :open_mouth: :grimacing: :confused: :hushed: :expressionless: :unamused: :sweat_smile: :sweat: :disappointed_relieved: :weary: :pensive: :disappointed: :confounded: :fearful: :cold_sweat: :persevere: :cry: :sob: :joy: :astonished: :scream: :neckbeard: :tired_face: :angry: :rage: :triumph: :sleepy: :yum: :mask: :sunglasses: :dizzy_face: :imp: :smiling_imp: :neutral_face: :no_mouth: :innocent: :alien: :yellow_heart: :blue_heart: :purple_heart: :heart: :green_heart: :broken_heart: :heartbeat: :heartpulse: :two_hearts: :revolving_hearts: :cupid: :sparkling_heart: :sparkles: :star: :star2: :dizzy: :boom: :collision: :anger: :exclamation: :question: :grey_exclamation: :grey_question: :zzz: :dash: :sweat_drops: :notes: :musical_note: :fire: :hankey: :poop: :shit: :+1: :thumbsup: :-1: :thumbsdown: :ok_hand: :punch: :facepunch: :fist: :v: :wave: :hand: :raised_hand: :open_hands: :point_up: :point_down: :point_left: :point_right: :raised_hands: :pray: :point_up_2: :clap: :muscle: :metal: :fu: :walking: :runner: :running: :couple: :family: :two_men_holding_hands: :two_women_holding_hands: :dancer: :dancers: :ok_woman: :no_good: :information_desk_person: :raising_hand: :bride_with_veil: :person_with_pouting_face: :person_frowning: :bow: :couplekiss: :couplekiss: :couple_with_heart: :massage: :haircut: :nail_care: :boy: :girl: :woman: :man: :baby: :older_woman: :older_man: :person_with_blond_hair: :man_with_gua_pi_mao: :man_with_turban: :construction_worker: :cop: :angel: :princess: :smiley_cat: :smile_cat: :heart_eyes_cat: :kissing_cat: :smirk_cat: :scream_cat: :crying_cat_face: :joy_cat: :pouting_cat: :japanese_ogre: :japanese_goblin: :see_no_evil: :hear_no_evil: :speak_no_evil: :guardsman: :skull: :feet: :lips: :kiss: :droplet: :ear: :eyes: :nose: :tongue: :love_letter: :bust_in_silhouette: :busts_in_silhouette: :speech_balloon: :thought_balloon: :feelsgood: :finnadie: :goberserk: :godmode: :hurtrealbad: :rage1: :rage2: :rage3: :rage4: :suspect: :trollface: Nature :sunny: :umbrella: :cloud: :snowflake: :snowman: :zap: :cyclone: :foggy: :ocean: :cat: :dog: :mouse: :hamster: :rabbit: :wolf: :frog: :tiger: :koala: :bear: :pig: :pig_nose: :cow: :boar: :monkey_face: :monkey: :horse: :racehorse: :camel: :sheep: :elephant: :panda_face: :snake: :bird: :baby_chick: :hatched_chick: :hatching_chick: :chicken: :penguin: :turtle: :bug: :honeybee: :ant: :beetle: :snail: :octopus: :tropical_fish: :fish: :whale: :whale2: :dolphin: :cow2: :ram: :rat: :water_buffalo: :tiger2: :rabbit2: :dragon: :goat: :rooster: :dog2: :pig2: :mouse2: :ox: :dragon_face: :blowfish: :crocodile: :dromedary_camel: :leopard: :cat2: :poodle: :paw_prints: :bouquet: :cherry_blossom: :tulip: :four_leaf_clover: :rose: :sunflower: :hibiscus: :maple_leaf: :leaves: :fallen_leaf: :herb: :mushroom: :cactus: :palm_tree: :evergreen_tree: :deciduous_tree: :chestnut: :seedling: :blossom: :ear_of_rice: :shell: :globe_with_meridians: :sun_with_face: :full_moon_with_face: :new_moon_with_face: :new_moon: :waxing_crescent_moon: :first_quarter_moon: :waxing_gibbous_moon: :full_moon: :waning_gibbous_moon: :last_quarter_moon: :waning_crescent_moon: :last_quarter_moon_with_face: :first_quarter_moon_with_face: :moon: :earth_africa: :earth_americas: :earth_asia: :volcano: :milky_way: :partly_sunny: :octocat: :squirrel: Objects :bamboo: :gift_heart: :dolls: :school_satchel: :mortar_board: :flags: :fireworks: :sparkler: :wind_chime: :rice_scene: :jack_o_lantern: :ghost: :santa: :christmas_tree: :gift: :bell: :no_bell: :tanabata_tree: :tada: :confetti_ball: :balloon: :crystal_ball: :cd: :dvd: :floppy_disk: :camera: :video_camera: :movie_camera: :computer: :tv: :iphone: :phone: :telephone: :telephone_receiver: :pager: :fax: :minidisc: :vhs: :sound: :speaker: :mute: :loudspeaker: :mega: :hourglass: :hourglass_flowing_sand: :alarm_clock: :watch: :radio: :satellite: :loop: :mag: :mag_right: :unlock: :lock: :lock_with_ink_pen: :closed_lock_with_key: :key: :bulb: :flashlight: :high_brightness: :low_brightness: :electric_plug: :battery: :calling: :email: :mailbox: :postbox: :bath: :bathtub: :shower: :toilet: :wrench: :nut_and_bolt: :hammer: :seat: :moneybag: :yen: :dollar: :pound: :euro: :credit_card: :money_with_wings: :e-mail: :inbox_tray: :outbox_tray: :envelope: :incoming_envelope: :postal_horn: :mailbox_closed: :mailbox_with_mail: :mailbox_with_no_mail: :door: :smoking: :bomb: :gun: :hocho: :pill: :syringe: :page_facing_up: :page_with_curl: :bookmark_tabs: :bar_chart: :chart_with_upwards_trend: :chart_with_downwards_trend: :scroll: :clipboard: :calendar: :date: :card_index: :file_folder: :open_file_folder: :scissors: :pushpin: :paperclip: :black_nib: :pencil2: :straight_ruler: :triangular_ruler: :closed_book: :green_book: :blue_book: :orange_book: :notebook: :notebook_with_decorative_cover: :ledger: :books: :bookmark: :name_badge: :microscope: :telescope: :newspaper: :football: :basketball: :soccer: :baseball: :tennis: :8ball: :rugby_football: :bowling: :golf: :mountain_bicyclist: :bicyclist: :horse_racing: :snowboarder: :swimmer: :surfer: :ski: :spades: :hearts: :clubs: :diamonds: :gem: :ring: :trophy: :musical_score: :musical_keyboard: :violin: :space_invader: :video_game: :black_joker: :flower_playing_cards: :game_die: :dart: :mahjong: :clapper: :memo: :pencil: :book: :art: :microphone: :headphones: :trumpet: :saxophone: :guitar: :shoe: :sandal: :high_heel: :lipstick: :boot: :shirt: :tshirt: :necktie: :womans_clothes: :dress: :running_shirt_with_sash: :jeans: :kimono: :bikini: :ribbon: :tophat: :crown: :womans_hat: :mans_shoe: :closed_umbrella: :briefcase: :handbag: :pouch: :purse: :eyeglasses: :fishing_pole_and_fish: :coffee: :tea: :sake: :baby_bottle: :beer: :beers: :cocktail: :tropical_drink: :wine_glass: :fork_and_knife: :pizza: :hamburger: :fries: :poultry_leg: :meat_on_bone: :spaghetti: :curry: :fried_shrimp: :bento: :sushi: :fish_cake: :rice_ball: :rice_cracker: :rice: :ramen: :stew: :oden: :dango: :egg: :bread: :doughnut: :custard: :icecream: :ice_cream: :shaved_ice: :birthday: :cake: :cookie: :chocolate_bar: :candy: :lollipop: :honey_pot: :apple: :green_apple: :tangerine: :lemon: :cherries: :grapes: :watermelon: :strawberry: :peach: :melon: :banana: :pear: :pineapple: :sweet_potato: :eggplant: :tomato: :corn: Places :house: :house_with_garden: :school: :office: :post_office: :hospital: :bank: :convenience_store: :love_hotel: :hotel: :wedding: :church: :department_store: :european_post_office: :city_sunrise: :city_sunset: :japanese_castle: :european_castle: :tent: :factory: :tokyo_tower: :japan: :mount_fuji: :sunrise_over_mountains: :sunrise: :stars: :statue_of_liberty: :bridge_at_night: :carousel_horse: :rainbow: :ferris_wheel: :fountain: :roller_coaster: :ship: :speedboat: :boat: :sailboat: :rowboat: :anchor: :rocket: :airplane: :helicopter: :steam_locomotive: :tram: :mountain_railway: :bike: :aerial_tramway: :suspension_railway: :mountain_cableway: :tractor: :blue_car: :oncoming_automobile: :car: :red_car: :taxi: :oncoming_taxi: :articulated_lorry: :bus: :oncoming_bus: :rotating_light: :police_car: :oncoming_police_car: :fire_engine: :ambulance: :minibus: :truck: :train: :station: :train2: :bullettrain_front: :bullettrain_side: :light_rail: :monorail: :railway_car: :trolleybus: :ticket: :fuelpump: :vertical_traffic_light: :traffic_light: :warning: :construction: :beginner: :atm: :slot_machine: :busstop: :barber: :hotsprings: :checkered_flag: :crossed_flags: :izakaya_lantern: :moyai: :circus_tent: :performing_arts: :round_pushpin: :triangular_flag_on_post: :jp: :kr: :cn: :us: :fr: :es: :it: :ru: :gb: :uk: :de: Symbols :one: :two: :three: :four: :five: :six: :seven: :eight: :nine: :keycap_ten: :1234: :zero: :hash: :symbols: :arrow_backward: :arrow_down: :arrow_forward: :arrow_left: :capital_abcd: :abcd: :abc: :arrow_lower_left: :arrow_lower_right: :arrow_right: :arrow_up: :arrow_upper_left: :arrow_upper_right: :arrow_double_down: :arrow_double_up: :arrow_down_small: :arrow_heading_down: :arrow_heading_up: :leftwards_arrow_with_hook: :arrow_right_hook: :left_right_arrow: :arrow_up_down: :arrow_up_small: :arrows_clockwise: :arrows_counterclockwise: :rewind: :fast_forward: :information_source: :ok: :twisted_rightwards_arrows: :repeat: :repeat_one: :new: :top: :up: :cool: :free: :ng: :cinema: :koko: :signal_strength: :u5272: :u5408: :u55b6: :u6307: :u6708: :u6709: :u6e80: :u7121: :u7533: :u7a7a: :u7981: :sa: :restroom: :mens: :womens: :baby_symbol: :no_smoking: :parking: :wheelchair: :metro: :baggage_claim: :accept: :wc: :potable_water: :put_litter_in_its_place: :secret: :congratulations: :m: :passport_control: :left_luggage: :customs: :ideograph_advantage: :cl: :sos: :id: :no_entry_sign: :underage: :no_mobile_phones: :do_not_litter: :non-potable_water: :no_bicycles: :no_pedestrians: :children_crossing: :no_entry: :eight_spoked_asterisk: :eight_pointed_black_star: :heart_decoration: :vs: :vibration_mode: :mobile_phone_off: :chart: :currency_exchange: :aries: :taurus: :gemini: :cancer: :leo: :virgo: :libra: :scorpius: :sagittarius: :capricorn: :aquarius: :pisces: :ophiuchus: :six_pointed_star: :negative_squared_cross_mark: :a: :b: :ab: :o2: :diamond_shape_with_a_dot_inside: :recycle: :end: :on: :soon: :clock1: :clock130: :clock10: :clock1030: :clock11: :clock1130: :clock12: :clock1230: :clock2: :clock230: :clock3: :clock330: :clock4: :clock430: :clock5: :clock530: :clock6: :clock630: :clock7: :clock730: :clock8: :clock830: :clock9: :clock930: :heavy_dollar_sign: :copyright: :registered: :tm: :x: :heavy_exclamation_mark: :bangbang: :interrobang: :o: :heavy_multiplication_x: :heavy_plus_sign: :heavy_minus_sign: :heavy_division_sign: :white_flower: :100: :heavy_check_mark: :ballot_box_with_check: :radio_button: :link: :curly_loop: :wavy_dash: :part_alternation_mark: :trident: :black_square: :black_square: :white_square: :white_square: :white_check_mark: :black_square_button: :white_square_button: :black_circle: :white_circle: :red_circle: :large_blue_circle: :large_blue_diamond: :large_orange_diamond: :small_blue_diamond: :small_orange_diamond: :small_red_triangle: :small_red_triangle_down: :shipit:","title":"Emoji"},{"location":"Contribute/HowToContribute/","text":"How To Contribute \u00b6 This is My first open source project under very active development and is also being used to ship code to everybody on codeforces , AtCoder , HackerRank , LeetCode , BaekJoonOnlineJudge and so on. I'm still working out to make contributing to this project as easy and transparent as possible, but I'm not quite there yet. Hopefully this document makes the process for contributing clear and answers some quiestions that you may have. Code of Conduct \u00b6 I adopted a Code of Conduct that i expect project participants to adhere to. You can see full document of Code Of Conduct . 1. copy & pastable \u00b6 All the codes in this site are ready-to-be-compiled that means you could just copy & paste it to see it works. 2. Compatibility \u00b6 Data Structure implementations should be compatible with C++ STL. ex) sort(Your_implementation.begin(), Your_implementation.end()) should work. Use template for your convinience \u00b6 there's a template for contribute","title":"HowToContribute"},{"location":"Contribute/HowToContribute/#how_to_contribute","text":"This is My first open source project under very active development and is also being used to ship code to everybody on codeforces , AtCoder , HackerRank , LeetCode , BaekJoonOnlineJudge and so on. I'm still working out to make contributing to this project as easy and transparent as possible, but I'm not quite there yet. Hopefully this document makes the process for contributing clear and answers some quiestions that you may have.","title":"How To Contribute"},{"location":"Contribute/HowToContribute/#code_of_conduct","text":"I adopted a Code of Conduct that i expect project participants to adhere to. You can see full document of Code Of Conduct .","title":"Code of Conduct"},{"location":"Contribute/HowToContribute/#1_copy_pastable","text":"All the codes in this site are ready-to-be-compiled that means you could just copy & paste it to see it works.","title":"1. copy &amp; pastable"},{"location":"Contribute/HowToContribute/#2_compatibility","text":"Data Structure implementations should be compatible with C++ STL. ex) sort(Your_implementation.begin(), Your_implementation.end()) should work.","title":"2. Compatibility"},{"location":"Contribute/HowToContribute/#use_template_for_your_convinience","text":"there's a template for contribute","title":"Use template for your convinience"},{"location":"Contribute/Template/","text":"Vector \u00b6 Brief explanation. Operations & time complexity \u00b6 Methods RunningTime push_back(val) O(1) pop() O(1) empty() O(1) you can use table generator Implementation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #include <cstido> #include <iostream> template < typename T > class Vector { //use TitleCase for DataStructure implementation --- snip --- } int main () { //few lines of code to test your implementation Vector < int > v ; for ( int i = 0 ; i < n ; i ++ ) { v . push_back ( rand () % 100 ); } sort ( v . begin (), v . end ()) for ( auto x : v ) { cout << x << ' ' ; } } keep your implementation self-contained. Related Problems \u00b6 title of easy problem Some hard problem Lily want a phone add difficulty information(optional) Related Topics \u00b6 Stack Analysis (Optional) \u00b6 You can add some mathematical things here using KaTex as a block tag $$ T(N) = O(N*M) $$ or as a inline tag $T(N) = O(N) $ Contributers (Optional) \u00b6 07.12.2019 contributer1 07.14.2019 typo correction contributer2 07.15.2019 add new section \"Analysis\" contributer3 07.18.2019 fix bugs in \"implementation\" contributer4 07.19.2019 improved performance \"implementation\" contributer5 07.23.2019 refactoring \"implementation\" contributer6 08.02.2019 add related problems contributer7","title":"Template"},{"location":"Contribute/Template/#vector","text":"Brief explanation.","title":"Vector"},{"location":"Contribute/Template/#operations_time_complexity","text":"Methods RunningTime push_back(val) O(1) pop() O(1) empty() O(1) you can use table generator","title":"Operations &amp; time complexity"},{"location":"Contribute/Template/#implementation","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #include <cstido> #include <iostream> template < typename T > class Vector { //use TitleCase for DataStructure implementation --- snip --- } int main () { //few lines of code to test your implementation Vector < int > v ; for ( int i = 0 ; i < n ; i ++ ) { v . push_back ( rand () % 100 ); } sort ( v . begin (), v . end ()) for ( auto x : v ) { cout << x << ' ' ; } } keep your implementation self-contained.","title":"Implementation"},{"location":"Contribute/Template/#related_problems","text":"title of easy problem Some hard problem Lily want a phone add difficulty information(optional)","title":"Related Problems"},{"location":"Contribute/Template/#related_topics","text":"Stack","title":"Related Topics"},{"location":"Contribute/Template/#analysis_optional","text":"You can add some mathematical things here using KaTex as a block tag $$ T(N) = O(N*M) $$ or as a inline tag $T(N) = O(N) $","title":"Analysis (Optional)"},{"location":"Contribute/Template/#contributers_optional","text":"07.12.2019 contributer1 07.14.2019 typo correction contributer2 07.15.2019 add new section \"Analysis\" contributer3 07.18.2019 fix bugs in \"implementation\" contributer4 07.19.2019 improved performance \"implementation\" contributer5 07.23.2019 refactoring \"implementation\" contributer6 08.02.2019 add related problems contributer7","title":"Contributers (Optional)"},{"location":"DataStructures/HashTables/HashFunctions/","text":"Hash Function \u00b6 A hash function is any function that can be used to map data of arbitrary size onto data of a fixed size. Hash Functions \u00b6 1. DJB2 \u00b6 this algorithm (k=33) was first reported by dan bernstein many years ago in comp.lang.c. another version of this algorithm (now favored by bernstein) uses xor: hash(i) = hash(i-1) * 33 ^ str[i]; the magic of number 33 (why it works better than many other constants, prime or not) has never adequately explained 1 2 3 4 5 6 7 8 9 10 unsigned long long djb2 ( char * str ) { unsigned long long hash = 5381 ; int c ; while (( c = * ( str ++ ))) { hash = ( hash << 5 ) + hash + c ; } return hash ; } 2. sdbm \u00b6 this algorithm was created for sdbm (a public-domain reimplementation of ndbm) database library. it was found to do well in scrambling bits, causing better distribution of the keys and fewer splits. it also happens to be a good general hashing function with good distribution. the actual function is hash(i) = hash(i - 1) * 65599 + str[i];; what is included below is faster version used in gawk. (there iseven a faster, duff's device version) the magic constant 65599 was picked out of thin air while experimenting with different constants, and turns out to be a prime. this is one of the algorithms used in berkeley db (see sleepy cat) and else where 1 2 3 4 5 6 7 8 9 10 unsigned long long sdbm ( char * str ) { unsigned long long hash = 5381 ; int c ; while (( c = * ( str ++ ))) { hash = c + ( hash << 6 ) + ( hash << 16 ) - hash ; } return hash ; } 3. lose lose \u00b6 This hash function appeared in K&R (1st ed) but at least the reader was warned: \"This is not the best possible algorithm, but it has the merit of extreme simplicity\". This is an understatement; It is a terrible hashing algorithm, and it could have been much better without scarificing its \"extreme simplicity.\" Many C programmers use this function without actually testing it, or checking something like Knuth's Sorting and searching, so it stuck. It is now found mixed with other respectable code, eg.cnews. Warning Don't use this algorithm, it's terrible. 1 2 3 4 5 6 7 8 9 10 unsigned long long loseLose ( char * str ) { unsigned long long hash = 0 ; int c ; while (( c = * ( str ++ ))) { hash += c ; } return hash ; }","title":"Hash Functions"},{"location":"DataStructures/HashTables/HashFunctions/#hash_function","text":"A hash function is any function that can be used to map data of arbitrary size onto data of a fixed size.","title":"Hash Function"},{"location":"DataStructures/HashTables/HashFunctions/#hash_functions","text":"","title":"Hash Functions"},{"location":"DataStructures/HashTables/HashFunctions/#1_djb2","text":"this algorithm (k=33) was first reported by dan bernstein many years ago in comp.lang.c. another version of this algorithm (now favored by bernstein) uses xor: hash(i) = hash(i-1) * 33 ^ str[i]; the magic of number 33 (why it works better than many other constants, prime or not) has never adequately explained 1 2 3 4 5 6 7 8 9 10 unsigned long long djb2 ( char * str ) { unsigned long long hash = 5381 ; int c ; while (( c = * ( str ++ ))) { hash = ( hash << 5 ) + hash + c ; } return hash ; }","title":"1. DJB2"},{"location":"DataStructures/HashTables/HashFunctions/#2_sdbm","text":"this algorithm was created for sdbm (a public-domain reimplementation of ndbm) database library. it was found to do well in scrambling bits, causing better distribution of the keys and fewer splits. it also happens to be a good general hashing function with good distribution. the actual function is hash(i) = hash(i - 1) * 65599 + str[i];; what is included below is faster version used in gawk. (there iseven a faster, duff's device version) the magic constant 65599 was picked out of thin air while experimenting with different constants, and turns out to be a prime. this is one of the algorithms used in berkeley db (see sleepy cat) and else where 1 2 3 4 5 6 7 8 9 10 unsigned long long sdbm ( char * str ) { unsigned long long hash = 5381 ; int c ; while (( c = * ( str ++ ))) { hash = c + ( hash << 6 ) + ( hash << 16 ) - hash ; } return hash ; }","title":"2. sdbm"},{"location":"DataStructures/HashTables/HashFunctions/#3_lose_lose","text":"This hash function appeared in K&R (1st ed) but at least the reader was warned: \"This is not the best possible algorithm, but it has the merit of extreme simplicity\". This is an understatement; It is a terrible hashing algorithm, and it could have been much better without scarificing its \"extreme simplicity.\" Many C programmers use this function without actually testing it, or checking something like Knuth's Sorting and searching, so it stuck. It is now found mixed with other respectable code, eg.cnews. Warning Don't use this algorithm, it's terrible. 1 2 3 4 5 6 7 8 9 10 unsigned long long loseLose ( char * str ) { unsigned long long hash = 0 ; int c ; while (( c = * ( str ++ ))) { hash += c ; } return hash ; }","title":"3. lose lose"},{"location":"DataStructures/HashTables/Preface/","text":"Preface \u00b6 Many applications require a dynamic set that supports only the dictionary operations. A Hash Table is an effective data structure for implementing dictionaries. A hash table typically uses an array of size proportional to the number of keys actually stored. Hash functions \u00b6 Instead of using the key as an array index directly, the array index is computed from the key Dealing with collisions \u00b6 Collision: two keys hash to the same slot. Since a hash table uses array of size relatively small to the number of possible keys, there is a chance to collisions in which more than one key maps to the same array index Chaining OpenAddressing PerfectHashing OPERATIONS average worst average worst average worst INSERT $O(1)$ - - SEARCH $O(n/m)$ $O(n)$ $O(1)$ $O(1)$ DELETE $O(1)$ - - 1. Chaining \u00b6 In Chaining, we place all the elements that hash to the same slot in to the same linked llist 2. Open Addressing \u00b6 Resolve Collisions with iterative hashing Perfect Hashing \u00b6 Perfect Hasing uses second level Hashtable that has no collision. perfect hashing can support searches in $O(1)\\ wosrt-case$ time, when the set is static(!= dynamic)","title":"Preface"},{"location":"DataStructures/HashTables/Preface/#preface","text":"Many applications require a dynamic set that supports only the dictionary operations. A Hash Table is an effective data structure for implementing dictionaries. A hash table typically uses an array of size proportional to the number of keys actually stored.","title":"Preface"},{"location":"DataStructures/HashTables/Preface/#hash_functions","text":"Instead of using the key as an array index directly, the array index is computed from the key","title":"Hash functions"},{"location":"DataStructures/HashTables/Preface/#dealing_with_collisions","text":"Collision: two keys hash to the same slot. Since a hash table uses array of size relatively small to the number of possible keys, there is a chance to collisions in which more than one key maps to the same array index Chaining OpenAddressing PerfectHashing OPERATIONS average worst average worst average worst INSERT $O(1)$ - - SEARCH $O(n/m)$ $O(n)$ $O(1)$ $O(1)$ DELETE $O(1)$ - -","title":"Dealing with collisions"},{"location":"DataStructures/HashTables/Preface/#1_chaining","text":"In Chaining, we place all the elements that hash to the same slot in to the same linked llist","title":"1. Chaining"},{"location":"DataStructures/HashTables/Preface/#2_open_addressing","text":"Resolve Collisions with iterative hashing","title":"2. Open Addressing"},{"location":"DataStructures/HashTables/Preface/#perfect_hashing","text":"Perfect Hasing uses second level Hashtable that has no collision. perfect hashing can support searches in $O(1)\\ wosrt-case$ time, when the set is static(!= dynamic)","title":"Perfect Hashing"},{"location":"DataStructures/Linear/Heap/","text":"Heap \u00b6 The (binary) heap data structure is an array object that we can view as a nearly complete binary tree. An array A that represent a heap is an obejct with two attributes: $A.length$, $A.heap-size$ The root of the tree is $A[1]$, and given the index $i$ of a node, we can easily compute the indices of its parent, left child and right child. The values in the nodes satisfy a heap property. heap property max-heap-property: A[parent(i)] >= A[i]. min-heap-property: A[parent(i)] <= A[i]. priority queue often $heap$ is implemented as priority queue, because of space complexity of heap, space complexity: $\\Omicron(n)$ Operations \u00b6 Member Function Running Time max_heapify() $\\Omicron(lg(n))$ build_max_heap() $\\Omicron(n)$ heapsort() $\\Omicron(nlg(n))$ max_heap_insert() $\\Omicron(lg(n))$ heap_increase_key() $\\Omicron(lg(n))$ heap_maximum() $\\Omicron(lg(n))$ Applications \u00b6 HeapSort Priority queue: A priority queue is an abstract concept like \"a list\" or \" map\"; just as a list can be implemented with a linked list or an array, a priority queue can be implemented with heap or a variety of other methods. Graph algorithms Selection algorithms Implementation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 #include <iostream> using namespace std ; template < typename T > void _swap ( T & a , T & b ) { T temp = a ; a = b ; b = temp ; } template < typename T > class heap { int _size ; int _capacity ; T * _buf ; inline int parent ( int i ) { return ( i - 1 ) >> 1 ; } inline int left ( int i ) { return ( i << 1 ) + 1 ; } inline int right ( int i ) { return ( i << 1 ) + 2 ; } void max_heapify ( int i ) { int largest ; int l = left ( i ); int r = right ( i ); if ( l < _size && _buf [ l ] > _buf [ i ]) { largest = l ; } else { largest = i ; } if ( r < _size && _buf [ r ] > _buf [ largest ]) { largest = r ; } if ( largest != i ) { _swap ( _buf [ largest ], _buf [ i ]); max_heapify ( largest ); } } void reserve ( int n ) { if ( n <= _size ) return ; T * temp = new T [ n ]; for ( int i = 0 ; i < _size ; i ++ ) { temp [ i ] = _buf [ i ]; } _capacity = n ; delete [] _buf ; _buf = temp ; } public : heap () : _size ( 0 ), _capacity ( 0 ), _buf ( 0 ){} heap ( int n ) { _size = n ; _capacity = n ; _buf = new T [ n ]; for ( int i = 0 ; i < n ; i ++ ) { _buf [ i ] = rand () % 400 ; } } void build_max_heap () { for ( int i = parent ( _size - 1 ); i >= 0 ; i -- ) max_heapify ( i ); } void heapsort () { build_max_heap (); int original_size = _size ; for ( int i = _size - 1 ; i > 0 ; i -- ) { swap ( _buf [ 0 ], _buf [ i ]); _size -- ; max_heapify ( 0 ); } _size = original_size ; } // priority queue; T maximum () { return _buf [ 0 ]; } T extract_max () { if ( _size == 0 ) { cout << \"underflow\" ; return - 1 ; } int max = _buf [ 0 ]; _buf [ 0 ] = _buf [ -- _size ]; max_heapify ( 0 ); return max ; } void increase_key ( int i , T key ) { if ( key < _buf [ i ]) { cout << \"new key is smaller than current key\" << endl ; return ; } _buf [ i ] = key ; while ( i > 0 && _buf [ parent ( i )] < _buf [ i ]) { _swap ( _buf [ i ], _buf [ parent ( i )]); i = parent ( i ); } } void insert ( T key ) { if ( _size == 0 ) { reserve ( 1 ); } else if ( _size == _capacity ) { reserve ( _size << 1 ); } _buf [ _size ] = key - 1 ; increase_key ( _size ++ , key ); } void print () { for ( int i = 0 ; i < _size ; i ++ ) { cout << _buf [ i ] << ' ' ; } cout << endl ; } }; int main () { heap < int > q ( 30 ); // make random heap; cout << \"================= random ===============\" << endl ; q . print (); cout << \"========== after build-max-heap=========\" << endl ; q . build_max_heap (); q . print (); cout << \"============ after heapsort ============\" << endl ; q . heapsort (); q . print (); cout << \"============ priority queue ===========\" << endl ; heap < int > pq ; for ( int i = 0 ; i < 1000 ; i ++ ) { pq . insert ( rand () % 100 ); } for ( int i = 0 ; i < 1000 ; i ++ ) { cout << pq . extract_max () << ' ' ; } //pq.print(); return 0 ; }","title":"Heap"},{"location":"DataStructures/Linear/Heap/#heap","text":"The (binary) heap data structure is an array object that we can view as a nearly complete binary tree. An array A that represent a heap is an obejct with two attributes: $A.length$, $A.heap-size$ The root of the tree is $A[1]$, and given the index $i$ of a node, we can easily compute the indices of its parent, left child and right child. The values in the nodes satisfy a heap property. heap property max-heap-property: A[parent(i)] >= A[i]. min-heap-property: A[parent(i)] <= A[i]. priority queue often $heap$ is implemented as priority queue, because of space complexity of heap, space complexity: $\\Omicron(n)$","title":"Heap"},{"location":"DataStructures/Linear/Heap/#operations","text":"Member Function Running Time max_heapify() $\\Omicron(lg(n))$ build_max_heap() $\\Omicron(n)$ heapsort() $\\Omicron(nlg(n))$ max_heap_insert() $\\Omicron(lg(n))$ heap_increase_key() $\\Omicron(lg(n))$ heap_maximum() $\\Omicron(lg(n))$","title":"Operations"},{"location":"DataStructures/Linear/Heap/#applications","text":"HeapSort Priority queue: A priority queue is an abstract concept like \"a list\" or \" map\"; just as a list can be implemented with a linked list or an array, a priority queue can be implemented with heap or a variety of other methods. Graph algorithms Selection algorithms","title":"Applications"},{"location":"DataStructures/Linear/Heap/#implementation","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 #include <iostream> using namespace std ; template < typename T > void _swap ( T & a , T & b ) { T temp = a ; a = b ; b = temp ; } template < typename T > class heap { int _size ; int _capacity ; T * _buf ; inline int parent ( int i ) { return ( i - 1 ) >> 1 ; } inline int left ( int i ) { return ( i << 1 ) + 1 ; } inline int right ( int i ) { return ( i << 1 ) + 2 ; } void max_heapify ( int i ) { int largest ; int l = left ( i ); int r = right ( i ); if ( l < _size && _buf [ l ] > _buf [ i ]) { largest = l ; } else { largest = i ; } if ( r < _size && _buf [ r ] > _buf [ largest ]) { largest = r ; } if ( largest != i ) { _swap ( _buf [ largest ], _buf [ i ]); max_heapify ( largest ); } } void reserve ( int n ) { if ( n <= _size ) return ; T * temp = new T [ n ]; for ( int i = 0 ; i < _size ; i ++ ) { temp [ i ] = _buf [ i ]; } _capacity = n ; delete [] _buf ; _buf = temp ; } public : heap () : _size ( 0 ), _capacity ( 0 ), _buf ( 0 ){} heap ( int n ) { _size = n ; _capacity = n ; _buf = new T [ n ]; for ( int i = 0 ; i < n ; i ++ ) { _buf [ i ] = rand () % 400 ; } } void build_max_heap () { for ( int i = parent ( _size - 1 ); i >= 0 ; i -- ) max_heapify ( i ); } void heapsort () { build_max_heap (); int original_size = _size ; for ( int i = _size - 1 ; i > 0 ; i -- ) { swap ( _buf [ 0 ], _buf [ i ]); _size -- ; max_heapify ( 0 ); } _size = original_size ; } // priority queue; T maximum () { return _buf [ 0 ]; } T extract_max () { if ( _size == 0 ) { cout << \"underflow\" ; return - 1 ; } int max = _buf [ 0 ]; _buf [ 0 ] = _buf [ -- _size ]; max_heapify ( 0 ); return max ; } void increase_key ( int i , T key ) { if ( key < _buf [ i ]) { cout << \"new key is smaller than current key\" << endl ; return ; } _buf [ i ] = key ; while ( i > 0 && _buf [ parent ( i )] < _buf [ i ]) { _swap ( _buf [ i ], _buf [ parent ( i )]); i = parent ( i ); } } void insert ( T key ) { if ( _size == 0 ) { reserve ( 1 ); } else if ( _size == _capacity ) { reserve ( _size << 1 ); } _buf [ _size ] = key - 1 ; increase_key ( _size ++ , key ); } void print () { for ( int i = 0 ; i < _size ; i ++ ) { cout << _buf [ i ] << ' ' ; } cout << endl ; } }; int main () { heap < int > q ( 30 ); // make random heap; cout << \"================= random ===============\" << endl ; q . print (); cout << \"========== after build-max-heap=========\" << endl ; q . build_max_heap (); q . print (); cout << \"============ after heapsort ============\" << endl ; q . heapsort (); q . print (); cout << \"============ priority queue ===========\" << endl ; heap < int > pq ; for ( int i = 0 ; i < 1000 ; i ++ ) { pq . insert ( rand () % 100 ); } for ( int i = 0 ; i < 1000 ; i ++ ) { cout << pq . extract_max () << ' ' ; } //pq.print(); return 0 ; }","title":"Implementation"},{"location":"DataStructures/Linear/LinkedList/","text":"LinkedList(Doubly Linked List) \u00b6 A linked list is a linear data structure, in which the elements are not stored at contiguous memory locations. The elements in a linked lists are linked using pointers. Operations & time complexity \u00b6 Member Function Running Time insert_front() $\\Omicron(1)$ insert_back() $\\Omicron(1)$ insert_after() $\\Omicron(1)$ erase $\\Omicron(1)$ search() $\\Omicron(n)$ Implementation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 #include <bits/stdc++.h> using namespace std ; template < typename T > class LinkedList { struct Node { Node * before ; Node * next ; T data ; Node ( T data ) : before ( 0 ), next ( 0 ), data ( data ){} ~ Node () { delete before ; delete next ; delete data ; } }; Node * tail ; unsigned int _size ; public : Node * head ; LinkedList () : head ( 0 ), tail ( 0 ) {} void insert_front ( T val ) { Node * temp = new Node ( val ); if ( head == 0 ) { head = temp ; tail = temp ; } else { temp -> next = head ; head -> before = temp ; head = temp ; } } void insert_back ( T val ) { Node * temp = new Node ( val ); if ( tail == 0 ) { head = temp ; tail = temp ; } else { temp -> before = tail ; tail -> next = temp ; tail = temp ; } } void insert_after ( Node * node , T val ) { Node * temp = new Node ( val ); if ( temp -> next == 0 ) { tail = temp ; } temp -> next = node -> next ; temp -> next -> before = temp ; node -> next = temp ; temp -> before = node ; } Node * search ( T val ) { //search_from head Node * it = head ; while ( it != 0 && it -> data != val ) it = it -> next ; return it ; } void erase ( Node * node ) { if ( node == 0 ) return ; if ( node -> next == 0 ) { tail = node -> before ; tail -> next = 0 ; } else if ( node -> before == 0 ) { head = node -> next ; head -> before = 0 ; } else { node -> before -> next = node -> next ; node -> next = node -> before ; } delete node ; } void print () { Node * it = head ; while ( it != 0 ) { cout << it -> data << ' ' ; it = it -> next ; } } }; int main () { LinkedList < int > list ; for ( int i = 0 ; i < 100 ; i ++ ) { list . insert_front ( i ); } for ( int i = 0 ; i < 100 ; i ++ ) { list . insert_back ( i ); } for ( int i = 0 ; i < 100 ; i ++ ) { list . insert_after ( list . head -> next , i ); } list . print (); return 0 ; } Related Problems \u00b6 NEED_TO_BE_ADDED Related Topics \u00b6 NOT_YET Analysis (Later..) \u00b6 You can add some mathematical things here using KaTex as a block tag $$ T(N) = O(N*M) $$ or as a inline tag $T(N) = O(N) $ Contributers \u00b6 08.15.2019 jchrys","title":"Linked List"},{"location":"DataStructures/Linear/LinkedList/#linkedlistdoubly_linked_list","text":"A linked list is a linear data structure, in which the elements are not stored at contiguous memory locations. The elements in a linked lists are linked using pointers.","title":"LinkedList(Doubly Linked List)"},{"location":"DataStructures/Linear/LinkedList/#operations_time_complexity","text":"Member Function Running Time insert_front() $\\Omicron(1)$ insert_back() $\\Omicron(1)$ insert_after() $\\Omicron(1)$ erase $\\Omicron(1)$ search() $\\Omicron(n)$","title":"Operations &amp; time complexity"},{"location":"DataStructures/Linear/LinkedList/#implementation","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 #include <bits/stdc++.h> using namespace std ; template < typename T > class LinkedList { struct Node { Node * before ; Node * next ; T data ; Node ( T data ) : before ( 0 ), next ( 0 ), data ( data ){} ~ Node () { delete before ; delete next ; delete data ; } }; Node * tail ; unsigned int _size ; public : Node * head ; LinkedList () : head ( 0 ), tail ( 0 ) {} void insert_front ( T val ) { Node * temp = new Node ( val ); if ( head == 0 ) { head = temp ; tail = temp ; } else { temp -> next = head ; head -> before = temp ; head = temp ; } } void insert_back ( T val ) { Node * temp = new Node ( val ); if ( tail == 0 ) { head = temp ; tail = temp ; } else { temp -> before = tail ; tail -> next = temp ; tail = temp ; } } void insert_after ( Node * node , T val ) { Node * temp = new Node ( val ); if ( temp -> next == 0 ) { tail = temp ; } temp -> next = node -> next ; temp -> next -> before = temp ; node -> next = temp ; temp -> before = node ; } Node * search ( T val ) { //search_from head Node * it = head ; while ( it != 0 && it -> data != val ) it = it -> next ; return it ; } void erase ( Node * node ) { if ( node == 0 ) return ; if ( node -> next == 0 ) { tail = node -> before ; tail -> next = 0 ; } else if ( node -> before == 0 ) { head = node -> next ; head -> before = 0 ; } else { node -> before -> next = node -> next ; node -> next = node -> before ; } delete node ; } void print () { Node * it = head ; while ( it != 0 ) { cout << it -> data << ' ' ; it = it -> next ; } } }; int main () { LinkedList < int > list ; for ( int i = 0 ; i < 100 ; i ++ ) { list . insert_front ( i ); } for ( int i = 0 ; i < 100 ; i ++ ) { list . insert_back ( i ); } for ( int i = 0 ; i < 100 ; i ++ ) { list . insert_after ( list . head -> next , i ); } list . print (); return 0 ; }","title":"Implementation"},{"location":"DataStructures/Linear/LinkedList/#related_problems","text":"NEED_TO_BE_ADDED","title":"Related Problems"},{"location":"DataStructures/Linear/LinkedList/#related_topics","text":"NOT_YET","title":"Related Topics"},{"location":"DataStructures/Linear/LinkedList/#analysis_later","text":"You can add some mathematical things here using KaTex as a block tag $$ T(N) = O(N*M) $$ or as a inline tag $T(N) = O(N) $","title":"Analysis (Later..)"},{"location":"DataStructures/Linear/LinkedList/#contributers","text":"08.15.2019 jchrys","title":"Contributers"},{"location":"DataStructures/Linear/Stack/","text":"Stack \u00b6 Element deleted from the set is the one most recently inserted; Stack implements last-in, first out or LIFO policy You can use array to implement Stack supported operations \u00b6 insert, delete, empty, top, size 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 template < typename T > class Stack { public : struct Node { T val ; Node * next ; Node () {}; Node ( T val ) : val ( val ), next ( 0 ){}; }; Node * head ; int _size ; Stack () { head = 0 ; _size = 0 ; } void push ( T val ) { Node * temp = new Node ( val ); if ( head == 0 ) { head = temp ; } else { temp -> next = head ; head = temp ; } _size ++ ; } void pop () { if ( empty ()) return ; Node * temp = head ; head = head -> next ; delete temp ; _size -- ; } bool empty () const { return _size == 0 ; } T top () const { return head -> val ; } int size () const { return _size ; } };","title":"Stack"},{"location":"DataStructures/Linear/Stack/#stack","text":"Element deleted from the set is the one most recently inserted; Stack implements last-in, first out or LIFO policy You can use array to implement Stack","title":"Stack"},{"location":"DataStructures/Linear/Stack/#supported_operations","text":"insert, delete, empty, top, size 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 template < typename T > class Stack { public : struct Node { T val ; Node * next ; Node () {}; Node ( T val ) : val ( val ), next ( 0 ){}; }; Node * head ; int _size ; Stack () { head = 0 ; _size = 0 ; } void push ( T val ) { Node * temp = new Node ( val ); if ( head == 0 ) { head = temp ; } else { temp -> next = head ; head = temp ; } _size ++ ; } void pop () { if ( empty ()) return ; Node * temp = head ; head = head -> next ; delete temp ; _size -- ; } bool empty () const { return _size == 0 ; } T top () const { return head -> val ; } int size () const { return _size ; } };","title":"supported operations"},{"location":"DataStructures/Linear/Vector/","text":"Vector \u00b6 Vector is Dynamic array structure in c++ Operations & time complexity \u00b6 Member Function Running Time push_back() $O(1) amortized$ pop_back() $O(1)$ empty() $O(1)$ reserve() $O(n)$ operator [] $O(1)$ Implementation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 using size_t = unsigned long ; template < typename T > class Vector { size_t _size ; size_t _capacity ; T * _buf ; public : // constructors // Vector(SomeType); //\"ordinary constructor\" Vector ( int k ) { _size = k ; _capacity = k ; _buf = new T [ _capacity ]; } // Vector(); //default constructor Vector () { _size = 0 ; _capacity = 0 ; _buf = new T [ _capacity ]; } // Vector(const &X); // copy constructor // Vector(&&X); //move constructor // &Vector operator=(const Vector&); //copy assignment: cleanup target and copy // &Vector operator=(Vector&&); // move assignment: cleanup target and move //~Vector(); //destructor: cleanup ~ Vector () { delete [] _buf ; } // capacity: size_t size () { return _size ; } void resize ( size_t n ) { _size = n ; } size_t capacity () { return _capacity ; }; bool empty () { return _size == 0 ; }; // unsigned int max_size(); void reserve ( size_t n ) { //Requests that the vector capacity be at least enough to contain n elements. if ( _size >= n ) return ; T * _temp = new T [ n ]; for ( size_t i = 0 ; i < _size ; i ++ ) { _temp [ i ] = _buf [ i ]; } _capacity = n ; delete [] _buf ; _buf = _temp ; } // shrink_to_fit() //element access: T back (); // operator[]() T & operator []( int idx ) { return _buf [ idx ]; } T operator []( int idx ) const { return _buf [ idx ]; } // at() // front() // data() //Modifiers void clear () { resize ( 0 ); }; void push_back ( T const & val ) { if ( _size == _capacity ) { if ( _capacity ) { reserve ( _capacity << 1 ); } else { reserve ( 1 ); } } _buf [ _size ++ ] = val ; }; void pop_back () { _size -- ; }; // assign() // insert() // erase() // emplace() // emplace_back //Iterators T * begin () { return & _buf [ 0 ]; } T * end () { return & _buf [ 0 ] + _size ; }; //T* rbegin(); //T* rend(); //T* const cbegin(); //T* const cend(); //T* const crbegin(); //T* const crend(); }; Related Problems \u00b6 Letters Shop Related Topics \u00b6 Stack Analysis (Later..) \u00b6 You can add some mathematical things here using KaTex as a block tag $$ T(N) = O(N*M) $$ or as a inline tag $T(N) = O(N) $ Contributers \u00b6 08.13.2019 jchrys","title":"Vector"},{"location":"DataStructures/Linear/Vector/#vector","text":"Vector is Dynamic array structure in c++","title":"Vector"},{"location":"DataStructures/Linear/Vector/#operations_time_complexity","text":"Member Function Running Time push_back() $O(1) amortized$ pop_back() $O(1)$ empty() $O(1)$ reserve() $O(n)$ operator [] $O(1)$","title":"Operations &amp; time complexity"},{"location":"DataStructures/Linear/Vector/#implementation","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 using size_t = unsigned long ; template < typename T > class Vector { size_t _size ; size_t _capacity ; T * _buf ; public : // constructors // Vector(SomeType); //\"ordinary constructor\" Vector ( int k ) { _size = k ; _capacity = k ; _buf = new T [ _capacity ]; } // Vector(); //default constructor Vector () { _size = 0 ; _capacity = 0 ; _buf = new T [ _capacity ]; } // Vector(const &X); // copy constructor // Vector(&&X); //move constructor // &Vector operator=(const Vector&); //copy assignment: cleanup target and copy // &Vector operator=(Vector&&); // move assignment: cleanup target and move //~Vector(); //destructor: cleanup ~ Vector () { delete [] _buf ; } // capacity: size_t size () { return _size ; } void resize ( size_t n ) { _size = n ; } size_t capacity () { return _capacity ; }; bool empty () { return _size == 0 ; }; // unsigned int max_size(); void reserve ( size_t n ) { //Requests that the vector capacity be at least enough to contain n elements. if ( _size >= n ) return ; T * _temp = new T [ n ]; for ( size_t i = 0 ; i < _size ; i ++ ) { _temp [ i ] = _buf [ i ]; } _capacity = n ; delete [] _buf ; _buf = _temp ; } // shrink_to_fit() //element access: T back (); // operator[]() T & operator []( int idx ) { return _buf [ idx ]; } T operator []( int idx ) const { return _buf [ idx ]; } // at() // front() // data() //Modifiers void clear () { resize ( 0 ); }; void push_back ( T const & val ) { if ( _size == _capacity ) { if ( _capacity ) { reserve ( _capacity << 1 ); } else { reserve ( 1 ); } } _buf [ _size ++ ] = val ; }; void pop_back () { _size -- ; }; // assign() // insert() // erase() // emplace() // emplace_back //Iterators T * begin () { return & _buf [ 0 ]; } T * end () { return & _buf [ 0 ] + _size ; }; //T* rbegin(); //T* rend(); //T* const cbegin(); //T* const cend(); //T* const crbegin(); //T* const crend(); };","title":"Implementation"},{"location":"DataStructures/Linear/Vector/#related_problems","text":"Letters Shop","title":"Related Problems"},{"location":"DataStructures/Linear/Vector/#related_topics","text":"Stack","title":"Related Topics"},{"location":"DataStructures/Linear/Vector/#analysis_later","text":"You can add some mathematical things here using KaTex as a block tag $$ T(N) = O(N*M) $$ or as a inline tag $T(N) = O(N) $","title":"Analysis (Later..)"},{"location":"DataStructures/Linear/Vector/#contributers","text":"08.13.2019 jchrys","title":"Contributers"},{"location":"DataStructures/Trees/BST/","text":"Binary Search Tree \u00b6 A Search tree is called Binary Search if it satisfies BST property and it's #children $\\leq$ 2. Binary Search Tree Property \u00b6 Let $x$ be a node in a binary search tree. If $y$ is a node in the left subtree of $x$, then $y.key \\leq x.key$. If $y$ is a node in the right subtree of $x$, then $y.key \\leq x.key$. Operations & timeComplexity \u00b6 $h = height(tree)$ Member Function Running Time insert() $\\Omicron(h)$ erase() $\\Omicron(h)$ inorder_tree_walk $\\Theta(n)$ find() $\\Omicron(h)$ minimum() $\\Omicron(h)$ maximum() $\\Omicron(h)$ successor() $\\Omicron(h)$ predecessor() $\\Omicron(h)$ Warning it is not guaranteed that $h = \\Omicron(log(n))$ this binary search tree is not balanced Implementation c++ \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 #include <iostream> using namespace std ; template < typename T > class Set { struct Node { T key ; Node * parent ; Node * left ; Node * right ; Node ( T key ) { this -> key = key ; this -> parent = 0 ; this -> left = 0 ; this -> right = 0 ; } }; Node * root ; unsigned int _size ; public : Set () : root ( 0 ), _size ( 0 ) { //default constructor } void insert ( T key ) { _insert ( new Node ( key )); //insert a key in to set (helper function) } void _insert ( Node * && node ) { // Node * y = 0 ; Node * x = this -> root ; while ( x != 0 ) { y = x ; if ( node -> key < x -> key ) { x = x -> left ; } else if ( node -> key == x -> key ) { return ; } else { x = x -> right ; } } node -> parent = y ; if ( y == 0 ) // when tree is empty -> you could check with _size; this -> root = node ; else if ( node -> key < y -> key ) { y -> left = node ; node -> parent = y ; } else { y -> right = node ; node -> parent = y ; } this -> _size ++ ; } Node * find ( T key ) { Node * x = this -> root ; while ( x != 0 && x -> key != key ) { if ( x -> key > key ) { x = x -> left ; } else { x = x -> right ; } } return x ; } Node * minimum () { _minimum ( this -> root ); } Node * _minimum ( Node * x ) { while ( x -> left != 0 ) { x = x -> left ; } return x ; } Node * maximum () { //returns Node* that with maximum key return _maximum ( this -> root ); } Node * _maximum ( Node * & x ) { while ( x -> right != 0 ) { x = x -> right ; } return x ; } Node * successor ( Node * x ) { if ( x -> right != 0 ) { return _minimum ( x -> right ); } Node * y = x -> parent ; while ( y != 0 && x == y -> right ) { x = y ; y = y -> parent ; } return y ; } Node * predecessor ( Node * x ) { if ( x -> left != 0 ) { return _maximum ( x -> left ); } Node * y = x -> parent ; while ( y != 0 && x == y -> left ) { x = y ; y = y -> parent ; } return y ; } unsigned int size () { return _size ; } void _inorder_tree_travel ( Node * const & node ) { if ( node == 0 ) return ; _inorder_tree_travel ( node -> left ); cout << node -> key << ' ' ; _inorder_tree_travel ( node -> right ); } void inorder_tree_travel () { _inorder_tree_travel ( this -> root ); } void transplant ( Node * u , Node * v ) { if ( u -> parent == 0 ) { this -> root = v ; } else if ( u == u -> parent -> left ) { u -> parent -> left = v ; } else { u -> parent -> right = v ; } if ( v != 0 ) { v -> parent = u -> parent ; } } void erase ( T key ) { _erase ( find ( key )); } void _erase ( Node * target ) { if ( target == 0 ) return ; if ( target -> left == 0 ) transplant ( target , target -> right ); else if ( target -> right == 0 ) transplant ( target , target -> left ); else { Node * y = _minimum ( target -> right ); if ( y -> parent != target ) { transplant ( y , y -> right ); y -> right = target -> right ; y -> right -> parent = y ; } transplant ( target , y ); y -> left = target -> left ; y -> left -> parent = y ; } delete target ; _size -- ; } unsigned int height ( Node * node ) { if ( node == 0 ) return 0 ; unsigned int lDepth = height ( node -> left ); unsigned int rDepth = height ( node -> right ); if ( lDepth > rDepth ) return lDepth + 1 ; return rDepth + 1 ; } unsigned int tree_height () { return height ( this -> root ); } }; int main () { Set < int > s ; // if input's are random; cout << \"Naive Binary Search Tree implementation\" << endl ; cout << \"-------BEST-CASE(random inputs)--------\" << endl ; cout << \"input: 10,000 random integers\" << endl ; for ( int i = 0 ; i < 10000 ; i ++ ) { s . insert ( rand () % 1000000 ); } cout << \"-----------------results----------------\" << endl ; cout << \"tree_height: \" << s . tree_height () << endl ; cout << endl << endl << endl ; cout << \"------WORST-CASE(sorted_inputs)---------\" << endl ; cout << \"input: [1, 2, 3, ..., 10000]\" << endl ; Set < int > worst ; for ( int i = 1 ; i <= 10000 ; i ++ ) { worst . insert ( i ); } cout << \"-----------------results----------------\" << endl ; cout << \"tree_height: \" << worst . tree_height () << endl ; return 0 ; } Related Problems \u00b6 Related Topics \u00b6 Analysis (Later..) \u00b6 You can add some mathematical things here using KaTex as a block tag $$ T(N) = O(N*M) $$ or as a inline tag $T(N) = O(N) $ Contributers \u00b6 08.15.2019 jchrys","title":"BST"},{"location":"DataStructures/Trees/BST/#binary_search_tree","text":"A Search tree is called Binary Search if it satisfies BST property and it's #children $\\leq$ 2.","title":"Binary Search Tree"},{"location":"DataStructures/Trees/BST/#binary_search_tree_property","text":"Let $x$ be a node in a binary search tree. If $y$ is a node in the left subtree of $x$, then $y.key \\leq x.key$. If $y$ is a node in the right subtree of $x$, then $y.key \\leq x.key$.","title":"Binary Search Tree Property"},{"location":"DataStructures/Trees/BST/#operations_timecomplexity","text":"$h = height(tree)$ Member Function Running Time insert() $\\Omicron(h)$ erase() $\\Omicron(h)$ inorder_tree_walk $\\Theta(n)$ find() $\\Omicron(h)$ minimum() $\\Omicron(h)$ maximum() $\\Omicron(h)$ successor() $\\Omicron(h)$ predecessor() $\\Omicron(h)$ Warning it is not guaranteed that $h = \\Omicron(log(n))$ this binary search tree is not balanced","title":"Operations &amp; timeComplexity"},{"location":"DataStructures/Trees/BST/#implementation_c","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 #include <iostream> using namespace std ; template < typename T > class Set { struct Node { T key ; Node * parent ; Node * left ; Node * right ; Node ( T key ) { this -> key = key ; this -> parent = 0 ; this -> left = 0 ; this -> right = 0 ; } }; Node * root ; unsigned int _size ; public : Set () : root ( 0 ), _size ( 0 ) { //default constructor } void insert ( T key ) { _insert ( new Node ( key )); //insert a key in to set (helper function) } void _insert ( Node * && node ) { // Node * y = 0 ; Node * x = this -> root ; while ( x != 0 ) { y = x ; if ( node -> key < x -> key ) { x = x -> left ; } else if ( node -> key == x -> key ) { return ; } else { x = x -> right ; } } node -> parent = y ; if ( y == 0 ) // when tree is empty -> you could check with _size; this -> root = node ; else if ( node -> key < y -> key ) { y -> left = node ; node -> parent = y ; } else { y -> right = node ; node -> parent = y ; } this -> _size ++ ; } Node * find ( T key ) { Node * x = this -> root ; while ( x != 0 && x -> key != key ) { if ( x -> key > key ) { x = x -> left ; } else { x = x -> right ; } } return x ; } Node * minimum () { _minimum ( this -> root ); } Node * _minimum ( Node * x ) { while ( x -> left != 0 ) { x = x -> left ; } return x ; } Node * maximum () { //returns Node* that with maximum key return _maximum ( this -> root ); } Node * _maximum ( Node * & x ) { while ( x -> right != 0 ) { x = x -> right ; } return x ; } Node * successor ( Node * x ) { if ( x -> right != 0 ) { return _minimum ( x -> right ); } Node * y = x -> parent ; while ( y != 0 && x == y -> right ) { x = y ; y = y -> parent ; } return y ; } Node * predecessor ( Node * x ) { if ( x -> left != 0 ) { return _maximum ( x -> left ); } Node * y = x -> parent ; while ( y != 0 && x == y -> left ) { x = y ; y = y -> parent ; } return y ; } unsigned int size () { return _size ; } void _inorder_tree_travel ( Node * const & node ) { if ( node == 0 ) return ; _inorder_tree_travel ( node -> left ); cout << node -> key << ' ' ; _inorder_tree_travel ( node -> right ); } void inorder_tree_travel () { _inorder_tree_travel ( this -> root ); } void transplant ( Node * u , Node * v ) { if ( u -> parent == 0 ) { this -> root = v ; } else if ( u == u -> parent -> left ) { u -> parent -> left = v ; } else { u -> parent -> right = v ; } if ( v != 0 ) { v -> parent = u -> parent ; } } void erase ( T key ) { _erase ( find ( key )); } void _erase ( Node * target ) { if ( target == 0 ) return ; if ( target -> left == 0 ) transplant ( target , target -> right ); else if ( target -> right == 0 ) transplant ( target , target -> left ); else { Node * y = _minimum ( target -> right ); if ( y -> parent != target ) { transplant ( y , y -> right ); y -> right = target -> right ; y -> right -> parent = y ; } transplant ( target , y ); y -> left = target -> left ; y -> left -> parent = y ; } delete target ; _size -- ; } unsigned int height ( Node * node ) { if ( node == 0 ) return 0 ; unsigned int lDepth = height ( node -> left ); unsigned int rDepth = height ( node -> right ); if ( lDepth > rDepth ) return lDepth + 1 ; return rDepth + 1 ; } unsigned int tree_height () { return height ( this -> root ); } }; int main () { Set < int > s ; // if input's are random; cout << \"Naive Binary Search Tree implementation\" << endl ; cout << \"-------BEST-CASE(random inputs)--------\" << endl ; cout << \"input: 10,000 random integers\" << endl ; for ( int i = 0 ; i < 10000 ; i ++ ) { s . insert ( rand () % 1000000 ); } cout << \"-----------------results----------------\" << endl ; cout << \"tree_height: \" << s . tree_height () << endl ; cout << endl << endl << endl ; cout << \"------WORST-CASE(sorted_inputs)---------\" << endl ; cout << \"input: [1, 2, 3, ..., 10000]\" << endl ; Set < int > worst ; for ( int i = 1 ; i <= 10000 ; i ++ ) { worst . insert ( i ); } cout << \"-----------------results----------------\" << endl ; cout << \"tree_height: \" << worst . tree_height () << endl ; return 0 ; }","title":"Implementation c++"},{"location":"DataStructures/Trees/BST/#related_problems","text":"","title":"Related Problems"},{"location":"DataStructures/Trees/BST/#related_topics","text":"","title":"Related Topics"},{"location":"DataStructures/Trees/BST/#analysis_later","text":"You can add some mathematical things here using KaTex as a block tag $$ T(N) = O(N*M) $$ or as a inline tag $T(N) = O(N) $","title":"Analysis (Later..)"},{"location":"DataStructures/Trees/BST/#contributers","text":"08.15.2019 jchrys","title":"Contributers"},{"location":"DataStructures/Trees/RedBlackTree/","text":"Red Black Tree \u00b6 Red Black Tree is balanced binary search tree with one extra bit of storage per node: color Red Black Tree satisfies the Red-Black-Properties Red-Black-Properties Every node is black or red The root is black Every leaf(NIL) is black if a node is red, then both its children are black For each node, all simple paths from the node to descendant leaves contains the same number of black nodes. Operations & time complexity \u00b6 $N$ = number of elements in Tree Member Function Running Time insert() $\\Omicron(\\lg(N))$ erase() $\\Omicron(\\lg(N))$ inorder_tree_walk $\\Theta(N)$ find() $\\Omicron(\\lg(N))$ minimum() $\\Omicron(\\lg(N))$ maximum() $\\Omicron(\\lg(N))$ successor() $\\Omicron(\\lg(N))$ predecessor() $\\Omicron(\\lg(N))$ Note Red Black Tree is Balanced Binary Search Tree It is guaranteed that height of the tree is $\\Omicron(\\lg(N))$ in worst case Implementation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 #include <bits/stdc++.h> using namespace std ; template < typename K , typename V > class Map { struct Node { K key ; V val ; bool color ; // 0: red, 1:black Node * p ; Node * left ; Node * right ; Node ( Map & out ) : key (), val (), color ( 1 ), p ( out . NIL ), left ( out . NIL ), right ( out . NIL ){} Node ( Map & out , K key , V val ) : key ( key ), val ( val ), color ( 0 ), p ( out . NIL ), left ( out . NIL ), right ( out . NIL ){} }; Node * root ; void left_rotate ( Node * x ) { Node * y = x -> right ; // y is x's right child x -> right = y -> left ; // set y's left child to x's right child; x -> right -> p = x ; // set parent y -> p = x -> p ; //set x's parent pointing to y; if ( x -> p == NIL ) { // if x is root this -> root = y ; } else if ( x == x -> p -> left ) { //if x is left child x -> p -> left = y ; } else { x -> p -> right = y ; } x -> p = y ; // y is x's parent y -> left = x ; // x is y's left child } void right_rotate ( Node * x ) { Node * y = x -> left ; x -> left = y -> right ; x -> left -> p = x ; y -> p = x -> p ; if ( x -> p == NIL ) { this -> root = y ; } else if ( x == x -> p -> left ) { x -> p -> left = y ; } else { x -> p -> right = y ; } x -> p = y ; y -> right = x ; } void transplant ( Node * u , Node * v ) { // gives u's parent relations to v if ( u -> p == NIL ) { root = v ; } else if ( u == u -> p -> right ) { u -> p -> right = v ; } else { u -> p -> left = v ; } v -> p = u -> p ; // unconditionally because NIL can have parent also; } public : Node * NIL ; Map () { NIL = new Node ( * this ); root = NIL ; } void insert_fixup ( Node * & z ) { while ( z -> p -> color == 0 ) { Node * y ; // z's uncle if ( z -> p == z -> p -> p -> left ) { // when z's parent is left child y = z -> p -> p -> right ; if ( y -> color == 0 ) { // if uncle is red, uncles parent should be black z -> p -> color = 1 ; // recoloring and goes up y -> color = 1 ; z -> p -> p -> color = 0 ; z = z -> p -> p ; } else { if ( z == z -> p -> right ) { // if uncle is black and z is right child z = z -> p ; left_rotate ( z ); } // if uncle is black and z is right child z -> p -> color = 1 ; z -> p -> p -> color = 0 ; right_rotate ( z -> p -> p ); } } else { // when z's parent is right child y = z -> p -> p -> left ; if ( y -> color == 0 ) { z -> p -> color = 1 ; y -> color = 1 ; z -> p -> p -> color = 0 ; z = z -> p -> p ; } else { // if uncle's color is black if ( z == z -> p -> left ) { //when z is right child z = z -> p ; right_rotate ( z ); } z -> p -> color = 1 ; z -> p -> p -> color = 0 ; left_rotate ( z -> p -> p ); } } } this -> root -> color = 1 ; } void insert ( K key , V val ) { Node * z = new Node ( * this , key , val ); Node * y = NIL ; Node * x = this -> root ; while ( x != NIL ) { y = x ; if ( z -> key < x -> key ) { x = x -> left ; } else { x = x -> right ; } } z -> p = y ; if ( y == NIL ) { this -> root = z ; } else if ( z -> key < y -> key ) { y -> left = z ; } else { y -> right = z ; } // z->left = NIL; // z->right = ZIL; // z->color = 0; insert_fixup ( z ); } Node * find ( K key ) { Node * x = root ; while ( x != NIL && x -> key != key ) { if ( key < x -> key ) { x = x -> left ; } else { x = x -> right ; } } return x ; } Node * minimum ( Node * x ) { while ( x -> left != NIL ) { x = x -> left ; } return x ; } void erase_fixup ( Node * x ) { Node * w ; // sibling of x; while ( x != this -> root && x -> color == 1 ) { //only if x is black and not root if ( x == x -> p -> left ) { w = x -> p -> right ; if ( w -> color == 0 ) { // turns to case2, 3 or 4; w -> color = 1 ; x -> p -> color = 0 ; left_rotate ( x -> p ); w = x -> p -> right ; } if ( w -> left -> color == 1 && w -> right -> color == 1 ) { // case2 w -> color = 0 ; x = x -> p ; } else { if ( w -> right -> color == 1 ) { //case3 -> turns to case4 w -> left -> color = 1 ; w -> color = 0 ; right_rotate ( w ); w = x -> p -> right ; } w -> color = x -> p -> color ; // case4 -> we can make legit red-black tree x -> p -> color = 1 ; w -> right -> color = 1 ; left_rotate ( x -> p ); x = root ; } } else { // x == x->p->right w = x -> p -> left ; if ( w -> color == 0 ) { w -> color = 1 ; x -> p -> color = 0 ; right_rotate ( x -> p ); w = x -> p -> left ; } if ( w -> left -> color == 1 && w -> right -> color == 1 ) { w -> color = 0 ; x = x -> p ; } else { if ( w -> left -> color == 1 ) { w -> color = 0 ; w -> right -> color = 1 ; left_rotate ( w ); w = x -> p -> left ; } w -> color = w -> p -> color ; w -> p -> color = 1 ; w -> left -> color = 1 ; right_rotate ( x -> p ); x = root ; } } } x -> color = 1 ; } void erase ( Node * z ) { Node * y = z ; bool y_original_color = y -> color ; Node * x ; if ( z -> left == NIL ) { x = z -> right ; transplant ( z , z -> right ); } else if ( z -> right == NIL ) { x = z -> left ; transplant ( z , z -> left ); } else { y = minimum ( z -> right ); y_original_color = y -> color ; x = y -> right ; if ( y -> p == z ) { x -> p = y ; // incase of x is NIL!! we need to find it's parent! } else { transplant ( y , y -> right ); y -> right = z -> right ; y -> right -> p = y ; } transplant ( z , y ); y -> left = z -> left ; y -> left -> p = y ; y -> color = z -> color ; } if ( y_original_color == 1 ) { erase_fixup ( x ); } } void rb_printer ( Node * node , int indent ) { //prints red & black tree int count = 4 ; if ( node == NIL ) return ; indent += count ; rb_printer ( node -> right , indent ); cout << endl ; for ( int i = count ; i < indent ; i ++ ) { cout << \" \" ; } cout << ( node -> color == 0 ? \" \\033 [1;31m\" : \"\" ) << ( node == node -> p -> left ? \"l\" : \"r\" ) << node -> key << ( node -> color == 0 ? \" \\033 [0m\" : \"\" ) << endl ; rb_printer ( node -> left , indent ); } void print () { rb_printer ( this -> root , 0 ); } }; int main () { Map < int , int > m ; for ( int i = 0 ; i < 20 ; i ++ ) { m . insert ( rand () % 20 , 1 ); } m . print (); cout << \"deleting ---\" << endl ;; for ( int i = 0 ; i < 20 ; i ++ ) { int key = rand () % 20 ; auto it = m . find ( key ); cout << \"delete: \" << key << endl ; if ( it != m . NIL ) { cout << \"key exist... deleting...\" ; m . erase ( it ); m . print (); } else { cout << \"key not exist\" << endl ; } } return 0 ; } Related Problems \u00b6 NOT ADDED YET Related Topics \u00b6 BinarySearchTree Analysis (Optional) \u00b6 You can add some mathematical things here using KaTex as a block tag $$ T(N) = O(N*M) $$ or as a inline tag $T(N) = O(N) $ Contributers (Optional) \u00b6 08.18.2019 JCHRYS","title":"RedBlackTree"},{"location":"DataStructures/Trees/RedBlackTree/#red_black_tree","text":"Red Black Tree is balanced binary search tree with one extra bit of storage per node: color Red Black Tree satisfies the Red-Black-Properties Red-Black-Properties Every node is black or red The root is black Every leaf(NIL) is black if a node is red, then both its children are black For each node, all simple paths from the node to descendant leaves contains the same number of black nodes.","title":"Red Black Tree"},{"location":"DataStructures/Trees/RedBlackTree/#operations_time_complexity","text":"$N$ = number of elements in Tree Member Function Running Time insert() $\\Omicron(\\lg(N))$ erase() $\\Omicron(\\lg(N))$ inorder_tree_walk $\\Theta(N)$ find() $\\Omicron(\\lg(N))$ minimum() $\\Omicron(\\lg(N))$ maximum() $\\Omicron(\\lg(N))$ successor() $\\Omicron(\\lg(N))$ predecessor() $\\Omicron(\\lg(N))$ Note Red Black Tree is Balanced Binary Search Tree It is guaranteed that height of the tree is $\\Omicron(\\lg(N))$ in worst case","title":"Operations &amp; time complexity"},{"location":"DataStructures/Trees/RedBlackTree/#implementation","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 #include <bits/stdc++.h> using namespace std ; template < typename K , typename V > class Map { struct Node { K key ; V val ; bool color ; // 0: red, 1:black Node * p ; Node * left ; Node * right ; Node ( Map & out ) : key (), val (), color ( 1 ), p ( out . NIL ), left ( out . NIL ), right ( out . NIL ){} Node ( Map & out , K key , V val ) : key ( key ), val ( val ), color ( 0 ), p ( out . NIL ), left ( out . NIL ), right ( out . NIL ){} }; Node * root ; void left_rotate ( Node * x ) { Node * y = x -> right ; // y is x's right child x -> right = y -> left ; // set y's left child to x's right child; x -> right -> p = x ; // set parent y -> p = x -> p ; //set x's parent pointing to y; if ( x -> p == NIL ) { // if x is root this -> root = y ; } else if ( x == x -> p -> left ) { //if x is left child x -> p -> left = y ; } else { x -> p -> right = y ; } x -> p = y ; // y is x's parent y -> left = x ; // x is y's left child } void right_rotate ( Node * x ) { Node * y = x -> left ; x -> left = y -> right ; x -> left -> p = x ; y -> p = x -> p ; if ( x -> p == NIL ) { this -> root = y ; } else if ( x == x -> p -> left ) { x -> p -> left = y ; } else { x -> p -> right = y ; } x -> p = y ; y -> right = x ; } void transplant ( Node * u , Node * v ) { // gives u's parent relations to v if ( u -> p == NIL ) { root = v ; } else if ( u == u -> p -> right ) { u -> p -> right = v ; } else { u -> p -> left = v ; } v -> p = u -> p ; // unconditionally because NIL can have parent also; } public : Node * NIL ; Map () { NIL = new Node ( * this ); root = NIL ; } void insert_fixup ( Node * & z ) { while ( z -> p -> color == 0 ) { Node * y ; // z's uncle if ( z -> p == z -> p -> p -> left ) { // when z's parent is left child y = z -> p -> p -> right ; if ( y -> color == 0 ) { // if uncle is red, uncles parent should be black z -> p -> color = 1 ; // recoloring and goes up y -> color = 1 ; z -> p -> p -> color = 0 ; z = z -> p -> p ; } else { if ( z == z -> p -> right ) { // if uncle is black and z is right child z = z -> p ; left_rotate ( z ); } // if uncle is black and z is right child z -> p -> color = 1 ; z -> p -> p -> color = 0 ; right_rotate ( z -> p -> p ); } } else { // when z's parent is right child y = z -> p -> p -> left ; if ( y -> color == 0 ) { z -> p -> color = 1 ; y -> color = 1 ; z -> p -> p -> color = 0 ; z = z -> p -> p ; } else { // if uncle's color is black if ( z == z -> p -> left ) { //when z is right child z = z -> p ; right_rotate ( z ); } z -> p -> color = 1 ; z -> p -> p -> color = 0 ; left_rotate ( z -> p -> p ); } } } this -> root -> color = 1 ; } void insert ( K key , V val ) { Node * z = new Node ( * this , key , val ); Node * y = NIL ; Node * x = this -> root ; while ( x != NIL ) { y = x ; if ( z -> key < x -> key ) { x = x -> left ; } else { x = x -> right ; } } z -> p = y ; if ( y == NIL ) { this -> root = z ; } else if ( z -> key < y -> key ) { y -> left = z ; } else { y -> right = z ; } // z->left = NIL; // z->right = ZIL; // z->color = 0; insert_fixup ( z ); } Node * find ( K key ) { Node * x = root ; while ( x != NIL && x -> key != key ) { if ( key < x -> key ) { x = x -> left ; } else { x = x -> right ; } } return x ; } Node * minimum ( Node * x ) { while ( x -> left != NIL ) { x = x -> left ; } return x ; } void erase_fixup ( Node * x ) { Node * w ; // sibling of x; while ( x != this -> root && x -> color == 1 ) { //only if x is black and not root if ( x == x -> p -> left ) { w = x -> p -> right ; if ( w -> color == 0 ) { // turns to case2, 3 or 4; w -> color = 1 ; x -> p -> color = 0 ; left_rotate ( x -> p ); w = x -> p -> right ; } if ( w -> left -> color == 1 && w -> right -> color == 1 ) { // case2 w -> color = 0 ; x = x -> p ; } else { if ( w -> right -> color == 1 ) { //case3 -> turns to case4 w -> left -> color = 1 ; w -> color = 0 ; right_rotate ( w ); w = x -> p -> right ; } w -> color = x -> p -> color ; // case4 -> we can make legit red-black tree x -> p -> color = 1 ; w -> right -> color = 1 ; left_rotate ( x -> p ); x = root ; } } else { // x == x->p->right w = x -> p -> left ; if ( w -> color == 0 ) { w -> color = 1 ; x -> p -> color = 0 ; right_rotate ( x -> p ); w = x -> p -> left ; } if ( w -> left -> color == 1 && w -> right -> color == 1 ) { w -> color = 0 ; x = x -> p ; } else { if ( w -> left -> color == 1 ) { w -> color = 0 ; w -> right -> color = 1 ; left_rotate ( w ); w = x -> p -> left ; } w -> color = w -> p -> color ; w -> p -> color = 1 ; w -> left -> color = 1 ; right_rotate ( x -> p ); x = root ; } } } x -> color = 1 ; } void erase ( Node * z ) { Node * y = z ; bool y_original_color = y -> color ; Node * x ; if ( z -> left == NIL ) { x = z -> right ; transplant ( z , z -> right ); } else if ( z -> right == NIL ) { x = z -> left ; transplant ( z , z -> left ); } else { y = minimum ( z -> right ); y_original_color = y -> color ; x = y -> right ; if ( y -> p == z ) { x -> p = y ; // incase of x is NIL!! we need to find it's parent! } else { transplant ( y , y -> right ); y -> right = z -> right ; y -> right -> p = y ; } transplant ( z , y ); y -> left = z -> left ; y -> left -> p = y ; y -> color = z -> color ; } if ( y_original_color == 1 ) { erase_fixup ( x ); } } void rb_printer ( Node * node , int indent ) { //prints red & black tree int count = 4 ; if ( node == NIL ) return ; indent += count ; rb_printer ( node -> right , indent ); cout << endl ; for ( int i = count ; i < indent ; i ++ ) { cout << \" \" ; } cout << ( node -> color == 0 ? \" \\033 [1;31m\" : \"\" ) << ( node == node -> p -> left ? \"l\" : \"r\" ) << node -> key << ( node -> color == 0 ? \" \\033 [0m\" : \"\" ) << endl ; rb_printer ( node -> left , indent ); } void print () { rb_printer ( this -> root , 0 ); } }; int main () { Map < int , int > m ; for ( int i = 0 ; i < 20 ; i ++ ) { m . insert ( rand () % 20 , 1 ); } m . print (); cout << \"deleting ---\" << endl ;; for ( int i = 0 ; i < 20 ; i ++ ) { int key = rand () % 20 ; auto it = m . find ( key ); cout << \"delete: \" << key << endl ; if ( it != m . NIL ) { cout << \"key exist... deleting...\" ; m . erase ( it ); m . print (); } else { cout << \"key not exist\" << endl ; } } return 0 ; }","title":"Implementation"},{"location":"DataStructures/Trees/RedBlackTree/#related_problems","text":"NOT ADDED YET","title":"Related Problems"},{"location":"DataStructures/Trees/RedBlackTree/#related_topics","text":"BinarySearchTree","title":"Related Topics"},{"location":"DataStructures/Trees/RedBlackTree/#analysis_optional","text":"You can add some mathematical things here using KaTex as a block tag $$ T(N) = O(N*M) $$ or as a inline tag $T(N) = O(N) $","title":"Analysis (Optional)"},{"location":"DataStructures/Trees/RedBlackTree/#contributers_optional","text":"08.18.2019 JCHRYS","title":"Contributers (Optional)"},{"location":"DataStructures/Trees/SearchTree/","text":"Search Tree \u00b6 The Search tree data structure supports many dynamic-set operations, including $SEARCH,\\ MINIMUM,\\ MAXIMUM,\\ PREDECESSOR,$ $ SUCCESSOR,\\ INSERT,\\ DELETE$ so we can use a search tree as a $dictionary$ and as a $priority\\ queue$ DataStructure \u00b6 We can represent it as linked objects Each Node Containing \u00b6 Each Node Contatining pointers : rightChild , leftChild , parent","title":"SearchTree"},{"location":"DataStructures/Trees/SearchTree/#search_tree","text":"The Search tree data structure supports many dynamic-set operations, including $SEARCH,\\ MINIMUM,\\ MAXIMUM,\\ PREDECESSOR,$ $ SUCCESSOR,\\ INSERT,\\ DELETE$ so we can use a search tree as a $dictionary$ and as a $priority\\ queue$","title":"Search Tree"},{"location":"DataStructures/Trees/SearchTree/#datastructure","text":"We can represent it as linked objects","title":"DataStructure"},{"location":"DataStructures/Trees/SearchTree/#each_node_containing","text":"Each Node Contatining pointers : rightChild , leftChild , parent","title":"Each Node Containing"},{"location":"Language/Class/","text":"Class \u00b6 C++ classes are a tool for creating new types that can be used conveniently as builtin types The Fundamental idea in defining a new type is to separate the details of the implementation from the properties essential to the correc use of it Brief Summary of classes \u00b6 A class is user-defined type A class consists of a set of members. The most common kinds of members are data members and member functions. Member functions can define the meaning of initialization, copy, move, and cleanup Members are accessed using . (dot) for objects and -> (arrow) for pointers. Operators, such as, + , ! , and [] , can be defined for a class A class is a namespace containing its members The public members provide the class's interface and the private members provide implementation details A struct is a class where members are by default public Class Basics \u00b6 class example \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class X { private : // the representation (implementation) is private int m ; public : // the user interface is public X ( int i = 0 ) : m { i } {} //a constructor (initialize the data member m) int mf ( int i ) { // a member function int old = m ; m = i ; //set a new value return old ; // return the old value } }; X var { 7 }; // a variable of type X, initialized to 7 int user ( X var , X * ptr ) { int x = var . mf ( 7 ); // access using . int y = ptr -> mf ( 9 ); // access using -> int z = var . m ; // error: cannot acces private member } 1. Member functions \u00b6 Functions declared within a class definition are called member functions 2. Default copying \u00b6 a class object can be initialized with a copy of an obejct of its class 1 2 UserClass c1 = c0 ; // initialization by copy UserClass c2 { d1 }; // initialization by copy 3. Access Control \u00b6 class is consist of two parts private part: can be used only by member functions , public part : interface to objects of class 4. class and struct \u00b6 a struct is a class in which members are by default public struct S{}; is simply short hand for class S{public: }; 5. Constructors \u00b6 a constructor is recognized by having the same name as the class it self. programmers can declare a function with the explicit purpose of initializing objects. 1 2 3 4 5 6 7 8 9 10 11 12 class Date { int d , m , y ; public : Date ( int dd , int mm , int yy ); // constructor } Date today = Date ( 23 , 6 , 1983 ); // OK Date xmas ( 25 , 12 , 1990 ); // OK -> abbreviated form Date my_birthday ; //error: initializer missing Date release1_0 ( 10 , 12 ) //error: third argument missing Date today = Date { 23 , 6 , 1982 } // good! I recommend the {} notation over the () notation for initializing, because it is explicit about what is being done we could use default values directly as default arguments 1 2 3 4 5 class Date { int d , m , y ; public : Date ( int dd = today . d , int mm = today . m , int yy = today . y ); // constructor }","title":"Class"},{"location":"Language/Class/#class","text":"C++ classes are a tool for creating new types that can be used conveniently as builtin types The Fundamental idea in defining a new type is to separate the details of the implementation from the properties essential to the correc use of it","title":"Class"},{"location":"Language/Class/#brief_summary_of_classes","text":"A class is user-defined type A class consists of a set of members. The most common kinds of members are data members and member functions. Member functions can define the meaning of initialization, copy, move, and cleanup Members are accessed using . (dot) for objects and -> (arrow) for pointers. Operators, such as, + , ! , and [] , can be defined for a class A class is a namespace containing its members The public members provide the class's interface and the private members provide implementation details A struct is a class where members are by default public","title":"Brief Summary of classes"},{"location":"Language/Class/#class_basics","text":"","title":"Class Basics"},{"location":"Language/Class/#class_example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class X { private : // the representation (implementation) is private int m ; public : // the user interface is public X ( int i = 0 ) : m { i } {} //a constructor (initialize the data member m) int mf ( int i ) { // a member function int old = m ; m = i ; //set a new value return old ; // return the old value } }; X var { 7 }; // a variable of type X, initialized to 7 int user ( X var , X * ptr ) { int x = var . mf ( 7 ); // access using . int y = ptr -> mf ( 9 ); // access using -> int z = var . m ; // error: cannot acces private member }","title":"class example"},{"location":"Language/Class/#1_member_functions","text":"Functions declared within a class definition are called member functions","title":"1. Member functions"},{"location":"Language/Class/#2_default_copying","text":"a class object can be initialized with a copy of an obejct of its class 1 2 UserClass c1 = c0 ; // initialization by copy UserClass c2 { d1 }; // initialization by copy","title":"2. Default copying"},{"location":"Language/Class/#3_access_control","text":"class is consist of two parts private part: can be used only by member functions , public part : interface to objects of class","title":"3. Access Control"},{"location":"Language/Class/#4_class_and_struct","text":"a struct is a class in which members are by default public struct S{}; is simply short hand for class S{public: };","title":"4. class and struct"},{"location":"Language/Class/#5_constructors","text":"a constructor is recognized by having the same name as the class it self. programmers can declare a function with the explicit purpose of initializing objects. 1 2 3 4 5 6 7 8 9 10 11 12 class Date { int d , m , y ; public : Date ( int dd , int mm , int yy ); // constructor } Date today = Date ( 23 , 6 , 1983 ); // OK Date xmas ( 25 , 12 , 1990 ); // OK -> abbreviated form Date my_birthday ; //error: initializer missing Date release1_0 ( 10 , 12 ) //error: third argument missing Date today = Date { 23 , 6 , 1982 } // good! I recommend the {} notation over the () notation for initializing, because it is explicit about what is being done we could use default values directly as default arguments 1 2 3 4 5 class Date { int d , m , y ; public : Date ( int dd = today . d , int mm = today . m , int yy = today . y ); // constructor }","title":"5. Constructors"},{"location":"Language/Keywords/","text":"Aliases \u00b6 Aliases are used when we want to insulate our code from details of the underlying machine. - note that naming a type after its representation than its purpose is not neccessarily a good idea. 1. typedef \u00b6 1 2 cpp typedef double decimal_places ; // is equivalent to \"using decimal_places = double;\" 2. using \u00b6 the using keyword can also be used to introduce a template alias. 1 2 template < typename T > using Vector = std :: vector < T , my_allocator < T >> but we cannot apply type specifiers, such as unsigned, to an alias. 1 2 3 using Char = char ; using Uchar = unsigned Char ; //error using Uchar = unsigned char ; // ok","title":"Keywords"},{"location":"Language/Keywords/#aliases","text":"Aliases are used when we want to insulate our code from details of the underlying machine. - note that naming a type after its representation than its purpose is not neccessarily a good idea.","title":"Aliases"},{"location":"Language/Keywords/#1_typedef","text":"1 2 cpp typedef double decimal_places ; // is equivalent to \"using decimal_places = double;\"","title":"1. typedef"},{"location":"Language/Keywords/#2_using","text":"the using keyword can also be used to introduce a template alias. 1 2 template < typename T > using Vector = std :: vector < T , my_allocator < T >> but we cannot apply type specifiers, such as unsigned, to an alias. 1 2 3 using Char = char ; using Uchar = unsigned Char ; //error using Uchar = unsigned char ; // ok","title":"2. using"},{"location":"Language/Preface/","text":"Why C++? \u00b6 What You Should Know Before.. \u00b6 you should be able to write C++ programs using components such as IOstreams and containers from C++ STL. You Should be also be familiar with the basic features of \"Modern C++\", such as auto, decltype, move semantics, and lambdas. c++17 modern C++ \u00b6 We will use number of these new features of modern C++ 1. C++11 \u00b6 Variadic templates Alias templates Move semantics, rvalue references, and perfect forwarding Standard type traits 2. C++14 \u00b6 Variable templates Generic Lambdas 3. C++17 \u00b6 Class template argument deduction Compile-time if Fold expressions Style Guide \u00b6 1. the order of constant qualifier. \u00b6 What is in front of const qualifier is always a constant 1 2 int const MAX_SIZE = 100 ; // the int is constant int * const P ; // the pointer cannot change, but int value can; 1 2 const int MAX_SIZE = 100 ; const int * P ; // you can not find what's constant value; reason1. easy to know what's constant. it's always what is in front of the const qualifier reason2. syntatical substitution principle. consider following example 1 2 3 4 5 6 7 typedef char * CHARS ; typedef CHARS const CPTR ; // constant pointer to chars // => typedef char * const CPTR ; using CHARS = char * : using CPTR = CHARS const ; // constant pointer to chars // => using CPTR = char * const ; The meaning of the second declaration is preseved when we textually replace CHARS with what it stands for; How ever if you write const before the type it qualifies. textually 1 2 3 typedef char * CHARS ; typedef const CHARS CTPR ; //const pointer to chars; // => typedef const char* CTPR // pointer to constant chars; footnote: note that typedef defines a \"type alias\" rather than a new type 1 2 3 4 typedef int newInt ; int i = 29 ; newInt j = 1999 ; i = j ; // OK 2. put the space between the & and the parameter name; \u00b6 by doing this, we emphasize the separation between the parameter type and the parameter name. 1 void foo ( int const & x ); 3. avoid declaring multiple entities in this way!. \u00b6 1 char * a , b ; according to the rules inherited from C, a is a pointer but b is an ordinary char ;","title":"Preface"},{"location":"Language/Preface/#why_c","text":"","title":"Why C++?"},{"location":"Language/Preface/#what_you_should_know_before","text":"you should be able to write C++ programs using components such as IOstreams and containers from C++ STL. You Should be also be familiar with the basic features of \"Modern C++\", such as auto, decltype, move semantics, and lambdas. c++17","title":"What You Should Know Before.."},{"location":"Language/Preface/#modern_c","text":"We will use number of these new features of modern C++","title":"modern C++"},{"location":"Language/Preface/#1_c11","text":"Variadic templates Alias templates Move semantics, rvalue references, and perfect forwarding Standard type traits","title":"1. C++11"},{"location":"Language/Preface/#2_c14","text":"Variable templates Generic Lambdas","title":"2. C++14"},{"location":"Language/Preface/#3_c17","text":"Class template argument deduction Compile-time if Fold expressions","title":"3. C++17"},{"location":"Language/Preface/#style_guide","text":"","title":"Style Guide"},{"location":"Language/Preface/#1_the_order_of_constant_qualifier","text":"What is in front of const qualifier is always a constant 1 2 int const MAX_SIZE = 100 ; // the int is constant int * const P ; // the pointer cannot change, but int value can; 1 2 const int MAX_SIZE = 100 ; const int * P ; // you can not find what's constant value; reason1. easy to know what's constant. it's always what is in front of the const qualifier reason2. syntatical substitution principle. consider following example 1 2 3 4 5 6 7 typedef char * CHARS ; typedef CHARS const CPTR ; // constant pointer to chars // => typedef char * const CPTR ; using CHARS = char * : using CPTR = CHARS const ; // constant pointer to chars // => using CPTR = char * const ; The meaning of the second declaration is preseved when we textually replace CHARS with what it stands for; How ever if you write const before the type it qualifies. textually 1 2 3 typedef char * CHARS ; typedef const CHARS CTPR ; //const pointer to chars; // => typedef const char* CTPR // pointer to constant chars; footnote: note that typedef defines a \"type alias\" rather than a new type 1 2 3 4 typedef int newInt ; int i = 29 ; newInt j = 1999 ; i = j ; // OK","title":"1. the order of constant qualifier."},{"location":"Language/Preface/#2_put_the_space_between_the_amp_and_the_parameter_name","text":"by doing this, we emphasize the separation between the parameter type and the parameter name. 1 void foo ( int const & x );","title":"2. put the space between the &amp; and the parameter name;"},{"location":"Language/Preface/#3_avoid_declaring_multiple_entities_in_this_way","text":"1 char * a , b ; according to the rules inherited from C, a is a pointer but b is an ordinary char ;","title":"3. avoid declaring multiple entities in this way!."},{"location":"MISC/Lemma/","text":"Lemma \u00b6 Lemma-1 \u00b6 if we run $dfs(root)$ in a rooted tree, then v is an ancestor of $u$ if and only if $st_v\\leq st_u\\leq ft_u\\leq ft_v$. Lemma-2 \u00b6","title":"Lemma"},{"location":"MISC/Lemma/#lemma","text":"","title":"Lemma"},{"location":"MISC/Lemma/#lemma-1","text":"if we run $dfs(root)$ in a rooted tree, then v is an ancestor of $u$ if and only if $st_v\\leq st_u\\leq ft_u\\leq ft_v$.","title":"Lemma-1"},{"location":"MISC/Lemma/#lemma-2","text":"","title":"Lemma-2"}]}