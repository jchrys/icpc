{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to ICPC.NINJA \u00b6 Algorithms for competitive programming","title":"Home"},{"location":"#welcome_to_icpcninja","text":"Algorithms for competitive programming","title":"Welcome to ICPC.NINJA"},{"location":"Algorithms/EuclideanAlgorithm/","text":"Euclidean Algorithm \u00b6 Euclidean algorithm, or Euclid's algorithm, is an efficient method for computing the GCD(greatest common divisor) Implementation \u00b6 1 2 3 4 int gcd ( int a , int b ) { if ( ! a ) return b ; return gcd ( b % a , a ); } Proof \u00b6 This proof consists of two parts. - part 1: proves that Euclidean Algorithm gives us a common factor of two integers. - part 2: proves that the common divisor that Euclidean Algorithm produces is the largest possible Part1 \u00b6 Euclidean Algorithm gives us a common factor of two integers($a, b$). $$ \\begin{aligned} a & = q_1b + r_1, &\\text{where}(0<r<b)& \\\\ b & = q_2r_1 + r_2, &\\text{where}(0<r_2<r_1)& \\\\ r_1 & = q_3r_2 + r_3, &\\text{where}(0<r_3<r_2)&\\\\ &\\vdots \\\\ r_i & = q_{i+2}r_{i+1} + r_{i+2}, & \\text{where}( 0 < r_{i+2} < r_{i+1})& \\\\ &\\vdots \\\\ r_{k-2} & = q_{k}r_{k-1} + r_{k}\\\\ r_{k-1} & = q_{k+1}r_k \\\\ \\end{aligned} $$ From the last euqation, we know that $r_k|r_{k-1}$. So, we know that we can express $r_{k-1} = cr_k$. where $c$ is an integer. Now consider the previous equation. $$ \\begin{aligned} r_{k-2} & = q_{k} r_{k-1} + r_{k} \\\\ & = q_{k} cr_{k} + r_{k} \\\\ & = r_{k} (q_{k}c + 1) \\\\ \\end{aligned} $$ Thus, we have that $r_{k} | r_{k-2}$. In our equation previous to that one, we have $$ r_{k-3} = q_{k-1} r_{k-2} + r_{k-1} $$ From here, since $r_{k} | r_{k-1}$ and $r_{k}| r{k-2}$, using our rules of divisibility we have that $r_{k} | r_{k-3}$. As you can see, we can continue this process, considering each previous equation until we get to the last two, where we will find that $r_{k} | a$ and $r_{k} | b$. Thus, we find that Euclids algorithm gives us a common factor of a and b. Part2 \u00b6 The common divisor Euclidean Algorithm produces is the largest possible. We will start by assuming that $a$ and $b$ have a common factor $d$, and then show that $d | r_{k}$. consider an arbitrary common factor d of $a$ and $b$. If $d$ is a common factor, we can rewrite $a$ and $b$ as follows: $$ a = d a^{\\prime}, b = d b^{\\prime}, \\text{where } d, a, b \\text{ are all positive integers } $$ Now, consider the first euqation from Euclidean algorithm: $$ \\begin{aligned} a & = q_{1} b + r_{1} \\\\ r_{1} & = a - q_{1}b \\\\ & = da^{\\prime} - q_{1} d b^{\\prime} \\\\ & = d(a^{\\prime} - q_{1}b^{\\prime}) \\end{aligned} $$ Thus, we have that $d|r_{1}$. Now, consider the second equation, and repeat the steps we did on the first, this time solving for $r_{2}$ $$ \\begin{aligned} b & = q_{2}r_{1} + r_{2} \\\\ r_{2} & = b - q_{2}r_{1} \\\\ & = d b^{\\prime} - q_{2}dr_1^{\\prime} \\\\ & = d (b^{\\prime} - q_{2} r_1^{\\prime}) \\end{aligned} $$ As you can see, we can continue this process through each of the equations until we hit the second to last one, where we will have $$ \\begin{aligned} r_{k-2} & = q_{k}r_{k-1} + r_{k} \\\\ r_{k} & = q_{k}r_{k-1} - r_{k-2} \\\\ & = q_{k}dr_{k-1}^{\\prime} - d r_{k-2}^{\\prime} \\\\ & = d(q_{k}r_{k-1}^{\\prime} - r_{k-2}^{\\prime}) \\end{aligned} $$ Thus, $d| r_k$ But this says that any arbitrary common factor of $a$ and $b$ that we originally picked divides into $r_{k}$, the value that Euclidean algorithm produced. Since we know that $r_{k}$ is a common factor to both $a$ and $b$, this shows that is must be the largest possible common factor, or $gcd(a, b)$ $\\blacksquare$ Extended Euclidean Algorithm \u00b6 Given integers $a$ and $b$, there is always an integral solution to the equation $$ ax + by = gcd(a, b) $$ and we can find the values of $x$ and $y$. implementation \u00b6 not yet Proof \u00b6 Consider writing down the steps of Euclidean Algorithm $$ \\begin{aligned} a & = q_1b + r_1, &\\text{where}(0<r<b)& \\\\ b & = q_2r_1 + r_2, &\\text{where}(0<r_2<r_1)& \\\\ r_1 & = q_3r_2 + r_3, &\\text{where}(0<r_3<r_2)&\\\\ &\\vdots \\\\ r_i & = q_{i+2}r_{i+1} + r_{i+2}, & \\text{where}( 0 < r_{i+2} < r_{i+1})& \\\\ &\\vdots \\\\ r_{k-2} & = q_{k}r_{k-1} + r_{k} & \\text{where}(0 < r_k < r_{k-1}) \\\\ r_{k-1} & = q_{k+1}r_k \\\\ \\end{aligned} $$ Consider solving the second to last euqation for $r_k$. You get $$ \\begin{aligned} r_{k-2} & = q_{k}r_{k-1} + r_{k} \\\\ r_{k} & = r_{k-2} - q_{k}r_{k-1} \\\\ gcd(a, b) & = r_{k-2} - q_{k}r_{k-1} \\end{aligned} $$ Now, solve the previous equation for $r_{k-1}$ $$ \\begin{aligned} r_{k-3} & = q_{k-1}r_{k-2} + r_{k-1} \\\\ r_{k-1} & = r_{k-3} - q_{k-1}r_{k-2} \\\\ \\end{aligned} $$ and we substitute this value in to the previous derived equation $$ \\begin{aligned} gcd(a, b) & = r_{k-2} - q_{k}r_{k-1} \\\\ & = r_{k-2} - q_k(r_{k-3} - q_{k-1}r_{k-2}) \\\\ & = r_{k-2}(1 - q_{k-1}) - q_kr_{k-3} \\end{aligned} $$ Notice that now we have expressed $gcd(a, b)$ as a linear combination of $r_{k-2}$ and $r_{k-3}$. Next we can substitute for of $r_{k-2}$ in terms of $r_{k-3}$ and $r_{k-4}$, so that the $gcd(a, b)$ can be expressed as the linear combination of $r_{k-3}$ and $r_{k-4}$. Eventually, by continuing this process, $gcd(a, b)$ will be expressed as a linear combination of $a$ and $b$ as desired. This process will be much easier to see with examples: Find integers $x$ and $y$ such that $$ 135x + 50y = 5 $$ Use Euclidean Algorithm to compute $gcd(135, 50)$ $$ \\begin{aligned} 135 & = 2 \\sdot 50 + 35 \\cdots\\text{\\textcircled 1}\\\\ 50 & = 1 \\sdot 35 + 15 \\cdots\\text{\\textcircled 2}\\\\ 35 & = 2 \\sdot 15 + 5 \\cdots\\text{\\textcircled 3}\\\\ 15 & = 3 \\sdot 5 \\end{aligned} $$ Now, let's use the Extended Euclidean algorithm to solve the problem $ 5 = 35 - 2 \\sdot 15 $ from equation 3 But, we have that $ 15 = 50 - 35 $ from euqation $\\text{\\textcircled 2}$ Now we, substitute this value into the previously derived equation: $$ \\begin{aligned} 5 & = 35 - 2 \\sdot 15 \\\\ 5 & = 35 - 2 \\sdot (50 - 35) \\\\ 5 & = 3 \\sdot 35 - 2 \\sdot 50 \\end{aligned} $$ Now, finally use the first equation to determine an expression for $35$ as a linear combination of $135$ and $50$ $$ 35 = 135 - 2 \\sdot 50 \\text{ from equation}\\text{\\textcircled 1} $$ Plug this into our last euqation: $$ \\begin{aligned} 5 & = 3 \\sdot 35 - 2 \\sdot 50 \\\\ 5 & = 3 \\sdot (135 - 2 \\sdot 50) - 2 \\sdot 50 \\\\ 5 & = 3 \\sdot 135 - 8 \\sdot 50 \\end{aligned} $$ So, a set of solutions to the equation is $x=3, y = -8$ This article is from 'COT3100Euclid01' \u00b6","title":"Euclidean"},{"location":"Algorithms/EuclideanAlgorithm/#euclidean_algorithm","text":"Euclidean algorithm, or Euclid's algorithm, is an efficient method for computing the GCD(greatest common divisor)","title":"Euclidean Algorithm"},{"location":"Algorithms/EuclideanAlgorithm/#implementation","text":"1 2 3 4 int gcd ( int a , int b ) { if ( ! a ) return b ; return gcd ( b % a , a ); }","title":"Implementation"},{"location":"Algorithms/EuclideanAlgorithm/#proof","text":"This proof consists of two parts. - part 1: proves that Euclidean Algorithm gives us a common factor of two integers. - part 2: proves that the common divisor that Euclidean Algorithm produces is the largest possible","title":"Proof"},{"location":"Algorithms/EuclideanAlgorithm/#part1","text":"Euclidean Algorithm gives us a common factor of two integers($a, b$). $$ \\begin{aligned} a & = q_1b + r_1, &\\text{where}(0<r<b)& \\\\ b & = q_2r_1 + r_2, &\\text{where}(0<r_2<r_1)& \\\\ r_1 & = q_3r_2 + r_3, &\\text{where}(0<r_3<r_2)&\\\\ &\\vdots \\\\ r_i & = q_{i+2}r_{i+1} + r_{i+2}, & \\text{where}( 0 < r_{i+2} < r_{i+1})& \\\\ &\\vdots \\\\ r_{k-2} & = q_{k}r_{k-1} + r_{k}\\\\ r_{k-1} & = q_{k+1}r_k \\\\ \\end{aligned} $$ From the last euqation, we know that $r_k|r_{k-1}$. So, we know that we can express $r_{k-1} = cr_k$. where $c$ is an integer. Now consider the previous equation. $$ \\begin{aligned} r_{k-2} & = q_{k} r_{k-1} + r_{k} \\\\ & = q_{k} cr_{k} + r_{k} \\\\ & = r_{k} (q_{k}c + 1) \\\\ \\end{aligned} $$ Thus, we have that $r_{k} | r_{k-2}$. In our equation previous to that one, we have $$ r_{k-3} = q_{k-1} r_{k-2} + r_{k-1} $$ From here, since $r_{k} | r_{k-1}$ and $r_{k}| r{k-2}$, using our rules of divisibility we have that $r_{k} | r_{k-3}$. As you can see, we can continue this process, considering each previous equation until we get to the last two, where we will find that $r_{k} | a$ and $r_{k} | b$. Thus, we find that Euclids algorithm gives us a common factor of a and b.","title":"Part1"},{"location":"Algorithms/EuclideanAlgorithm/#part2","text":"The common divisor Euclidean Algorithm produces is the largest possible. We will start by assuming that $a$ and $b$ have a common factor $d$, and then show that $d | r_{k}$. consider an arbitrary common factor d of $a$ and $b$. If $d$ is a common factor, we can rewrite $a$ and $b$ as follows: $$ a = d a^{\\prime}, b = d b^{\\prime}, \\text{where } d, a, b \\text{ are all positive integers } $$ Now, consider the first euqation from Euclidean algorithm: $$ \\begin{aligned} a & = q_{1} b + r_{1} \\\\ r_{1} & = a - q_{1}b \\\\ & = da^{\\prime} - q_{1} d b^{\\prime} \\\\ & = d(a^{\\prime} - q_{1}b^{\\prime}) \\end{aligned} $$ Thus, we have that $d|r_{1}$. Now, consider the second equation, and repeat the steps we did on the first, this time solving for $r_{2}$ $$ \\begin{aligned} b & = q_{2}r_{1} + r_{2} \\\\ r_{2} & = b - q_{2}r_{1} \\\\ & = d b^{\\prime} - q_{2}dr_1^{\\prime} \\\\ & = d (b^{\\prime} - q_{2} r_1^{\\prime}) \\end{aligned} $$ As you can see, we can continue this process through each of the equations until we hit the second to last one, where we will have $$ \\begin{aligned} r_{k-2} & = q_{k}r_{k-1} + r_{k} \\\\ r_{k} & = q_{k}r_{k-1} - r_{k-2} \\\\ & = q_{k}dr_{k-1}^{\\prime} - d r_{k-2}^{\\prime} \\\\ & = d(q_{k}r_{k-1}^{\\prime} - r_{k-2}^{\\prime}) \\end{aligned} $$ Thus, $d| r_k$ But this says that any arbitrary common factor of $a$ and $b$ that we originally picked divides into $r_{k}$, the value that Euclidean algorithm produced. Since we know that $r_{k}$ is a common factor to both $a$ and $b$, this shows that is must be the largest possible common factor, or $gcd(a, b)$ $\\blacksquare$","title":"Part2"},{"location":"Algorithms/EuclideanAlgorithm/#extended_euclidean_algorithm","text":"Given integers $a$ and $b$, there is always an integral solution to the equation $$ ax + by = gcd(a, b) $$ and we can find the values of $x$ and $y$.","title":"Extended Euclidean Algorithm"},{"location":"Algorithms/EuclideanAlgorithm/#implementation_1","text":"not yet","title":"implementation"},{"location":"Algorithms/EuclideanAlgorithm/#proof_1","text":"Consider writing down the steps of Euclidean Algorithm $$ \\begin{aligned} a & = q_1b + r_1, &\\text{where}(0<r<b)& \\\\ b & = q_2r_1 + r_2, &\\text{where}(0<r_2<r_1)& \\\\ r_1 & = q_3r_2 + r_3, &\\text{where}(0<r_3<r_2)&\\\\ &\\vdots \\\\ r_i & = q_{i+2}r_{i+1} + r_{i+2}, & \\text{where}( 0 < r_{i+2} < r_{i+1})& \\\\ &\\vdots \\\\ r_{k-2} & = q_{k}r_{k-1} + r_{k} & \\text{where}(0 < r_k < r_{k-1}) \\\\ r_{k-1} & = q_{k+1}r_k \\\\ \\end{aligned} $$ Consider solving the second to last euqation for $r_k$. You get $$ \\begin{aligned} r_{k-2} & = q_{k}r_{k-1} + r_{k} \\\\ r_{k} & = r_{k-2} - q_{k}r_{k-1} \\\\ gcd(a, b) & = r_{k-2} - q_{k}r_{k-1} \\end{aligned} $$ Now, solve the previous equation for $r_{k-1}$ $$ \\begin{aligned} r_{k-3} & = q_{k-1}r_{k-2} + r_{k-1} \\\\ r_{k-1} & = r_{k-3} - q_{k-1}r_{k-2} \\\\ \\end{aligned} $$ and we substitute this value in to the previous derived equation $$ \\begin{aligned} gcd(a, b) & = r_{k-2} - q_{k}r_{k-1} \\\\ & = r_{k-2} - q_k(r_{k-3} - q_{k-1}r_{k-2}) \\\\ & = r_{k-2}(1 - q_{k-1}) - q_kr_{k-3} \\end{aligned} $$ Notice that now we have expressed $gcd(a, b)$ as a linear combination of $r_{k-2}$ and $r_{k-3}$. Next we can substitute for of $r_{k-2}$ in terms of $r_{k-3}$ and $r_{k-4}$, so that the $gcd(a, b)$ can be expressed as the linear combination of $r_{k-3}$ and $r_{k-4}$. Eventually, by continuing this process, $gcd(a, b)$ will be expressed as a linear combination of $a$ and $b$ as desired. This process will be much easier to see with examples: Find integers $x$ and $y$ such that $$ 135x + 50y = 5 $$ Use Euclidean Algorithm to compute $gcd(135, 50)$ $$ \\begin{aligned} 135 & = 2 \\sdot 50 + 35 \\cdots\\text{\\textcircled 1}\\\\ 50 & = 1 \\sdot 35 + 15 \\cdots\\text{\\textcircled 2}\\\\ 35 & = 2 \\sdot 15 + 5 \\cdots\\text{\\textcircled 3}\\\\ 15 & = 3 \\sdot 5 \\end{aligned} $$ Now, let's use the Extended Euclidean algorithm to solve the problem $ 5 = 35 - 2 \\sdot 15 $ from equation 3 But, we have that $ 15 = 50 - 35 $ from euqation $\\text{\\textcircled 2}$ Now we, substitute this value into the previously derived equation: $$ \\begin{aligned} 5 & = 35 - 2 \\sdot 15 \\\\ 5 & = 35 - 2 \\sdot (50 - 35) \\\\ 5 & = 3 \\sdot 35 - 2 \\sdot 50 \\end{aligned} $$ Now, finally use the first equation to determine an expression for $35$ as a linear combination of $135$ and $50$ $$ 35 = 135 - 2 \\sdot 50 \\text{ from equation}\\text{\\textcircled 1} $$ Plug this into our last euqation: $$ \\begin{aligned} 5 & = 3 \\sdot 35 - 2 \\sdot 50 \\\\ 5 & = 3 \\sdot (135 - 2 \\sdot 50) - 2 \\sdot 50 \\\\ 5 & = 3 \\sdot 135 - 8 \\sdot 50 \\end{aligned} $$ So, a set of solutions to the equation is $x=3, y = -8$","title":"Proof"},{"location":"Algorithms/EuclideanAlgorithm/#this_article_is_from_cot3100euclid01","text":"","title":"This article is from 'COT3100Euclid01'"},{"location":"Algorithms/FastPower/","text":"Fast Power Algorithm \u00b6 FAST! time complexity $\\Omicron(pow)$ Implementation \u00b6 Recursive \u00b6 1 2 3 4 5 6 7 int power ( int base , int pow ) { if ( ! pow ) return 1 ; if ( n & 1 ) // if n is odd return base * power ( base , ( pow - 1 ) >> 1 ) * power ( base , ( pow - 1 ) >> 1 ); return power ( base , pow >> 1 ) * power ( base , pow >> 1 ); } Iterative \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 int power ( int base , int pow ) { if ( ! pow ) return 1 ; int result = 1 ; while ( pow ) { if ( pow & 1 ) { result *= base ; pow -- ; // not necessary } pow >>= 1 ; base *= base ; } return result ; } How it works? \u00b6","title":"FastPower"},{"location":"Algorithms/FastPower/#fast_power_algorithm","text":"FAST! time complexity $\\Omicron(pow)$","title":"Fast Power Algorithm"},{"location":"Algorithms/FastPower/#implementation","text":"","title":"Implementation"},{"location":"Algorithms/FastPower/#recursive","text":"1 2 3 4 5 6 7 int power ( int base , int pow ) { if ( ! pow ) return 1 ; if ( n & 1 ) // if n is odd return base * power ( base , ( pow - 1 ) >> 1 ) * power ( base , ( pow - 1 ) >> 1 ); return power ( base , pow >> 1 ) * power ( base , pow >> 1 ); }","title":"Recursive"},{"location":"Algorithms/FastPower/#iterative","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 int power ( int base , int pow ) { if ( ! pow ) return 1 ; int result = 1 ; while ( pow ) { if ( pow & 1 ) { result *= base ; pow -- ; // not necessary } pow >>= 1 ; base *= base ; } return result ; }","title":"Iterative"},{"location":"Algorithms/FastPower/#how_it_works","text":"","title":"How it works?"},{"location":"Algorithms/Graph/Basics/","text":"Graph \u00b6 A graph is consists of nodes and edges Path: A path leads from node a to node b through edges of the graph. Length: The length of a path is the number of edges in it. Cycle: A path is a cycle if the first and last node is the same. Simple: A path is simple if each node appears at most once in the path. Terminologies \u00b6 1. Connectivity \u00b6 A graph is connected if there is a path between any two nodes. Components: Connected parts of its grpah. Tree: a tree is a connected graph that consists of $n$ nodes and $n-1$ edges. 2. Edge directions \u00b6 A graph is directed if the edges can be traversed in one direction only 3. Edge weights \u00b6 In a weighted graph, each edge is assigned a weight. often interpreted as edge length 4. Neighbors and degrees \u00b6 Two nodes are $neighbors$ or $adjacent$ if there is an edge between them Degree: The degrege of a node is number of its neighbors Indegree: The indegree of a node is the number of edges that end at the node Outdegree: The outdegree of a node is the number of edges that start at the node Regular graph: A graph is regular if the degree of every node is a constant d Complete graph: A graph is complete if the degree of every node is $n-1$ Colorings \u00b6 In a coloring of a graph, each node is assigned a color so that no adjacent nodes have the same color Note It turns out that a graph is bipartite exactly when it does not contain a cycle with an odd number of edges Simplicity \u00b6 A graph is simple if no edge starts and ends at the same node(loop), and there are no multiple edges between two nodes. Graph representation \u00b6 There are several ways to represent graphs in algorithms. The choice of a data structure depends on the size of graph and the way the algorithm processes it 1. Adjacency list representation \u00b6 In the adjacency list representation, each node x in the graph is assigned an adjacency list that consists of nodes to which there is an edge from x we can efficiently find the nodes to which we can move from a given node through an edge 2. Adjacency matrix representation \u00b6 An adjacency matrix is two dimensional array that indicates which edges the graph contains. we can efficiently check from an adjacency matrix if there is an edge between two nodes. 3. Edge list representation \u00b6 An edge list contains all edges of a graph in some order. This is convenient way to represent a graph if the algorithm proesses all edges of the graph and it is not needed to find edges that start at a given node.","title":"Basics"},{"location":"Algorithms/Graph/Basics/#graph","text":"A graph is consists of nodes and edges Path: A path leads from node a to node b through edges of the graph. Length: The length of a path is the number of edges in it. Cycle: A path is a cycle if the first and last node is the same. Simple: A path is simple if each node appears at most once in the path.","title":"Graph"},{"location":"Algorithms/Graph/Basics/#terminologies","text":"","title":"Terminologies"},{"location":"Algorithms/Graph/Basics/#1_connectivity","text":"A graph is connected if there is a path between any two nodes. Components: Connected parts of its grpah. Tree: a tree is a connected graph that consists of $n$ nodes and $n-1$ edges.","title":"1. Connectivity"},{"location":"Algorithms/Graph/Basics/#2_edge_directions","text":"A graph is directed if the edges can be traversed in one direction only","title":"2. Edge directions"},{"location":"Algorithms/Graph/Basics/#3_edge_weights","text":"In a weighted graph, each edge is assigned a weight. often interpreted as edge length","title":"3. Edge weights"},{"location":"Algorithms/Graph/Basics/#4_neighbors_and_degrees","text":"Two nodes are $neighbors$ or $adjacent$ if there is an edge between them Degree: The degrege of a node is number of its neighbors Indegree: The indegree of a node is the number of edges that end at the node Outdegree: The outdegree of a node is the number of edges that start at the node Regular graph: A graph is regular if the degree of every node is a constant d Complete graph: A graph is complete if the degree of every node is $n-1$","title":"4. Neighbors and degrees"},{"location":"Algorithms/Graph/Basics/#colorings","text":"In a coloring of a graph, each node is assigned a color so that no adjacent nodes have the same color Note It turns out that a graph is bipartite exactly when it does not contain a cycle with an odd number of edges","title":"Colorings"},{"location":"Algorithms/Graph/Basics/#simplicity","text":"A graph is simple if no edge starts and ends at the same node(loop), and there are no multiple edges between two nodes.","title":"Simplicity"},{"location":"Algorithms/Graph/Basics/#graph_representation","text":"There are several ways to represent graphs in algorithms. The choice of a data structure depends on the size of graph and the way the algorithm processes it","title":"Graph representation"},{"location":"Algorithms/Graph/Basics/#1_adjacency_list_representation","text":"In the adjacency list representation, each node x in the graph is assigned an adjacency list that consists of nodes to which there is an edge from x we can efficiently find the nodes to which we can move from a given node through an edge","title":"1. Adjacency list representation"},{"location":"Algorithms/Graph/Basics/#2_adjacency_matrix_representation","text":"An adjacency matrix is two dimensional array that indicates which edges the graph contains. we can efficiently check from an adjacency matrix if there is an edge between two nodes.","title":"2. Adjacency matrix representation"},{"location":"Algorithms/Graph/Basics/#3_edge_list_representation","text":"An edge list contains all edges of a graph in some order. This is convenient way to represent a graph if the algorithm proesses all edges of the graph and it is not needed to find edges that start at a given node.","title":"3. Edge list representation"},{"location":"Algorithms/Graph/DeBruijnSequences/","text":"De Bruijn sequences \u00b6 A De Bruijn sequence is a string that contains every string of length $n$ exactly once as a substring, for fixed alphabet of $k$ characters. The length of such a string is $k^n + n - 1$ characters. It turns out that each De Bruijn sequence corresponds to an Eulerian path in a graph. The idea is to construct a graph where each node contains a string of $n-1$ characters and each edge adds one character to the string. An Eulerian path in this graph corresponds to a string that contains all strings of length $n$. The string contains the characters of the starting node and all characters of the edges. The starting node has $n-1$ characters and there are $k^n$ characters in the edges, so the length of the string is $k^n + n - 1$.","title":"De Bruijn sequences"},{"location":"Algorithms/Graph/DeBruijnSequences/#de_bruijn_sequences","text":"A De Bruijn sequence is a string that contains every string of length $n$ exactly once as a substring, for fixed alphabet of $k$ characters. The length of such a string is $k^n + n - 1$ characters. It turns out that each De Bruijn sequence corresponds to an Eulerian path in a graph. The idea is to construct a graph where each node contains a string of $n-1$ characters and each edge adds one character to the string. An Eulerian path in this graph corresponds to a string that contains all strings of length $n$. The string contains the characters of the starting node and all characters of the edges. The starting node has $n-1$ characters and there are $k^n$ characters in the edges, so the length of the string is $k^n + n - 1$.","title":"De Bruijn sequences"},{"location":"Algorithms/Graph/DirectedGraphs/","text":"Directed graphs \u00b6 Acyclic graphs($\\text{DAGs}$): There are no cycles in the graph, so there is no path from any node to itself. Successor graphs: The out degree of each node is 1, so each node has a unique successor. Topological sorting \u00b6 A Topological sort is an ordering of the nodes of a directed graph such that if there is a path from node $a$ to node $b$, then node $a$ appears before node $b$ in the ordering. An acyclic graph always has a topological sort. Note It turns out that depth-first search($\\text{DFS}$) can be used to both check if a directed graph contains a cycle and, if it does not contain a cycle, to construct a topological sort. Implementation(Using DFS) \u00b6 The idea is to go through the nodes of the graph and always begin a depth-first search at the current node if it has not been processed yet. during the the searches the node have three possible states: state 0: the node has not been processed state 1: the node is under processing state 2: the node has been processed Initially, the state of each node is 0. When a search reaches a node for the first time, its state becomes1. Finally, after all successors of the node have been processed, its state becomes 2. If the graph contains a cycle, we will find this out during the search, because sooner or later we wil arrive at a node whose state is 1. In this case, it is not possible to construct a topological sort. If the graph does not contain a cycle, we can construct a topological sort by adding each node to a list when the state of the node becomes 2. This list in reverse order is topological sort. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 vector < int > adj []; // adjancency list int color []; // array: color of nodes vector < int > topsort bool cycle = false ; void dfs ( int u ) { if ( color [ u ] == 2 ) return ; //already processed if ( color [ u ] == 1 ) { //cycle detected cycle = true ; return ; } color [ u ] = 1 ; // color as under processing for ( auto v : adj [ u ]) dfs ( v ); color [ u ] == 2 topsort . push_back ( u ); } for ( int i = 0 ; i < # edges ; i ++ ) { if ( color [ i ] == 0 ) // if node is not processed yet dfs ( i ); } if ( cycle ) { cout << \"cycle detected\" << endl ; } else { for ( auto it = topsort . rbegin (); it != topsort . rend (); it ++ ) { cout << * it << ' ' ; } cout << endl ; } Related Problems \u00b6 \uc791\uc5c5\uc21c\uc11c Dynamic Programming \u00b6 If a directed graph is acyclic, dynamic programming can be applied to it. For example, we can efficiently solve the following problems concerning paths from a starting node to and ending node 1. Counting the number of paths \u00b6 Let paths(x) denote the number of paths from node $1$ to node $x$. As a basecase, paths(1) 1. Then to calculate other values of paths(x) , we may use recursion 1 paths ( x ) = paths ( a1 ) + paths ( a2 ) + ... + paths ( ak ) where a1, a2, ..., ak are the nodes from which there is an edge to x. Since the graph is acyclic the values of paths(x) can be calculated in the order of topological sort. 2. Extending Dijkstra's algorithm \u00b6 A by-product of Dijkstra's algorithm is a directed, acyclic graph that indicates for each node of the original graph the possible ways to reach the node using a shortest path from the starting node. Dynamic programming can be applied to that graph. thus we can find number of shortest paths from node $a$ to node $b$ 3. Representing problems as graphs \u00b6 Actually, any dynamic programming problem can be represented as a directed acyclic graph. In such a graph, each node coressponds to a dynamic programming state and the edges indicate how the states depend on each other. Successor paths \u00b6 Successor graphs: the outdegree of each node is 1. A successor graph consists of one or more components, each of which contains one cycle and some paths that lead to it. Successor graphs are sometimes called functional graphs. The reason for that is that any successor graph corresponds to a function that defines the edges of the graph. The parameter of the function is a node of the graph, and the function gives the successor of that node. succ(x, k) \u00b6 Since each node of a successor graph has a unique successor, we can also define a function succ(x, k) that gives the node that we will reach if we begin at node x and k step forward. Using preprocessing, any value of succ(x, k) can be calculated only $\\Omicron(lgk)$ time. The idea is to precalculate all values of succ(x, k) where k is a power of two and at most u , where u is the maximum number of steps we will ever walk. This can be efficiently done, because we can use the following recursion 1 2 3 4 5 6 int succ ( x , k ) { if ( k == 1 ) { return succ ( x ) ; } succ ( succ ( x , k / 2 ) , k / 2 ) ; } Precalculating the values takes $\\Omicron(nlgu)$ time because $\\Omicron(lgu)$ values are calculated for each node. After precalculating, any value of succ(x, k) can be calculating presenting the number of steps k as a sum of pwoers of two. $$ succ(x, 11) = succ(succ(succ(x, 8), 2), 1); $$ Such a representation always consists of $\\Omicron(lgk)$ parts, so calculating a value of succ(x, k) takes $\\Omicron(lgk)$ time. Cycle Detection \u00b6 Consider a successor graph that only contains a path that ends in a cycle. A simple way to detect the cycle is to walk in the graph and keep track of all nodes that have been visited. Once a node is visited for the second time we can conclude that the node is the first node in the cycle. This method works in $\\Omicron(n)$ time and also uses $\\Omciron(n)$ memory. There are better algorithms for cycle detection. The time complexity of such algorithm is still $\\Omicron(n)$, but they only use $\\Omicron(1)$ memory. This is an important improvement if $n$ is large. Floyd's algorithm \u00b6 Floyd's algorithm walks forward in the graph using two pointers $a$ and $b$. Both pointers begin at node $x$ that is the starting node of the graph. Then on each turn, the pointer $a$ walks one step forward and the pointer $b$ walks to steps forward. The process continues until the pointer meet each other. 1 2 3 4 5 6 a = succ ( x ); b = succ ( succ ( x )); while ( a != b ) { a = succ ( a ); b = succ ( succ ( b )); } At this point, the pointer $a$ has walked $k$ steps and the pointer $b$ has walked $2k$ steps, so the length of the cycle divides $k$. Thus, the first node that belongs to the cycle can be found by moving the pointer $a$ to node $x$ and advancing the pointers step by step until they meet again. 1 2 3 4 5 6 a = x ; while ( a != b ) { a = succ ( a ); b = succ ( b ); } first = a ; After this, the length of the cycle can be calculated as follows 1 2 3 4 5 6 b = succ ( a ); length = 1 ; while ( a != b ) { b = succ ( b ); length ++ ; }","title":"Directed Graphs"},{"location":"Algorithms/Graph/DirectedGraphs/#directed_graphs","text":"Acyclic graphs($\\text{DAGs}$): There are no cycles in the graph, so there is no path from any node to itself. Successor graphs: The out degree of each node is 1, so each node has a unique successor.","title":"Directed graphs"},{"location":"Algorithms/Graph/DirectedGraphs/#topological_sorting","text":"A Topological sort is an ordering of the nodes of a directed graph such that if there is a path from node $a$ to node $b$, then node $a$ appears before node $b$ in the ordering. An acyclic graph always has a topological sort. Note It turns out that depth-first search($\\text{DFS}$) can be used to both check if a directed graph contains a cycle and, if it does not contain a cycle, to construct a topological sort.","title":"Topological sorting"},{"location":"Algorithms/Graph/DirectedGraphs/#implementationusing_dfs","text":"The idea is to go through the nodes of the graph and always begin a depth-first search at the current node if it has not been processed yet. during the the searches the node have three possible states: state 0: the node has not been processed state 1: the node is under processing state 2: the node has been processed Initially, the state of each node is 0. When a search reaches a node for the first time, its state becomes1. Finally, after all successors of the node have been processed, its state becomes 2. If the graph contains a cycle, we will find this out during the search, because sooner or later we wil arrive at a node whose state is 1. In this case, it is not possible to construct a topological sort. If the graph does not contain a cycle, we can construct a topological sort by adding each node to a list when the state of the node becomes 2. This list in reverse order is topological sort. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 vector < int > adj []; // adjancency list int color []; // array: color of nodes vector < int > topsort bool cycle = false ; void dfs ( int u ) { if ( color [ u ] == 2 ) return ; //already processed if ( color [ u ] == 1 ) { //cycle detected cycle = true ; return ; } color [ u ] = 1 ; // color as under processing for ( auto v : adj [ u ]) dfs ( v ); color [ u ] == 2 topsort . push_back ( u ); } for ( int i = 0 ; i < # edges ; i ++ ) { if ( color [ i ] == 0 ) // if node is not processed yet dfs ( i ); } if ( cycle ) { cout << \"cycle detected\" << endl ; } else { for ( auto it = topsort . rbegin (); it != topsort . rend (); it ++ ) { cout << * it << ' ' ; } cout << endl ; }","title":"Implementation(Using DFS)"},{"location":"Algorithms/Graph/DirectedGraphs/#related_problems","text":"\uc791\uc5c5\uc21c\uc11c","title":"Related Problems"},{"location":"Algorithms/Graph/DirectedGraphs/#dynamic_programming","text":"If a directed graph is acyclic, dynamic programming can be applied to it. For example, we can efficiently solve the following problems concerning paths from a starting node to and ending node","title":"Dynamic Programming"},{"location":"Algorithms/Graph/DirectedGraphs/#1_counting_the_number_of_paths","text":"Let paths(x) denote the number of paths from node $1$ to node $x$. As a basecase, paths(1) 1. Then to calculate other values of paths(x) , we may use recursion 1 paths ( x ) = paths ( a1 ) + paths ( a2 ) + ... + paths ( ak ) where a1, a2, ..., ak are the nodes from which there is an edge to x. Since the graph is acyclic the values of paths(x) can be calculated in the order of topological sort.","title":"1. Counting the number of paths"},{"location":"Algorithms/Graph/DirectedGraphs/#2_extending_dijkstras_algorithm","text":"A by-product of Dijkstra's algorithm is a directed, acyclic graph that indicates for each node of the original graph the possible ways to reach the node using a shortest path from the starting node. Dynamic programming can be applied to that graph. thus we can find number of shortest paths from node $a$ to node $b$","title":"2. Extending Dijkstra's algorithm"},{"location":"Algorithms/Graph/DirectedGraphs/#3_representing_problems_as_graphs","text":"Actually, any dynamic programming problem can be represented as a directed acyclic graph. In such a graph, each node coressponds to a dynamic programming state and the edges indicate how the states depend on each other.","title":"3. Representing problems as graphs"},{"location":"Algorithms/Graph/DirectedGraphs/#successor_paths","text":"Successor graphs: the outdegree of each node is 1. A successor graph consists of one or more components, each of which contains one cycle and some paths that lead to it. Successor graphs are sometimes called functional graphs. The reason for that is that any successor graph corresponds to a function that defines the edges of the graph. The parameter of the function is a node of the graph, and the function gives the successor of that node.","title":"Successor paths"},{"location":"Algorithms/Graph/DirectedGraphs/#succx_k","text":"Since each node of a successor graph has a unique successor, we can also define a function succ(x, k) that gives the node that we will reach if we begin at node x and k step forward. Using preprocessing, any value of succ(x, k) can be calculated only $\\Omicron(lgk)$ time. The idea is to precalculate all values of succ(x, k) where k is a power of two and at most u , where u is the maximum number of steps we will ever walk. This can be efficiently done, because we can use the following recursion 1 2 3 4 5 6 int succ ( x , k ) { if ( k == 1 ) { return succ ( x ) ; } succ ( succ ( x , k / 2 ) , k / 2 ) ; } Precalculating the values takes $\\Omicron(nlgu)$ time because $\\Omicron(lgu)$ values are calculated for each node. After precalculating, any value of succ(x, k) can be calculating presenting the number of steps k as a sum of pwoers of two. $$ succ(x, 11) = succ(succ(succ(x, 8), 2), 1); $$ Such a representation always consists of $\\Omicron(lgk)$ parts, so calculating a value of succ(x, k) takes $\\Omicron(lgk)$ time.","title":"succ(x, k)"},{"location":"Algorithms/Graph/DirectedGraphs/#cycle_detection","text":"Consider a successor graph that only contains a path that ends in a cycle. A simple way to detect the cycle is to walk in the graph and keep track of all nodes that have been visited. Once a node is visited for the second time we can conclude that the node is the first node in the cycle. This method works in $\\Omicron(n)$ time and also uses $\\Omciron(n)$ memory. There are better algorithms for cycle detection. The time complexity of such algorithm is still $\\Omicron(n)$, but they only use $\\Omicron(1)$ memory. This is an important improvement if $n$ is large.","title":"Cycle Detection"},{"location":"Algorithms/Graph/DirectedGraphs/#floyds_algorithm","text":"Floyd's algorithm walks forward in the graph using two pointers $a$ and $b$. Both pointers begin at node $x$ that is the starting node of the graph. Then on each turn, the pointer $a$ walks one step forward and the pointer $b$ walks to steps forward. The process continues until the pointer meet each other. 1 2 3 4 5 6 a = succ ( x ); b = succ ( succ ( x )); while ( a != b ) { a = succ ( a ); b = succ ( succ ( b )); } At this point, the pointer $a$ has walked $k$ steps and the pointer $b$ has walked $2k$ steps, so the length of the cycle divides $k$. Thus, the first node that belongs to the cycle can be found by moving the pointer $a$ to node $x$ and advancing the pointers step by step until they meet again. 1 2 3 4 5 6 a = x ; while ( a != b ) { a = succ ( a ); b = succ ( b ); } first = a ; After this, the length of the cycle can be calculated as follows 1 2 3 4 5 6 b = succ ( a ); length = 1 ; while ( a != b ) { b = succ ( b ); length ++ ; }","title":"Floyd's algorithm"},{"location":"Algorithms/Graph/DisjointPaths/","text":"Disjoint paths \u00b6 our task is to find the maximum number of disjoint paths from the source to the sink Edge-disjoint paths \u00b6 We will first focus on the problem of finding the maximum number of edge-disjoint paths from the source to the sink. This means that we should construct a set of paths such that each edge appears in at most one path. It turns out that the maximum number of edge-disjoint paths equals the maximum flow of the graph, assuming that the capacity of each edge is one. After the maximum flow has been constructed, the edge-disjoint paths can be found greedily by following paths from the source to sink. Node-disjoint paths \u00b6 Let us now consider another problem: finding the maximum number of node-disjoint paths from the source to the sink. In this problem, every node, except for the source and sink, may appear in at most one path. The number of node disjoint paths may be smaller than the number of edge-disjoint paths. We can reduce also this problem to the maximum flow problem. Since each node can appear in at most one path, we have to limit the flow that goes through the nodes. A standard method for this is to divide each node into two nodes such that the first node has the incoming edges of the original node, the second node has the outgoing edges of the original node, and there is a new edge from the first node to second node.","title":"Disjoint Paths"},{"location":"Algorithms/Graph/DisjointPaths/#disjoint_paths","text":"our task is to find the maximum number of disjoint paths from the source to the sink","title":"Disjoint paths"},{"location":"Algorithms/Graph/DisjointPaths/#edge-disjoint_paths","text":"We will first focus on the problem of finding the maximum number of edge-disjoint paths from the source to the sink. This means that we should construct a set of paths such that each edge appears in at most one path. It turns out that the maximum number of edge-disjoint paths equals the maximum flow of the graph, assuming that the capacity of each edge is one. After the maximum flow has been constructed, the edge-disjoint paths can be found greedily by following paths from the source to sink.","title":"Edge-disjoint paths"},{"location":"Algorithms/Graph/DisjointPaths/#node-disjoint_paths","text":"Let us now consider another problem: finding the maximum number of node-disjoint paths from the source to the sink. In this problem, every node, except for the source and sink, may appear in at most one path. The number of node disjoint paths may be smaller than the number of edge-disjoint paths. We can reduce also this problem to the maximum flow problem. Since each node can appear in at most one path, we have to limit the flow that goes through the nodes. A standard method for this is to divide each node into two nodes such that the first node has the incoming edges of the original node, the second node has the outgoing edges of the original node, and there is a new edge from the first node to second node.","title":"Node-disjoint paths"},{"location":"Algorithms/Graph/EdgeDisjointPaths/","text":"Edge-disjoint paths \u00b6 Problem of finding the maximum number of edge-disjoint paths from source to the sink. This means that we should construct a set of paths such that each edge appears in at most one path. Idea \u00b6 It turns out that the maximum number of of edge-disjoint paths equals the maximum flow of the graph, assuming that the capacity of each edge is one. After the maximum flow has been constructed, the edge-disjoint paths can be found greedily by following paths from the source to the sink","title":"Edge-disjoint paths"},{"location":"Algorithms/Graph/EdgeDisjointPaths/#edge-disjoint_paths","text":"Problem of finding the maximum number of edge-disjoint paths from source to the sink. This means that we should construct a set of paths such that each edge appears in at most one path.","title":"Edge-disjoint paths"},{"location":"Algorithms/Graph/EdgeDisjointPaths/#idea","text":"It turns out that the maximum number of of edge-disjoint paths equals the maximum flow of the graph, assuming that the capacity of each edge is one. After the maximum flow has been constructed, the edge-disjoint paths can be found greedily by following paths from the source to the sink","title":"Idea"},{"location":"Algorithms/Graph/EulerianPaths/","text":"Eulerian Path \u00b6 An Eulerian Path is a path that goes through each edge exactly one. It turns out that there is a simple rule that determines whether a graph contains an Eulerian path, and there is also an efficient algorithm to find a path if it exists. Existence \u00b6 The existence of Eulerian paths and circuits depends on the degrees of the nodes. In undireccted graph \u00b6 an undirected graph has an Eulerian path exactly when \"all the edges belong to the same connected component\" and Case 1: the degree of each node is even Case 2: the degree of exactly two nodes is odd, and the degree of all other nodes is even In the Case 1, each Eulerian path is also Eulerian circuit. In the Case2, the odd-degree nodes are the starting and ending nodes of an eulerian path which is not an Eulerian circuit. Directed graph \u00b6 In a directed graph, we focus on indegrees and outdegrees of the nodes. A directed graph contains an Eulerian path exactly when \"all the edges belong to the same connected component\" and Case 1: in each node, the indegree equals the outdegree Case 2: in one node, the indegree is one larger than the outdegree, in another node, the outdegree is one larger than the indegree, and in all other nodes, the indegree equals the out degree. In Case 1, each Eulerian path is also an Eulerian circuit. In case 2, the graph contains an Eulerian path that begins at the node whose out degree is larger and ends at the node whose indegree is larger. Hierholzer's Algorithm \u00b6 Hierholzer's algorithm is an efficient method for constructing an Eulerian circuit. The algorithm consists of several rounds, each of which adds new edges to the circuit. Of course, we assume that the graph contains an Eulerian circuit; otherwise Hierholzer's algorithm cannot find it First, the algorithm constructs a circuit that contains some of the edges of the graph. After this, the algorithm extends the circuit step by step by adding subcircuits to it. The process continues until all edges have been added to the circuit. The algorithm extends the circuit by always finding a node $x$ that belong to the circuit but has an outgoing edge that is not included in the circuit. The algorithm constructs a new path from node $x$ that only contains edges that are not yet in the circuit. Sooner or later, the path will return to node $x$, which creates a subcircuit If the graph only contains an Eulerian path, we can still use Hierholzer's algorithm to find it by adding an extra edge to the graph and removing the edge after the circuit has been constructed. For example, in an undirected graph, we add the extra edge between the two odd-degree nodes. Implementation \u00b6 --- will be added---","title":"Eulerian Paths"},{"location":"Algorithms/Graph/EulerianPaths/#eulerian_path","text":"An Eulerian Path is a path that goes through each edge exactly one. It turns out that there is a simple rule that determines whether a graph contains an Eulerian path, and there is also an efficient algorithm to find a path if it exists.","title":"Eulerian Path"},{"location":"Algorithms/Graph/EulerianPaths/#existence","text":"The existence of Eulerian paths and circuits depends on the degrees of the nodes.","title":"Existence"},{"location":"Algorithms/Graph/EulerianPaths/#in_undireccted_graph","text":"an undirected graph has an Eulerian path exactly when \"all the edges belong to the same connected component\" and Case 1: the degree of each node is even Case 2: the degree of exactly two nodes is odd, and the degree of all other nodes is even In the Case 1, each Eulerian path is also Eulerian circuit. In the Case2, the odd-degree nodes are the starting and ending nodes of an eulerian path which is not an Eulerian circuit.","title":"In undireccted graph"},{"location":"Algorithms/Graph/EulerianPaths/#directed_graph","text":"In a directed graph, we focus on indegrees and outdegrees of the nodes. A directed graph contains an Eulerian path exactly when \"all the edges belong to the same connected component\" and Case 1: in each node, the indegree equals the outdegree Case 2: in one node, the indegree is one larger than the outdegree, in another node, the outdegree is one larger than the indegree, and in all other nodes, the indegree equals the out degree. In Case 1, each Eulerian path is also an Eulerian circuit. In case 2, the graph contains an Eulerian path that begins at the node whose out degree is larger and ends at the node whose indegree is larger.","title":"Directed graph"},{"location":"Algorithms/Graph/EulerianPaths/#hierholzers_algorithm","text":"Hierholzer's algorithm is an efficient method for constructing an Eulerian circuit. The algorithm consists of several rounds, each of which adds new edges to the circuit. Of course, we assume that the graph contains an Eulerian circuit; otherwise Hierholzer's algorithm cannot find it First, the algorithm constructs a circuit that contains some of the edges of the graph. After this, the algorithm extends the circuit step by step by adding subcircuits to it. The process continues until all edges have been added to the circuit. The algorithm extends the circuit by always finding a node $x$ that belong to the circuit but has an outgoing edge that is not included in the circuit. The algorithm constructs a new path from node $x$ that only contains edges that are not yet in the circuit. Sooner or later, the path will return to node $x$, which creates a subcircuit If the graph only contains an Eulerian path, we can still use Hierholzer's algorithm to find it by adding an extra edge to the graph and removing the edge after the circuit has been constructed. For example, in an undirected graph, we add the extra edge between the two odd-degree nodes.","title":"Hierholzer's Algorithm"},{"location":"Algorithms/Graph/EulerianPaths/#implementation","text":"--- will be added---","title":"Implementation"},{"location":"Algorithms/Graph/FordFulkerson/","text":"Ford-Fulkerson algorithm \u00b6 The Ford-Fulkerson algorithm finds the maximum flow in a graph. The algorithm begins with an empty flow, and at each step finds a path from the source to the sink that generates more flow. Finally, when the algorithm cannot increase the flow anymore, the maximum flow has been found. The algorithm uses a special representation of the graph where each original edge has a reverse edge in another direction. The weight of each edge indicates how much more flow we could route through it. At the beginning of the algorithm, the weight of each original edge equals the capacity of the edge and the weight of each reverse edge is zero. Algorithm description \u00b6 The Ford-Fulkerson algorithm consists of several rounds. On each round, the algorithm finds a path from the source to the sink such that each edge on the path has a positive weight. If there is more than one possible path available, we can choose any of them. After choosing the path, the flow increases by $x$ units, where $x$ is the smallest edge weight on the path. In addition, the weight of each edge on the path decreases by $x$ and the weight of each reverse edge increases by $x$. The idea is that increasing the flow decreases the amount of flow that can go through the edges in the future. On the other hand, it is possible to cancel flow later using the reverse edges of the graph if it turns out that it would be beneficial to route the flow in another way. The algorithm increases the flow as long as there is a path from the source to the sink through positive-weight edges. If It is not possible to increase the flow anymore, because there is no path from the source to the sink with positive edge weights, the algorithm terminates and has found the maximum flow. Finding paths \u00b6 The Ford-Fulkerson algorithm does not specify how we should choose the paths that increase the flow. In any case, the algorithm will terminate sooner or later and correctly find the maximum flow. However, the efficiency of the algorithm depends on the way the paths are chosen. Path finding techniques \u00b6 In practive, the scaling algorithm is easier to implement , because depth-first search can be used for finding paths. Both Algorithms are efficient enough for problems that typically appear in programming contests. Edmond-Karp algorithm \u00b6 The algorithm chooses each path so that the number of edges on the path is as small as possible. This can be done by using $\\text{breadth-first search}$ instead of depth-first search for finding paths. It can be proven that this guarantees that the flow increases quickly, and the time complexity of the algorithm is $\\Omicron(m^2n)$. Scaling algorithm \u00b6 The algorithm uses depth-first search to find paths where each weight is at least a threshold value. Initially, the threshold value is some large number, for example the sum of all edge weights of the graph. Always when a path cannot be found, the threshold value is divided by 2. The time complexity of the algorithm is $\\Omicron(m^2lgc)$ , where $c$ is the initial threshold value. Dinic Algorithm \u00b6 Time complex $\\Omicron(EV^2)$ Like Edmond-Karp algorithm, Dinic's algorithm uses following concepts A flow is maximum if there is no $source$ to $sink$ path in residual graph. $\\text{BFS}$ is used in a loop. There is difference though in the way we use BFS in both algorithms. In Edmond's Karp algorithm, we use $\\text{BFS}$ to find a augmenting path and send flow accress this path. In Dinic's algorithm we use $\\text{BFS}$ to check if more flow is possible and to construct level graph. In, level graph, we assign levels to all nodes, level of a node is shortest distance (in terms of number of edges) of the node from source. Once level graph is constructed, we send multiple flows using this level graph. This is the reason it works better than Edmond-Karp. In Edmond-Karp, we send only flow that is send across the path found by BFS. Implementation (Dinic with Scaling) \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 /******************************************************************************** MaxFlow Dinic algorithm with scaling. O(N * M * log(MC)), where MC is maximum edge capacity. Based on problem 2784 from informatics.mccme.ru http://informatics.mccme.ru/mod/statements/view3.php?chapterid=2784#1 ********************************************************************************/ #include <bits/stdc++.h> using namespace std ; struct edge { int a , b , f , c ; }; const int inf = 1000 * 1000 * 1000 ; const int MAXN = 1050 ; int n , m ; vector < edge > e ; int pt [ MAXN ]; // very important performance trick vector < int > g [ MAXN ]; long long flow = 0 ; queue < int > q ; int d [ MAXN ]; int lim ; int s , t ; void add_edge ( int a , int b , int c ) { edge ed ; //keep edges in vector: e[ind] - direct edge, e[ind ^ 1] - back edge ed . a = a ; ed . b = b ; ed . f = 0 ; ed . c = c ; g [ a ]. push_back ( e . size ()); e . push_back ( ed ); ed . a = b ; ed . b = a ; ed . f = c ; ed . c = c ; g [ b ]. push_back ( e . size ()); e . push_back ( ed ); } bool bfs () { for ( int i = s ; i <= t ; i ++ ) d [ i ] = inf ; d [ s ] = 0 ; q . push ( s ); while ( ! q . empty () && d [ t ] == inf ) { int cur = q . front (); q . pop (); for ( size_t i = 0 ; i < g [ cur ]. size (); i ++ ) { int id = g [ cur ][ i ]; int to = e [ id ]. b ; //printf(\"cur = %d id = %d a = %d b = %d f = %d c = %d\\n\", cur, id, e[id].a, e[id].b, e[id].f, e[id].c); if ( d [ to ] == inf && e [ id ]. c - e [ id ]. f >= lim ) { d [ to ] = d [ cur ] + 1 ; q . push ( to ); } } } while ( ! q . empty ()) q . pop (); return d [ t ] != inf ; } bool dfs ( int v , int flow ) { if ( flow == 0 ) return false ; if ( v == t ) { //cout << v << endl; return true ; } for (; pt [ v ] < g [ v ]. size (); pt [ v ] ++ ) { int id = g [ v ][ pt [ v ]]; int to = e [ id ]. b ; //printf(\"v = %d id = %d a = %d b = %d f = %d c = %d\\n\", v, id, e[id].a, e[id].b, e[id].f, e[id].c); if ( d [ to ] == d [ v ] + 1 && e [ id ]. c - e [ id ]. f >= flow ) { int pushed = dfs ( to , flow ); if ( pushed ) { e [ id ]. f += flow ; e [ id ^ 1 ]. f -= flow ; return true ; } } } return false ; } void dinic () { for ( lim = ( 1 << 30 ); lim >= 1 ;) { if ( ! bfs ()) { lim >>= 1 ; continue ; } for ( int i = s ; i <= t ; i ++ ) pt [ i ] = 0 ; int pushed ; while ( pushed = dfs ( s , lim )) { flow = flow + lim ; } //cout << flow << endl; } } int main () { //freopen(\"input.txt\",\"r\",stdin); //freopen(\"output.txt\",\"w\",stdout); scanf ( \"%d %d\" , & n , & m ); s = 1 ; t = n ; for ( int i = 1 ; i <= m ; i ++ ) { int a , b , c ; scanf ( \"%d %d %d\" , & a , & b , & c ); add_edge ( a , b , c ); } dinic (); cout << flow << endl ; return 0 ; } Problems \u00b6 \ucd5c\ub300 \uc720\ub7c9 Related topics \u00b6 Maximum flow Minimum cut","title":"Ford-Fulkerson"},{"location":"Algorithms/Graph/FordFulkerson/#ford-fulkerson_algorithm","text":"The Ford-Fulkerson algorithm finds the maximum flow in a graph. The algorithm begins with an empty flow, and at each step finds a path from the source to the sink that generates more flow. Finally, when the algorithm cannot increase the flow anymore, the maximum flow has been found. The algorithm uses a special representation of the graph where each original edge has a reverse edge in another direction. The weight of each edge indicates how much more flow we could route through it. At the beginning of the algorithm, the weight of each original edge equals the capacity of the edge and the weight of each reverse edge is zero.","title":"Ford-Fulkerson algorithm"},{"location":"Algorithms/Graph/FordFulkerson/#algorithm_description","text":"The Ford-Fulkerson algorithm consists of several rounds. On each round, the algorithm finds a path from the source to the sink such that each edge on the path has a positive weight. If there is more than one possible path available, we can choose any of them. After choosing the path, the flow increases by $x$ units, where $x$ is the smallest edge weight on the path. In addition, the weight of each edge on the path decreases by $x$ and the weight of each reverse edge increases by $x$. The idea is that increasing the flow decreases the amount of flow that can go through the edges in the future. On the other hand, it is possible to cancel flow later using the reverse edges of the graph if it turns out that it would be beneficial to route the flow in another way. The algorithm increases the flow as long as there is a path from the source to the sink through positive-weight edges. If It is not possible to increase the flow anymore, because there is no path from the source to the sink with positive edge weights, the algorithm terminates and has found the maximum flow.","title":"Algorithm description"},{"location":"Algorithms/Graph/FordFulkerson/#finding_paths","text":"The Ford-Fulkerson algorithm does not specify how we should choose the paths that increase the flow. In any case, the algorithm will terminate sooner or later and correctly find the maximum flow. However, the efficiency of the algorithm depends on the way the paths are chosen.","title":"Finding paths"},{"location":"Algorithms/Graph/FordFulkerson/#path_finding_techniques","text":"In practive, the scaling algorithm is easier to implement , because depth-first search can be used for finding paths. Both Algorithms are efficient enough for problems that typically appear in programming contests.","title":"Path finding techniques"},{"location":"Algorithms/Graph/FordFulkerson/#edmond-karp_algorithm","text":"The algorithm chooses each path so that the number of edges on the path is as small as possible. This can be done by using $\\text{breadth-first search}$ instead of depth-first search for finding paths. It can be proven that this guarantees that the flow increases quickly, and the time complexity of the algorithm is $\\Omicron(m^2n)$.","title":"Edmond-Karp algorithm"},{"location":"Algorithms/Graph/FordFulkerson/#scaling_algorithm","text":"The algorithm uses depth-first search to find paths where each weight is at least a threshold value. Initially, the threshold value is some large number, for example the sum of all edge weights of the graph. Always when a path cannot be found, the threshold value is divided by 2. The time complexity of the algorithm is $\\Omicron(m^2lgc)$ , where $c$ is the initial threshold value.","title":"Scaling algorithm"},{"location":"Algorithms/Graph/FordFulkerson/#dinic_algorithm","text":"Time complex $\\Omicron(EV^2)$ Like Edmond-Karp algorithm, Dinic's algorithm uses following concepts A flow is maximum if there is no $source$ to $sink$ path in residual graph. $\\text{BFS}$ is used in a loop. There is difference though in the way we use BFS in both algorithms. In Edmond's Karp algorithm, we use $\\text{BFS}$ to find a augmenting path and send flow accress this path. In Dinic's algorithm we use $\\text{BFS}$ to check if more flow is possible and to construct level graph. In, level graph, we assign levels to all nodes, level of a node is shortest distance (in terms of number of edges) of the node from source. Once level graph is constructed, we send multiple flows using this level graph. This is the reason it works better than Edmond-Karp. In Edmond-Karp, we send only flow that is send across the path found by BFS.","title":"Dinic Algorithm"},{"location":"Algorithms/Graph/FordFulkerson/#implementation_dinic_with_scaling","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 /******************************************************************************** MaxFlow Dinic algorithm with scaling. O(N * M * log(MC)), where MC is maximum edge capacity. Based on problem 2784 from informatics.mccme.ru http://informatics.mccme.ru/mod/statements/view3.php?chapterid=2784#1 ********************************************************************************/ #include <bits/stdc++.h> using namespace std ; struct edge { int a , b , f , c ; }; const int inf = 1000 * 1000 * 1000 ; const int MAXN = 1050 ; int n , m ; vector < edge > e ; int pt [ MAXN ]; // very important performance trick vector < int > g [ MAXN ]; long long flow = 0 ; queue < int > q ; int d [ MAXN ]; int lim ; int s , t ; void add_edge ( int a , int b , int c ) { edge ed ; //keep edges in vector: e[ind] - direct edge, e[ind ^ 1] - back edge ed . a = a ; ed . b = b ; ed . f = 0 ; ed . c = c ; g [ a ]. push_back ( e . size ()); e . push_back ( ed ); ed . a = b ; ed . b = a ; ed . f = c ; ed . c = c ; g [ b ]. push_back ( e . size ()); e . push_back ( ed ); } bool bfs () { for ( int i = s ; i <= t ; i ++ ) d [ i ] = inf ; d [ s ] = 0 ; q . push ( s ); while ( ! q . empty () && d [ t ] == inf ) { int cur = q . front (); q . pop (); for ( size_t i = 0 ; i < g [ cur ]. size (); i ++ ) { int id = g [ cur ][ i ]; int to = e [ id ]. b ; //printf(\"cur = %d id = %d a = %d b = %d f = %d c = %d\\n\", cur, id, e[id].a, e[id].b, e[id].f, e[id].c); if ( d [ to ] == inf && e [ id ]. c - e [ id ]. f >= lim ) { d [ to ] = d [ cur ] + 1 ; q . push ( to ); } } } while ( ! q . empty ()) q . pop (); return d [ t ] != inf ; } bool dfs ( int v , int flow ) { if ( flow == 0 ) return false ; if ( v == t ) { //cout << v << endl; return true ; } for (; pt [ v ] < g [ v ]. size (); pt [ v ] ++ ) { int id = g [ v ][ pt [ v ]]; int to = e [ id ]. b ; //printf(\"v = %d id = %d a = %d b = %d f = %d c = %d\\n\", v, id, e[id].a, e[id].b, e[id].f, e[id].c); if ( d [ to ] == d [ v ] + 1 && e [ id ]. c - e [ id ]. f >= flow ) { int pushed = dfs ( to , flow ); if ( pushed ) { e [ id ]. f += flow ; e [ id ^ 1 ]. f -= flow ; return true ; } } } return false ; } void dinic () { for ( lim = ( 1 << 30 ); lim >= 1 ;) { if ( ! bfs ()) { lim >>= 1 ; continue ; } for ( int i = s ; i <= t ; i ++ ) pt [ i ] = 0 ; int pushed ; while ( pushed = dfs ( s , lim )) { flow = flow + lim ; } //cout << flow << endl; } } int main () { //freopen(\"input.txt\",\"r\",stdin); //freopen(\"output.txt\",\"w\",stdout); scanf ( \"%d %d\" , & n , & m ); s = 1 ; t = n ; for ( int i = 1 ; i <= m ; i ++ ) { int a , b , c ; scanf ( \"%d %d %d\" , & a , & b , & c ); add_edge ( a , b , c ); } dinic (); cout << flow << endl ; return 0 ; }","title":"Implementation (Dinic with Scaling)"},{"location":"Algorithms/Graph/FordFulkerson/#problems","text":"\ucd5c\ub300 \uc720\ub7c9","title":"Problems"},{"location":"Algorithms/Graph/FordFulkerson/#related_topics","text":"Maximum flow Minimum cut","title":"Related topics"},{"location":"Algorithms/Graph/GraphTraversal/","text":"Graph Traversal \u00b6 We will cover two fundamental graph algorithms. depth-first search & breadth-first search. BFS vs DFS Both algorithms are given starting node in the graph and they visit all nodes that can be reached from the starting node. The difference in algorithms is the order in which they visit the nodes. Depth-first search($\\text{DFS}$) \u00b6 Depth-first search always follows a single path in the graph as long as it find new nodes. After this, it returns to previous nodes and begin to explore other parts of the graph. The algorithm keeps track of visited nodes, so that it processes each node only once Implementation \u00b6 Using adjacency lists in an array maintain an array visited[N] 1 2 3 4 5 6 7 8 9 10 11 vector < int > adj [ N ]; //adjacency lists bool visited [ N ]; void dfs ( int s ) { //starting node s if ( visited [ s ]) return ; visited [ s ] = true ; // process node for ( auto u : adj [ s ]) { dfs ( u ); } } Breadth-first search($\\text{BFS}$) \u00b6 Breadth-first search visits the nodes in increasing order of their distance from the starting node. Thus, we can calculate the distance from the starting node to all other nodes using $BFS$. Implementation \u00b6 Typical implementation is based on a queue that contains nodes. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 queue < int > q ; bool visited [ N ]; int distance [ N ]; visited [ s ] = true ; // starting node s distance [ s ] = 0 ; q . push ( s ); while ( ! q . empty ()) { int cur = q . front (); q . pop (); // process node for ( auto next : adj [ cur ]) { if ( visited [ next ]) continue ; visited [ next ] = true ; distance [ next ] = distance [ cur ] + 1 ; q . push ( next ); } } Applications \u00b6 You can use any of both to check properties of graph but in practice, depth-first search is a better choice, because of ease of implementation 1. Connectivity check \u00b6 A graph is connected if there is a path between any two nodes of the graph Implementation \u00b6 Connected Graph: If a search did not visit all the nodes, we can conclude that the graph is not connected Find all components of Graph: iterating through the nodes and always starting a new depth-first search if the current node does not belong to any component yet 2. Finding cycles \u00b6 Implementation \u00b6 Way1: A graph contains a cycle if during a graph traversal, we find a node whose neighbor (other than the previous node in the current path) has already been visited Way2(math): if a component contains c nodes and no cycle, it must contain exactly c-1 edges. if there are c or more edges, the component surely contains a cycle 3. Bipartiteness check \u00b6 A graph is bipartite if its nodes can be colored using two colors so that there are no adjacent nodes with the same color Implementation \u00b6 The idea is to color the starting node blue, all its neighbors red, all their neighbors blue, and so on. If at some point of the search we notice that two adjacent nodes have the same color, this means that the graph is not bipartite. Otherwise, the graph is bipartite. Why it works? This algorithm always works, because when there are only two colors available, the color of the starting node in a component determines the colors of all other nodes in the component NP-hard Note that in the general case, it is difficult to find out if the nodes in a graph can be colored using $k$ colors so that no adjacent nodes have the same color. Even when $k=3$, no efficient algorithm is known but the problem is NP-hard","title":"Graph Traversal"},{"location":"Algorithms/Graph/GraphTraversal/#graph_traversal","text":"We will cover two fundamental graph algorithms. depth-first search & breadth-first search. BFS vs DFS Both algorithms are given starting node in the graph and they visit all nodes that can be reached from the starting node. The difference in algorithms is the order in which they visit the nodes.","title":"Graph Traversal"},{"location":"Algorithms/Graph/GraphTraversal/#depth-first_searchtextdfs","text":"Depth-first search always follows a single path in the graph as long as it find new nodes. After this, it returns to previous nodes and begin to explore other parts of the graph. The algorithm keeps track of visited nodes, so that it processes each node only once","title":"Depth-first search($\\text{DFS}$)"},{"location":"Algorithms/Graph/GraphTraversal/#implementation","text":"Using adjacency lists in an array maintain an array visited[N] 1 2 3 4 5 6 7 8 9 10 11 vector < int > adj [ N ]; //adjacency lists bool visited [ N ]; void dfs ( int s ) { //starting node s if ( visited [ s ]) return ; visited [ s ] = true ; // process node for ( auto u : adj [ s ]) { dfs ( u ); } }","title":"Implementation"},{"location":"Algorithms/Graph/GraphTraversal/#breadth-first_searchtextbfs","text":"Breadth-first search visits the nodes in increasing order of their distance from the starting node. Thus, we can calculate the distance from the starting node to all other nodes using $BFS$.","title":"Breadth-first search($\\text{BFS}$)"},{"location":"Algorithms/Graph/GraphTraversal/#implementation_1","text":"Typical implementation is based on a queue that contains nodes. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 queue < int > q ; bool visited [ N ]; int distance [ N ]; visited [ s ] = true ; // starting node s distance [ s ] = 0 ; q . push ( s ); while ( ! q . empty ()) { int cur = q . front (); q . pop (); // process node for ( auto next : adj [ cur ]) { if ( visited [ next ]) continue ; visited [ next ] = true ; distance [ next ] = distance [ cur ] + 1 ; q . push ( next ); } }","title":"Implementation"},{"location":"Algorithms/Graph/GraphTraversal/#applications","text":"You can use any of both to check properties of graph but in practice, depth-first search is a better choice, because of ease of implementation","title":"Applications"},{"location":"Algorithms/Graph/GraphTraversal/#1_connectivity_check","text":"A graph is connected if there is a path between any two nodes of the graph","title":"1. Connectivity check"},{"location":"Algorithms/Graph/GraphTraversal/#implementation_2","text":"Connected Graph: If a search did not visit all the nodes, we can conclude that the graph is not connected Find all components of Graph: iterating through the nodes and always starting a new depth-first search if the current node does not belong to any component yet","title":"Implementation"},{"location":"Algorithms/Graph/GraphTraversal/#2_finding_cycles","text":"","title":"2. Finding cycles"},{"location":"Algorithms/Graph/GraphTraversal/#implementation_3","text":"Way1: A graph contains a cycle if during a graph traversal, we find a node whose neighbor (other than the previous node in the current path) has already been visited Way2(math): if a component contains c nodes and no cycle, it must contain exactly c-1 edges. if there are c or more edges, the component surely contains a cycle","title":"Implementation"},{"location":"Algorithms/Graph/GraphTraversal/#3_bipartiteness_check","text":"A graph is bipartite if its nodes can be colored using two colors so that there are no adjacent nodes with the same color","title":"3. Bipartiteness check"},{"location":"Algorithms/Graph/GraphTraversal/#implementation_4","text":"The idea is to color the starting node blue, all its neighbors red, all their neighbors blue, and so on. If at some point of the search we notice that two adjacent nodes have the same color, this means that the graph is not bipartite. Otherwise, the graph is bipartite. Why it works? This algorithm always works, because when there are only two colors available, the color of the starting node in a component determines the colors of all other nodes in the component NP-hard Note that in the general case, it is difficult to find out if the nodes in a graph can be colored using $k$ colors so that no adjacent nodes have the same color. Even when $k=3$, no efficient algorithm is known but the problem is NP-hard","title":"Implementation"},{"location":"Algorithms/Graph/HamiltonianPaths/","text":"Hamiltonian Paths \u00b6 A Hamiltonian path is a path that visits each node of the graph exactly once. If a Hamiltonian path begins and ends at the same node, it is called Hamiltonian circuit. Existence \u00b6 No efficient method is known for testing if a graph contains Hamiltonian path, and the problem is NP-hard Still, in some special cases, we can be certain that a graph contains a Hamiltonian path. A simple observation is that if the graph is complete. It also contains a Hamiltonian path. Dirac's theorem: If a degree of each node is at least $n/2$, the graph contains a Hamiltonian path Ore's theorem: If the sum of degrees of each non-adjacent pair of nodes is at least $n$, the graph contains a Hamiltonian path. A common property in these theorems and other results is that they guarantee the existence of a Hamiltonian path if the graph has a large number of edges. Construction \u00b6 Since there is no efficient way to check if a Hamiltonian path exists, it is clear that there is also no method to efficiently construct the path, because otherwise we could just try to construct the path and see whether it exists. A simple way to search for a Hamiltonian path is use backtracking algorithm that goes through all possible ways to construct the path. The time complexity of such an algorithm is at least $\\Omicron(n!)$, because there are $n!$ different ways to choose the order of $n$ nodes. A more efficient solution is based on dynamic programming. The idea is to calculate values of a function possible(S, x) , where $S$ is a subset of nodes and $x$ is one of the nodes. The function indicates whether there is a hamiltonian path that visits the nodes of $S$ and ends at node $x$. It is possible to implement this solution in $\\Omicron(2^nn^2)$ time. Implementation \u00b6 Related subjects \u00b6 Kinght's tours","title":"Hamiltonian Paths"},{"location":"Algorithms/Graph/HamiltonianPaths/#hamiltonian_paths","text":"A Hamiltonian path is a path that visits each node of the graph exactly once. If a Hamiltonian path begins and ends at the same node, it is called Hamiltonian circuit.","title":"Hamiltonian Paths"},{"location":"Algorithms/Graph/HamiltonianPaths/#existence","text":"No efficient method is known for testing if a graph contains Hamiltonian path, and the problem is NP-hard Still, in some special cases, we can be certain that a graph contains a Hamiltonian path. A simple observation is that if the graph is complete. It also contains a Hamiltonian path. Dirac's theorem: If a degree of each node is at least $n/2$, the graph contains a Hamiltonian path Ore's theorem: If the sum of degrees of each non-adjacent pair of nodes is at least $n$, the graph contains a Hamiltonian path. A common property in these theorems and other results is that they guarantee the existence of a Hamiltonian path if the graph has a large number of edges.","title":"Existence"},{"location":"Algorithms/Graph/HamiltonianPaths/#construction","text":"Since there is no efficient way to check if a Hamiltonian path exists, it is clear that there is also no method to efficiently construct the path, because otherwise we could just try to construct the path and see whether it exists. A simple way to search for a Hamiltonian path is use backtracking algorithm that goes through all possible ways to construct the path. The time complexity of such an algorithm is at least $\\Omicron(n!)$, because there are $n!$ different ways to choose the order of $n$ nodes. A more efficient solution is based on dynamic programming. The idea is to calculate values of a function possible(S, x) , where $S$ is a subset of nodes and $x$ is one of the nodes. The function indicates whether there is a hamiltonian path that visits the nodes of $S$ and ends at node $x$. It is possible to implement this solution in $\\Omicron(2^nn^2)$ time.","title":"Construction"},{"location":"Algorithms/Graph/HamiltonianPaths/#implementation","text":"","title":"Implementation"},{"location":"Algorithms/Graph/HamiltonianPaths/#related_subjects","text":"Kinght's tours","title":"Related subjects"},{"location":"Algorithms/Graph/KnightsTours/","text":"Knight's tours \u00b6 A knight's tour is sequence of moves of a knight on an $n \\times n$ chess board following the rules of chess such that the knight visits each square exactly once. A knight's tour is closed if the knight finally returns to the starting square and otherwise it is called open tour. A knight's tour corresponds to a Hamiltonian path in a graph whose nodes represent the suqare of the board, and two nodes are connected with an edge if a knight can move between the squares according to the rules of chess. A natural way to construct a knight's tour is to use backtracking. The search can be made more fficient by using heuristics that attempt to guide the knight so that a complete tour will be found quickly Polynomial algorithms There are also polynomial algorithms for finding knight's tours, but they are more complicated. Warnsdorf's rule \u00b6 Warnsdorf's rule is a simple and effective heruistic for finding a knight's tour. Using the rule, it is possible to efficiently construct a tour even on a large board . The idea is to always move the knight so that it ends up in a square where the number of possible moves is as small as possible. Related Topic \u00b6 [Hamiltonian Paths] Related Problems \u00b6 \ub098\uc774\ud2b8 \ud22c\uc5b4 \ub098\uc774\ud2b8 \ud22c\uc5b4","title":"Knight's tours"},{"location":"Algorithms/Graph/KnightsTours/#knights_tours","text":"A knight's tour is sequence of moves of a knight on an $n \\times n$ chess board following the rules of chess such that the knight visits each square exactly once. A knight's tour is closed if the knight finally returns to the starting square and otherwise it is called open tour. A knight's tour corresponds to a Hamiltonian path in a graph whose nodes represent the suqare of the board, and two nodes are connected with an edge if a knight can move between the squares according to the rules of chess. A natural way to construct a knight's tour is to use backtracking. The search can be made more fficient by using heuristics that attempt to guide the knight so that a complete tour will be found quickly Polynomial algorithms There are also polynomial algorithms for finding knight's tours, but they are more complicated.","title":"Knight's tours"},{"location":"Algorithms/Graph/KnightsTours/#warnsdorfs_rule","text":"Warnsdorf's rule is a simple and effective heruistic for finding a knight's tour. Using the rule, it is possible to efficiently construct a tour even on a large board . The idea is to always move the knight so that it ends up in a square where the number of possible moves is as small as possible.","title":"Warnsdorf's rule"},{"location":"Algorithms/Graph/KnightsTours/#related_topic","text":"[Hamiltonian Paths]","title":"Related Topic"},{"location":"Algorithms/Graph/KnightsTours/#related_problems","text":"\ub098\uc774\ud2b8 \ud22c\uc5b4 \ub098\uc774\ud2b8 \ud22c\uc5b4","title":"Related Problems"},{"location":"Algorithms/Graph/KonigsTheorem/","text":"Konig's theorem \u00b6 Minimum node cover \u00b6 A minimum node cover of a graph is a minimum set of nodes such that each edge of the graph has at least one end point in the set. In general graph, finding a minimum node cover is a NP-hard problem. However, if the graph is bipartite, Konig's theorem tells us that the size of minimum node cover and the size of a maximum matching are always equal. Thus, we can calculate the size of a minimum node cover using a maximum flow algorithm. Maximum independent set \u00b6 The nodes that do not belong to a minimum node cover form a maximum independent set 1 . Finding a maximum independent set in a general graph is NP-hard problem, but in a bipartite graph we can use Konig's theorem to solve the problem efficiently. The largest possible set of nodes such that no two nodes in the set are connected with an edge. \u21a9","title":"Konig's Theorem"},{"location":"Algorithms/Graph/KonigsTheorem/#konigs_theorem","text":"","title":"Konig's theorem"},{"location":"Algorithms/Graph/KonigsTheorem/#minimum_node_cover","text":"A minimum node cover of a graph is a minimum set of nodes such that each edge of the graph has at least one end point in the set. In general graph, finding a minimum node cover is a NP-hard problem. However, if the graph is bipartite, Konig's theorem tells us that the size of minimum node cover and the size of a maximum matching are always equal. Thus, we can calculate the size of a minimum node cover using a maximum flow algorithm.","title":"Minimum node cover"},{"location":"Algorithms/Graph/KonigsTheorem/#maximum_independent_set","text":"The nodes that do not belong to a minimum node cover form a maximum independent set 1 . Finding a maximum independent set in a general graph is NP-hard problem, but in a bipartite graph we can use Konig's theorem to solve the problem efficiently. The largest possible set of nodes such that no two nodes in the set are connected with an edge. \u21a9","title":"Maximum independent set"},{"location":"Algorithms/Graph/MaximumFlow/","text":"Maximum Flow \u00b6 Finding a maximum flow: What is the maximum amount of flow we can send from a node to another node? The input for problem is directed, weighted graph that contains two special nodes: the $\\text{source}$ is a node with no incoming edges, and the $\\text{sink}$ is a node with no outgoing edges. in the maximum flow problem, our task is to send as much flow as possible from the source to the sink. The weight of each edge is a capacity that restricts the flow that can go through the edge. In each intermediate node, the incoming and outgoing flow has to be equal. Problems \u00b6 \ucd5c\ub300 \uc720\ub7c9 Related topiccs \u00b6 Ford-Fulkerson algorithm","title":"Maximum flow"},{"location":"Algorithms/Graph/MaximumFlow/#maximum_flow","text":"Finding a maximum flow: What is the maximum amount of flow we can send from a node to another node? The input for problem is directed, weighted graph that contains two special nodes: the $\\text{source}$ is a node with no incoming edges, and the $\\text{sink}$ is a node with no outgoing edges. in the maximum flow problem, our task is to send as much flow as possible from the source to the sink. The weight of each edge is a capacity that restricts the flow that can go through the edge. In each intermediate node, the incoming and outgoing flow has to be equal.","title":"Maximum Flow"},{"location":"Algorithms/Graph/MaximumFlow/#problems","text":"\ucd5c\ub300 \uc720\ub7c9","title":"Problems"},{"location":"Algorithms/Graph/MaximumFlow/#related_topiccs","text":"Ford-Fulkerson algorithm","title":"Related topiccs"},{"location":"Algorithms/Graph/MaximumMatchings/","text":"Maximum matchings \u00b6 The maximum matching problem asks to find a maximum-size of set of node pairs in an undirected graph such that each pair is connected with an edge and each node belongs to at most one pair. There are polynomial algorithms for finding maximum matchings in general graphs, but such algorithms are complex and rarely seen in programming contest However, in bipartite graphs , the maximum matching problem is much easier to solve, because we can reduce it to the maximum flow problem . Finding maximum matchings \u00b6 The node of a bipartite graph can be always divided into two groups such that all edges of the graph go from the left group to the right group We can reduce the bipartite maximum matching problem to the maximum flow problem by adding two new nodes to the graph: a source and a sink. We also add edges from the source to each left node and from each right node to the sink After this, the size of a maximum flow in the graph equals the size of a maximum matching in the original graph. Implementation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 // A C++ program to find maximal // Bipartite matching. #include <iostream> #include <string.h> using namespace std ; // M is number of applicants // and N is number of jobs #define M 6 #define N 6 // A DFS based recursive function // that returns true if a matching // for vertex u is possible bool bpm ( bool bpGraph [ M ][ N ], int u , bool seen [], int matchR []) { // Try every job one by one for ( int v = 0 ; v < N ; v ++ ) { // If applicant u is interested in // job v and v is not visited if ( bpGraph [ u ][ v ] && ! seen [ v ]) { // Mark v as visited seen [ v ] = true ; // If job 'v' is not assigned to an // applicant OR previously assigned // applicant for job v (which is matchR[v]) // has an alternate job available. // Since v is marked as visited in // the above line, matchR[v] in the following // recursive call will not get job 'v' again if ( matchR [ v ] < 0 || bpm ( bpGraph , matchR [ v ], seen , matchR )) { matchR [ v ] = u ; return true ; } } } return false ; } // Returns maximum number // of matching from M to N int maxBPM ( bool bpGraph [ M ][ N ]) { // An array to keep track of the // applicants assigned to jobs. // The value of matchR[i] is the // applicant number assigned to job i, // the value -1 indicates nobody is // assigned. int matchR [ N ]; // Initially all jobs are available memset ( matchR , - 1 , sizeof ( matchR )); // Count of jobs assigned to applicants int result = 0 ; for ( int u = 0 ; u < M ; u ++ ) { // Mark all jobs as not seen // for next applicant. bool seen [ N ]; memset ( seen , 0 , sizeof ( seen )); // Find if the applicant 'u' can get a job if ( bpm ( bpGraph , u , seen , matchR )) result ++ ; } return result ; } // Driver Code int main () { // Let us create a bpGraph // shown in the above example bool bpGraph [ M ][ N ] = {{ 0 , 1 , 1 , 0 , 0 , 0 }, { 1 , 0 , 0 , 1 , 0 , 0 }, { 0 , 0 , 1 , 0 , 0 , 0 }, { 0 , 0 , 1 , 1 , 0 , 0 }, { 0 , 0 , 0 , 0 , 0 , 0 }, { 0 , 0 , 0 , 0 , 0 , 1 }}; cout << \"Maximum number of applicants that can get job is \" << maxBPM ( bpGraph ); return 0 ; } Hall's theorem \u00b6 Hall's theorem can be used to find out whether a bipartite graph has a matching that contains all left or right nodes. If the number of left and right nodes is the same, Hall's theorem tells us if it is possible to construct a perfect matching that contains all nodes of the graph. Assume that we want to find a matching that contains all left nodes. Let $X$ be any set of left nodes and let $f(X)$ be the set of their neighbors. According to Hall's Theorem, a matching that contains all left nodes exists exactly when for each $X$, the condition $|X| \\leq |f(X)|$ holds. If the condition of Hall's theorem does not hold, the set $X$ provides an explanation why we cannot form such a matching. Since $X$ contains more nodes than $f(X)$, there are no pairs for all nodes in $X$. Problems \u00b6 \ucd95\uc0ac \ubc30\uc815 \uc5f4\ud608\uac15\ud638 \ub178\ud2b8\ubd81\uc758 \uc8fc\uc778\uc744 \ucc3e\uc544\uc11c","title":"Maximum Matchings"},{"location":"Algorithms/Graph/MaximumMatchings/#maximum_matchings","text":"The maximum matching problem asks to find a maximum-size of set of node pairs in an undirected graph such that each pair is connected with an edge and each node belongs to at most one pair. There are polynomial algorithms for finding maximum matchings in general graphs, but such algorithms are complex and rarely seen in programming contest However, in bipartite graphs , the maximum matching problem is much easier to solve, because we can reduce it to the maximum flow problem .","title":"Maximum matchings"},{"location":"Algorithms/Graph/MaximumMatchings/#finding_maximum_matchings","text":"The node of a bipartite graph can be always divided into two groups such that all edges of the graph go from the left group to the right group We can reduce the bipartite maximum matching problem to the maximum flow problem by adding two new nodes to the graph: a source and a sink. We also add edges from the source to each left node and from each right node to the sink After this, the size of a maximum flow in the graph equals the size of a maximum matching in the original graph.","title":"Finding maximum matchings"},{"location":"Algorithms/Graph/MaximumMatchings/#implementation","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 // A C++ program to find maximal // Bipartite matching. #include <iostream> #include <string.h> using namespace std ; // M is number of applicants // and N is number of jobs #define M 6 #define N 6 // A DFS based recursive function // that returns true if a matching // for vertex u is possible bool bpm ( bool bpGraph [ M ][ N ], int u , bool seen [], int matchR []) { // Try every job one by one for ( int v = 0 ; v < N ; v ++ ) { // If applicant u is interested in // job v and v is not visited if ( bpGraph [ u ][ v ] && ! seen [ v ]) { // Mark v as visited seen [ v ] = true ; // If job 'v' is not assigned to an // applicant OR previously assigned // applicant for job v (which is matchR[v]) // has an alternate job available. // Since v is marked as visited in // the above line, matchR[v] in the following // recursive call will not get job 'v' again if ( matchR [ v ] < 0 || bpm ( bpGraph , matchR [ v ], seen , matchR )) { matchR [ v ] = u ; return true ; } } } return false ; } // Returns maximum number // of matching from M to N int maxBPM ( bool bpGraph [ M ][ N ]) { // An array to keep track of the // applicants assigned to jobs. // The value of matchR[i] is the // applicant number assigned to job i, // the value -1 indicates nobody is // assigned. int matchR [ N ]; // Initially all jobs are available memset ( matchR , - 1 , sizeof ( matchR )); // Count of jobs assigned to applicants int result = 0 ; for ( int u = 0 ; u < M ; u ++ ) { // Mark all jobs as not seen // for next applicant. bool seen [ N ]; memset ( seen , 0 , sizeof ( seen )); // Find if the applicant 'u' can get a job if ( bpm ( bpGraph , u , seen , matchR )) result ++ ; } return result ; } // Driver Code int main () { // Let us create a bpGraph // shown in the above example bool bpGraph [ M ][ N ] = {{ 0 , 1 , 1 , 0 , 0 , 0 }, { 1 , 0 , 0 , 1 , 0 , 0 }, { 0 , 0 , 1 , 0 , 0 , 0 }, { 0 , 0 , 1 , 1 , 0 , 0 }, { 0 , 0 , 0 , 0 , 0 , 0 }, { 0 , 0 , 0 , 0 , 0 , 1 }}; cout << \"Maximum number of applicants that can get job is \" << maxBPM ( bpGraph ); return 0 ; }","title":"Implementation"},{"location":"Algorithms/Graph/MaximumMatchings/#halls_theorem","text":"Hall's theorem can be used to find out whether a bipartite graph has a matching that contains all left or right nodes. If the number of left and right nodes is the same, Hall's theorem tells us if it is possible to construct a perfect matching that contains all nodes of the graph. Assume that we want to find a matching that contains all left nodes. Let $X$ be any set of left nodes and let $f(X)$ be the set of their neighbors. According to Hall's Theorem, a matching that contains all left nodes exists exactly when for each $X$, the condition $|X| \\leq |f(X)|$ holds. If the condition of Hall's theorem does not hold, the set $X$ provides an explanation why we cannot form such a matching. Since $X$ contains more nodes than $f(X)$, there are no pairs for all nodes in $X$.","title":"Hall's theorem"},{"location":"Algorithms/Graph/MaximumMatchings/#problems","text":"\ucd95\uc0ac \ubc30\uc815 \uc5f4\ud608\uac15\ud638 \ub178\ud2b8\ubd81\uc758 \uc8fc\uc778\uc744 \ucc3e\uc544\uc11c","title":"Problems"},{"location":"Algorithms/Graph/MinimumCut/","text":"Minimum cut \u00b6 In the minimum cut problem, our task is to remove a set of edges from the graph such that there will be no path from the source to the sink after the removal and the total weight of the removed edges is minimum. It turns out that a maximum flow and a minimum cut are always equally large, so the concepts are two sides of the same coin. Ford-Fulkerson algorithm \u00b6 It turns out that Ford-Fulkerson algorithm has found a maximum flow, it has also determined a minimum cut. Let $A$ be the set of nodes that can be reached from the source using positive-weight edges. Now the minimum cut consists of the edges of the original graph that start at some node in $A$, end at some node outside $A$, and whose capacity is fully used in the maximum flow. Why this works? \u00b6 Why is the flow produced by the algorithm maximum and why is the cut minimum? The Reason is that a graph cannot contain a flow whose size is larger than the weight of any cut of the graph. Hence, always when a flow and a cut are equally large, they are a maximum flow and a minimum cut. Related topics \u00b6 Maximum flow Ford-Fulkerson algorithm","title":"Minimum cut"},{"location":"Algorithms/Graph/MinimumCut/#minimum_cut","text":"In the minimum cut problem, our task is to remove a set of edges from the graph such that there will be no path from the source to the sink after the removal and the total weight of the removed edges is minimum. It turns out that a maximum flow and a minimum cut are always equally large, so the concepts are two sides of the same coin.","title":"Minimum cut"},{"location":"Algorithms/Graph/MinimumCut/#ford-fulkerson_algorithm","text":"It turns out that Ford-Fulkerson algorithm has found a maximum flow, it has also determined a minimum cut. Let $A$ be the set of nodes that can be reached from the source using positive-weight edges. Now the minimum cut consists of the edges of the original graph that start at some node in $A$, end at some node outside $A$, and whose capacity is fully used in the maximum flow.","title":"Ford-Fulkerson algorithm"},{"location":"Algorithms/Graph/MinimumCut/#why_this_works","text":"Why is the flow produced by the algorithm maximum and why is the cut minimum? The Reason is that a graph cannot contain a flow whose size is larger than the weight of any cut of the graph. Hence, always when a flow and a cut are equally large, they are a maximum flow and a minimum cut.","title":"Why this works?"},{"location":"Algorithms/Graph/MinimumCut/#related_topics","text":"Maximum flow Ford-Fulkerson algorithm","title":"Related topics"},{"location":"Algorithms/Graph/PathCovers/","text":"Path Covers \u00b6 A path cover is a set of paths in a graph such that each node of the graph belongs to at least one path. It turns out that in directed, acyclic graphs ($\\text{DAGs}$), we can reduce the problem of finding a minimum path cover to the problem of finding a maximum flow in another graph. Node-disjoint path cover \u00b6 In a node-disjoint path cover, each node belongs to exactly one path. It is possible that a path does not contains any edges. (can have single node) We can find a minimum node-disjoint path cover by constructing a [matching graph] where each node of the original graph is represented by two nodes : A left node and a right node. There is an edge from a left node to a right node if there is such an edge in the original graph. In addition, the matching graph contains a source and a sink, and there are edges from the source to all left nodes and from all right nodes to the sink. [A Maximum matching] in the resulting graph corresponds to a minimum node-disjoint path cover in the original graph. Each edge in the maximum matching of the matching graph corresponds to an edge in the minimum node-disjoint path cover of the original graph. Thus, the size of the minimum node-disjoint cover is $n-c$ , where $n$ is the number of nodes in the original graph and $c$ is the size of the maximum matching. General path cover \u00b6 A general path cover is a path cover where a node can belong to more than one path. A minimum general path cover may be smaller than a minimum node-disjoint path cover, because a node can be used multiple times in paths. A minimum general path cover can be found almost like a minimum node disjoint path cover. It suffices to add some new edges to the matching graph so that there is an edge $a \\rarr b$ always when there is a path from $a$ to $b$ in the original graph(possibly through several edges). Dilworth's Theorem \u00b6 An antichain is a set of nodes of a graph such that there is no path from any node to another node using the edges of the graph. Dilworth's Theorem states that in a directed acyclic graph($\\text{DAGs}$), the size of a minimum general path cover equals the size of maximum antichain.","title":"Path Covers"},{"location":"Algorithms/Graph/PathCovers/#path_covers","text":"A path cover is a set of paths in a graph such that each node of the graph belongs to at least one path. It turns out that in directed, acyclic graphs ($\\text{DAGs}$), we can reduce the problem of finding a minimum path cover to the problem of finding a maximum flow in another graph.","title":"Path Covers"},{"location":"Algorithms/Graph/PathCovers/#node-disjoint_path_cover","text":"In a node-disjoint path cover, each node belongs to exactly one path. It is possible that a path does not contains any edges. (can have single node) We can find a minimum node-disjoint path cover by constructing a [matching graph] where each node of the original graph is represented by two nodes : A left node and a right node. There is an edge from a left node to a right node if there is such an edge in the original graph. In addition, the matching graph contains a source and a sink, and there are edges from the source to all left nodes and from all right nodes to the sink. [A Maximum matching] in the resulting graph corresponds to a minimum node-disjoint path cover in the original graph. Each edge in the maximum matching of the matching graph corresponds to an edge in the minimum node-disjoint path cover of the original graph. Thus, the size of the minimum node-disjoint cover is $n-c$ , where $n$ is the number of nodes in the original graph and $c$ is the size of the maximum matching.","title":"Node-disjoint path cover"},{"location":"Algorithms/Graph/PathCovers/#general_path_cover","text":"A general path cover is a path cover where a node can belong to more than one path. A minimum general path cover may be smaller than a minimum node-disjoint path cover, because a node can be used multiple times in paths. A minimum general path cover can be found almost like a minimum node disjoint path cover. It suffices to add some new edges to the matching graph so that there is an edge $a \\rarr b$ always when there is a path from $a$ to $b$ in the original graph(possibly through several edges).","title":"General path cover"},{"location":"Algorithms/Graph/PathCovers/#dilworths_theorem","text":"An antichain is a set of nodes of a graph such that there is no path from any node to another node using the edges of the graph. Dilworth's Theorem states that in a directed acyclic graph($\\text{DAGs}$), the size of a minimum general path cover equals the size of maximum antichain.","title":"Dilworth's Theorem"},{"location":"Algorithms/Graph/StrongConnectivity/","text":"Strong Connectivity(DAGs) \u00b6 A graph is strongly connected if there is a path from any node to all other nodes in the graph The strongly connected components of a graph divide the graph into strongly connected parts that are as large as possible. The strongly connected components form an acyclic component graph that represents deep structure of original graph. A component graph is a cyclic, directed graph, so it is easior to process than the original graph. Since the graph does not contain cycles, we can always construct a topological sort and use dynamic programming techniques. Kosaraju's algorithm \u00b6 Kosaraju's algorithm is an efficient method for finding the strongly connected components of a directed graph. The algorithm performs two depth-first search the first search constructs a list of nodes according to the structure of the graph the second search forms the strongly connected components. 1. Search 1 \u00b6 The first phase of Kosaraju's algorithm constructs a list of nodes in the order in which a depth-first search processes them. The algorithm goes through the nodes, and begins a depth-first search at each unprocessed node. Each node will be added to the list after it has been processed. 2. Search 2 \u00b6 The second phase of the algorithm forms the strongly connected components of the graph. First, the algorithm reverses every edge in the graph. This guarantees that during second search, we will always find strongly connected components that do not have extra nodes. After this, the algorithm goes through the list of nodes created by the first search, in reverse order. if a node does not belong to a component, the algorithm creates a new component and starts a depth-first search that adds all new nodes found durin g the search to the new component leak Note that since all edges are reversed, the component does not leak \"leak\" to other pars in the graph. Implementation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 vector < int > adj [ V ]; //adjancecy lists vector < int > radj [ V ]; // reversed adjacency list (directions are opposite to original) vector < int > order ; vector < vector < int >> result ; bool processed [ V ]; void search1 ( int u ) { if ( processed [ u ]) return ; processed [ u ] = true ; for ( auto v : adj [ u ]) search1 ( v ); order . push_back ( u ); } void search2 ( int u ) { if ( processed [ u ]) return ; processed [ u ] = true ; for ( auto v : radj [ u ]) search2 ( v ); result [ result . size () - 1 ]. push_back ( u ); //add current node to the last component } int main () { for ( int i = 0 ; i < V ; i ++ ) { // V = number of nodes if ( ! processed [ i ]) search1 ( i ); } reverse ( order . begin (), order . end ()) // reverse the order that created by the first search for ( int i = 0 ; i < V ; i ++ ) // initialized processed array processed [ i ] = false ; for ( auto u : order ) if ( ! processed [ u ]) result . push_back ( vector < int > ()); // add new component search2 ( i ); } Time complexity \u00b6 $\\Omicron(n+m)$ because the algorithm performs two depth-first searches. 2SAT problem \u00b6 Strong connectivity is also linked with the 2SAT problem . In this problem, we are given a logical formula for example $$ L_1 = (x_2 \\lor \\neg x_1) \\land (\\neg x_1 \\lor \\neg x_2) \\land (x_1 \\lor x_3) \\land (\\neg x_2 \\lor \\neg x_3) \\land (x_1 \\lor x_4) $$ is true when the variables are assigned as follows $$ \\begin{aligned} x_1 & = false \\\\ x_2 & = false \\\\ x_3 & = true \\\\ x_4 & = true \\\\ \\end{aligned} $$ The 2SAT problem can be represented as a graph whose nodes correspond to variable $x_i$ and negations $\\neg x_i$, and edges determine the connections between the variables. Each pair $(a_i \\lor b_i)$ generates two edges: $\\neg a_i \\rarr b_i$ and $\\neg b_i \\rarr a_i$. This means that if $a_i$ does not hold, $b_i$ must hold, and vice versa. The structure of the graph tells us whether it is possible to assign the values of the variables so that the formula is true. It turns out that this can be done exactly when there are no nodes $x_i$ and $\\neg x_i$ such that both nodes belong to the same strongly connected component. If there are such nodes, the graph contains a path from $x_i$ to $\\neg x_i$ and also a path from $\\neg x_i$ to $x_i$, so both $x_i$ and $\\neg x_i$ should be true which is not possible. In the graph of the formula $L_1$ there are no nodes $x_i$ and $\\neg x_i$ such that both nodes belong to the same strongly connected component, so solution exists. If a solution exists, the values for the variables can be found by going through the nodes of the component graph in a reverse topological sort order. at each step, we process a component that does not contain edges that lead to an unprocessed component. If the variables in the component have not been assigned values, their values will be determined accoring to the values in the component, and if they already have values, they remain unchanged. The process continues until each variable has been assigned a value. this method works because the graph has a special structure: if there are paths from node $x_i$ to node $x_j$ and from node $x_j$ to node $\\neg x_j$, then node $x_i$ never becomes true. The reason for this is that there is also a path from node $\\neg x_j$ to node $\\neg x_j$, and both $x_i$ and $x_j$ become false. Info A more difficult problem is 3SAT problem , where each part of the formula is form $(a_i \\lor b_i \\lor c_i)$. This problem is NP-hard, so no efficient algorithm for solving the problem is known. Implementation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 int const MAXN ; // maximum number of variables int N ; // number of variables int M ; // number of conditions vector < vector < int >> adj ; // adjancecy list vector < vector < int >> radj ; //inver adjancency list vector < bool > processed ( 2 * MAXN ); vector < bool > rprocessed ( 2 * MAXN ); vector < int > sccnum ( 2 * MAXN , - 1 ); // initialize -1 vector < vector < int >> scc ; vector < int > order ; // maintain order created by search 1 vector < int > answer ( 2 * MAXN ); void search1 ( int u ) { if ( processed [ u ]) return ; processed [ u ] = true ; for ( auto v : adj [ u ]) search1 ( v ); order . push_back ( u ); } void search2 ( int u ) { if ( rprocessed [ u ]) return ; rprocessed [ u ] = true ; for ( auto v : radj [ u ]) search2 ( v ); scc [ scc . size () - 1 ]. push_back ( u ); sccnum [ u ] = scc . size () - 1 ; // write which SCC node 'u' is in } int neg ( int i ) { // negate of i return ( i + N ) % ( 2 * N ); } int main () { for ( int i = 0 ; i < 2 * N ; i ++ ) { // to 2 * N because we have to consider neg. if ( ! processed [ i ]) search1 ( i ); } reverse ( order . begin (), order . end ()); for ( auto u : order ) { if ( ! rprocessed [ u ]) { scc . push_back ( vector < int > ()); search2 ( u ); } } for ( int i = 0 ; i < N ; i ++ ) { if ( sccnum [ i ] == sccnum [ neg ( i )]) { cout << \"WE FOUND CONTRADICTION!!!\" ; return 0 ; } } reverse ( scc . begin (), scc . end ()); // reverse order of topological order of SCC for ( auto component : scc ) { for ( auto x : component ) { if ( answer [ x ] == - 1 ) { // if not assigned yet answer [ x ] = true ; answer [ neg ( x )] = false ; } } } for ( int i = N ; i < 2 * N ; i ++ ) { cout << i << \": \" << answer [ i ] << endl ; } } Related Problems \u00b6 2-SAT 1 2-SAT 2 2-SAT 3 2-SAT 4","title":"Strong Connectivity"},{"location":"Algorithms/Graph/StrongConnectivity/#strong_connectivitydags","text":"A graph is strongly connected if there is a path from any node to all other nodes in the graph The strongly connected components of a graph divide the graph into strongly connected parts that are as large as possible. The strongly connected components form an acyclic component graph that represents deep structure of original graph. A component graph is a cyclic, directed graph, so it is easior to process than the original graph. Since the graph does not contain cycles, we can always construct a topological sort and use dynamic programming techniques.","title":"Strong Connectivity(DAGs)"},{"location":"Algorithms/Graph/StrongConnectivity/#kosarajus_algorithm","text":"Kosaraju's algorithm is an efficient method for finding the strongly connected components of a directed graph. The algorithm performs two depth-first search the first search constructs a list of nodes according to the structure of the graph the second search forms the strongly connected components.","title":"Kosaraju's algorithm"},{"location":"Algorithms/Graph/StrongConnectivity/#1_search_1","text":"The first phase of Kosaraju's algorithm constructs a list of nodes in the order in which a depth-first search processes them. The algorithm goes through the nodes, and begins a depth-first search at each unprocessed node. Each node will be added to the list after it has been processed.","title":"1. Search 1"},{"location":"Algorithms/Graph/StrongConnectivity/#2_search_2","text":"The second phase of the algorithm forms the strongly connected components of the graph. First, the algorithm reverses every edge in the graph. This guarantees that during second search, we will always find strongly connected components that do not have extra nodes. After this, the algorithm goes through the list of nodes created by the first search, in reverse order. if a node does not belong to a component, the algorithm creates a new component and starts a depth-first search that adds all new nodes found durin g the search to the new component leak Note that since all edges are reversed, the component does not leak \"leak\" to other pars in the graph.","title":"2. Search 2"},{"location":"Algorithms/Graph/StrongConnectivity/#implementation","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 vector < int > adj [ V ]; //adjancecy lists vector < int > radj [ V ]; // reversed adjacency list (directions are opposite to original) vector < int > order ; vector < vector < int >> result ; bool processed [ V ]; void search1 ( int u ) { if ( processed [ u ]) return ; processed [ u ] = true ; for ( auto v : adj [ u ]) search1 ( v ); order . push_back ( u ); } void search2 ( int u ) { if ( processed [ u ]) return ; processed [ u ] = true ; for ( auto v : radj [ u ]) search2 ( v ); result [ result . size () - 1 ]. push_back ( u ); //add current node to the last component } int main () { for ( int i = 0 ; i < V ; i ++ ) { // V = number of nodes if ( ! processed [ i ]) search1 ( i ); } reverse ( order . begin (), order . end ()) // reverse the order that created by the first search for ( int i = 0 ; i < V ; i ++ ) // initialized processed array processed [ i ] = false ; for ( auto u : order ) if ( ! processed [ u ]) result . push_back ( vector < int > ()); // add new component search2 ( i ); }","title":"Implementation"},{"location":"Algorithms/Graph/StrongConnectivity/#time_complexity","text":"$\\Omicron(n+m)$ because the algorithm performs two depth-first searches.","title":"Time complexity"},{"location":"Algorithms/Graph/StrongConnectivity/#2sat_problem","text":"Strong connectivity is also linked with the 2SAT problem . In this problem, we are given a logical formula for example $$ L_1 = (x_2 \\lor \\neg x_1) \\land (\\neg x_1 \\lor \\neg x_2) \\land (x_1 \\lor x_3) \\land (\\neg x_2 \\lor \\neg x_3) \\land (x_1 \\lor x_4) $$ is true when the variables are assigned as follows $$ \\begin{aligned} x_1 & = false \\\\ x_2 & = false \\\\ x_3 & = true \\\\ x_4 & = true \\\\ \\end{aligned} $$ The 2SAT problem can be represented as a graph whose nodes correspond to variable $x_i$ and negations $\\neg x_i$, and edges determine the connections between the variables. Each pair $(a_i \\lor b_i)$ generates two edges: $\\neg a_i \\rarr b_i$ and $\\neg b_i \\rarr a_i$. This means that if $a_i$ does not hold, $b_i$ must hold, and vice versa. The structure of the graph tells us whether it is possible to assign the values of the variables so that the formula is true. It turns out that this can be done exactly when there are no nodes $x_i$ and $\\neg x_i$ such that both nodes belong to the same strongly connected component. If there are such nodes, the graph contains a path from $x_i$ to $\\neg x_i$ and also a path from $\\neg x_i$ to $x_i$, so both $x_i$ and $\\neg x_i$ should be true which is not possible. In the graph of the formula $L_1$ there are no nodes $x_i$ and $\\neg x_i$ such that both nodes belong to the same strongly connected component, so solution exists. If a solution exists, the values for the variables can be found by going through the nodes of the component graph in a reverse topological sort order. at each step, we process a component that does not contain edges that lead to an unprocessed component. If the variables in the component have not been assigned values, their values will be determined accoring to the values in the component, and if they already have values, they remain unchanged. The process continues until each variable has been assigned a value. this method works because the graph has a special structure: if there are paths from node $x_i$ to node $x_j$ and from node $x_j$ to node $\\neg x_j$, then node $x_i$ never becomes true. The reason for this is that there is also a path from node $\\neg x_j$ to node $\\neg x_j$, and both $x_i$ and $x_j$ become false. Info A more difficult problem is 3SAT problem , where each part of the formula is form $(a_i \\lor b_i \\lor c_i)$. This problem is NP-hard, so no efficient algorithm for solving the problem is known.","title":"2SAT problem"},{"location":"Algorithms/Graph/StrongConnectivity/#implementation_1","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 int const MAXN ; // maximum number of variables int N ; // number of variables int M ; // number of conditions vector < vector < int >> adj ; // adjancecy list vector < vector < int >> radj ; //inver adjancency list vector < bool > processed ( 2 * MAXN ); vector < bool > rprocessed ( 2 * MAXN ); vector < int > sccnum ( 2 * MAXN , - 1 ); // initialize -1 vector < vector < int >> scc ; vector < int > order ; // maintain order created by search 1 vector < int > answer ( 2 * MAXN ); void search1 ( int u ) { if ( processed [ u ]) return ; processed [ u ] = true ; for ( auto v : adj [ u ]) search1 ( v ); order . push_back ( u ); } void search2 ( int u ) { if ( rprocessed [ u ]) return ; rprocessed [ u ] = true ; for ( auto v : radj [ u ]) search2 ( v ); scc [ scc . size () - 1 ]. push_back ( u ); sccnum [ u ] = scc . size () - 1 ; // write which SCC node 'u' is in } int neg ( int i ) { // negate of i return ( i + N ) % ( 2 * N ); } int main () { for ( int i = 0 ; i < 2 * N ; i ++ ) { // to 2 * N because we have to consider neg. if ( ! processed [ i ]) search1 ( i ); } reverse ( order . begin (), order . end ()); for ( auto u : order ) { if ( ! rprocessed [ u ]) { scc . push_back ( vector < int > ()); search2 ( u ); } } for ( int i = 0 ; i < N ; i ++ ) { if ( sccnum [ i ] == sccnum [ neg ( i )]) { cout << \"WE FOUND CONTRADICTION!!!\" ; return 0 ; } } reverse ( scc . begin (), scc . end ()); // reverse order of topological order of SCC for ( auto component : scc ) { for ( auto x : component ) { if ( answer [ x ] == - 1 ) { // if not assigned yet answer [ x ] = true ; answer [ neg ( x )] = false ; } } } for ( int i = N ; i < 2 * N ; i ++ ) { cout << i << \": \" << answer [ i ] << endl ; } }","title":"Implementation"},{"location":"Algorithms/Graph/StrongConnectivity/#related_problems","text":"2-SAT 1 2-SAT 2 2-SAT 3 2-SAT 4","title":"Related Problems"},{"location":"Algorithms/Graph/DFS/FindingCutEdges/","text":"Finding Cut Edges \u00b6 The code below works properly because of the lemma above(first lemma) 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 h [ root ] = 0 par [ v ] = - 1 dfs ( v ): d [ v ] = h [ v ] color [ v ] = gray for u in adj [ v ]: if color [ u ] == white : then par [ u ] = v and dfs ( u ) and d [ v ] = min ( d [ v ], d [ u ]) if d [ u ] > h [ v ] then the edge v - u is a cut edge else if u != par [ v ]: then d [ v ] = min ( d [ v ], h [ u ]) color [ v ] = black in this code, h[v] = height of vertex v in the DFS tree and d[v] = min(h[w] where there is at least vertex u in subtree of v in the DFS tree where there is an edge between $u$ and $w$) First lemma will be placed here \u21a9","title":"FindingCutEdges"},{"location":"Algorithms/Graph/DFS/FindingCutEdges/#finding_cut_edges","text":"The code below works properly because of the lemma above(first lemma) 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 h [ root ] = 0 par [ v ] = - 1 dfs ( v ): d [ v ] = h [ v ] color [ v ] = gray for u in adj [ v ]: if color [ u ] == white : then par [ u ] = v and dfs ( u ) and d [ v ] = min ( d [ v ], d [ u ]) if d [ u ] > h [ v ] then the edge v - u is a cut edge else if u != par [ v ]: then d [ v ] = min ( d [ v ], h [ u ]) color [ v ] = black in this code, h[v] = height of vertex v in the DFS tree and d[v] = min(h[w] where there is at least vertex u in subtree of v in the DFS tree where there is an edge between $u$ and $w$) First lemma will be placed here \u21a9","title":"Finding Cut Edges"},{"location":"Algorithms/Graph/DFS/Preface/","text":"DFS \u00b6 The most useful graph algorithms are search algorithms. DFS(Depth First Search) is one of them. While running DFS, we assign colors to the vertices (initially white) Algorithm itself is really simple 1 2 3 4 5 6 dfs ( v ): color [ v ] = gray for u in adj [ v ]: if color [ u ] == white : then dfs ( u ) color [ v ] = black Black color here is not used, but you can use it sometimes. Time complexity: $O(n + m)$","title":"Preface"},{"location":"Algorithms/Graph/DFS/Preface/#dfs","text":"The most useful graph algorithms are search algorithms. DFS(Depth First Search) is one of them. While running DFS, we assign colors to the vertices (initially white) Algorithm itself is really simple 1 2 3 4 5 6 dfs ( v ): color [ v ] = gray for u in adj [ v ]: if color [ u ] == white : then dfs ( u ) color [ v ] = black Black color here is not used, but you can use it sometimes. Time complexity: $O(n + m)$","title":"DFS"},{"location":"Algorithms/Graph/DFS/StartingFinishingTime/","text":"Starting time, finishing time \u00b6 Starting time of a vertex is the time we enter it (the order we enter it) and its finishing time is the time we leave it. Calculating these are easy 1 2 3 4 5 6 7 8 9 TIME = 0 dfs ( v ): st [ v ] = TIME ++ color [ v ] = gray for u in adj [ v ]: if color [ u ] == white : then dfs ( u ) color [ v ] = black ft [ v ] = TIME # or we can use TIME ++ It is useable in specially data structure problems (convert the tree into an array). Lemma-1 : if we run $dfs(root)$ in a rooted tree, then v is an ancestor of $u$ if and only if $st_v\\leq st_u\\leq ft_u\\leq ft_v$. So, given arrays $st$ and $ft$ we can rebuild the tree.","title":"StartingFinishingTime"},{"location":"Algorithms/Graph/DFS/StartingFinishingTime/#starting_time_finishing_time","text":"Starting time of a vertex is the time we enter it (the order we enter it) and its finishing time is the time we leave it. Calculating these are easy 1 2 3 4 5 6 7 8 9 TIME = 0 dfs ( v ): st [ v ] = TIME ++ color [ v ] = gray for u in adj [ v ]: if color [ u ] == white : then dfs ( u ) color [ v ] = black ft [ v ] = TIME # or we can use TIME ++ It is useable in specially data structure problems (convert the tree into an array). Lemma-1 : if we run $dfs(root)$ in a rooted tree, then v is an ancestor of $u$ if and only if $st_v\\leq st_u\\leq ft_u\\leq ft_v$. So, given arrays $st$ and $ft$ we can rebuild the tree.","title":"Starting time, finishing time"},{"location":"Algorithms/Graph/DFS/Tree/","text":"DFS tree \u00b6 DFS Tree is a rooted tree that is built like this 1 2 3 4 5 6 7 let T be a new tree dfs ( v ): color [ v ] = gray for u in adj [ v ]: if color [ u ] == white : then dfs ( u ) and par [ u ] = v ( in T ) color [ v ] = black Lemma : There is no cross edges, it means if there is an edge between $V$ and $u$, then $v=par[u]$ or $u=par[v]$","title":"DFSTree"},{"location":"Algorithms/Graph/DFS/Tree/#dfs_tree","text":"DFS Tree is a rooted tree that is built like this 1 2 3 4 5 6 7 let T be a new tree dfs ( v ): color [ v ] = gray for u in adj [ v ]: if color [ u ] == white : then dfs ( u ) and par [ u ] = v ( in T ) color [ v ] = black Lemma : There is no cross edges, it means if there is an edge between $V$ and $u$, then $v=par[u]$ or $u=par[v]$","title":"DFS tree"},{"location":"Algorithms/Graph/ShortestPaths/Bellman-Ford/","text":"Bellman-Ford Algorithm \u00b6 The Bellman-Ford algorithm finds shortest paths from a starting node to all nodes of the graph. The algorithm reduces the distance by finding edges that shorten the paths until it is not possible to reduce any distances. Bellman-Ford can process all kinds of graphs The algorithm can process all kinds of graphs, provided that the graph does not contain a cycle with negative length. If the graph contains a negative cycle, the algorithm can detect this. Implementation \u00b6 Assume that the graph is stored as an edge list edge that consists of tuples of the form$(a, b, w)$, meaing that there is an edge from node $a$ to node $b$ with weight $w$. The algorithm consists of $n-1$ rounds, and on each round the round the algorithm goes through all edges of the graph and tries to reduce the distances. The algorithm constructs an array $\\text{distance}$ that will contain the distance from x to all nodes of the graph. The constant INF denotes an infinite distance. $n = \\text{number of vertices(nodes)}$, $m = \\text{number of edges}$ 1 2 3 4 5 6 7 8 9 10 11 12 int const INF = 2e9 ; tuple < int , int , int > edges [ m ]; //edge list for ( int i = 1 ; i <= n ; i ++ ) distance [ i ] = INF ; distance [ s ] = 0 ; // starting node s for ( int i = 1 ; i <= n - 1 ; i ++ ) { for ( auto e : edges ) { int a , b , w ; tie ( a , b , w ) = e ; distance [ b ] = min ( distance [ b ], distance [ a ] + w ); } } Time Complexity \u00b6 $\\Omicron(nm)$ Negative Cycles \u00b6 The algorithm can also be used to check if the graph contains a cycle with negative length. A negative cycle can be detected using the Bellman-Ford algorithm by running the algorithm for $n$ rounds If the n-th round reduces any distance, the graph contains a negative cycle. Negative cycle in the whole graph Note that this algorithm can be used to search for a negative cycle in the whole graph regardless of the starting node","title":"Bellman-Ford"},{"location":"Algorithms/Graph/ShortestPaths/Bellman-Ford/#bellman-ford_algorithm","text":"The Bellman-Ford algorithm finds shortest paths from a starting node to all nodes of the graph. The algorithm reduces the distance by finding edges that shorten the paths until it is not possible to reduce any distances. Bellman-Ford can process all kinds of graphs The algorithm can process all kinds of graphs, provided that the graph does not contain a cycle with negative length. If the graph contains a negative cycle, the algorithm can detect this.","title":"Bellman-Ford Algorithm"},{"location":"Algorithms/Graph/ShortestPaths/Bellman-Ford/#implementation","text":"Assume that the graph is stored as an edge list edge that consists of tuples of the form$(a, b, w)$, meaing that there is an edge from node $a$ to node $b$ with weight $w$. The algorithm consists of $n-1$ rounds, and on each round the round the algorithm goes through all edges of the graph and tries to reduce the distances. The algorithm constructs an array $\\text{distance}$ that will contain the distance from x to all nodes of the graph. The constant INF denotes an infinite distance. $n = \\text{number of vertices(nodes)}$, $m = \\text{number of edges}$ 1 2 3 4 5 6 7 8 9 10 11 12 int const INF = 2e9 ; tuple < int , int , int > edges [ m ]; //edge list for ( int i = 1 ; i <= n ; i ++ ) distance [ i ] = INF ; distance [ s ] = 0 ; // starting node s for ( int i = 1 ; i <= n - 1 ; i ++ ) { for ( auto e : edges ) { int a , b , w ; tie ( a , b , w ) = e ; distance [ b ] = min ( distance [ b ], distance [ a ] + w ); } }","title":"Implementation"},{"location":"Algorithms/Graph/ShortestPaths/Bellman-Ford/#time_complexity","text":"$\\Omicron(nm)$","title":"Time Complexity"},{"location":"Algorithms/Graph/ShortestPaths/Bellman-Ford/#negative_cycles","text":"The algorithm can also be used to check if the graph contains a cycle with negative length. A negative cycle can be detected using the Bellman-Ford algorithm by running the algorithm for $n$ rounds If the n-th round reduces any distance, the graph contains a negative cycle. Negative cycle in the whole graph Note that this algorithm can be used to search for a negative cycle in the whole graph regardless of the starting node","title":"Negative Cycles"},{"location":"Algorithms/Graph/ShortestPaths/Dijkstra/","text":"Dijkstra's Algorithm \u00b6 The algorithm finds shortest paths from the starting node to all nodes of the graph, Like Bellman-Ford algorithm . The Benefit of Dijkstra's algorithm is that it is more efficent and can be used for processing large graphs. Dijkstra's algorithm is efficient, because it only process each edge in the graph once, using the fact that there are no negative edges. Negative edges Dijkstra's algorithm requires that there are no negative weight edges in the graph Implementation \u00b6 Assume that the graph is stored as an adjacency lists so that $adj[a]$ contains a pair $(b, w)$ always when there is an edge from node $a$ to node $b$ with weight $w$ Use priority queue that contains nodes the nodes ordered by their distances. Using priority queue, the next node to be processed can be retrieved in logarithmic time 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 int distance []; bool processed []; priority_queue < int , int > q ; // (-dis, node) for ( int i = 1 ; i <= n ; i ++ ) distance [ i ] = INF ; distance [ s ] = 0 ; // starting node s q . push ({ 0 , x }); while ( ! q . empty ()) { int a = q . top (). second ; q . pop (); if ( processed [ a ]) continue ; processed [ a ] = true ; for ( auto u : adj [ a ]) { int b = u . first , w = u . second ; if ( distance [ a ] + w < distance [ b ]) { distance [ b ] = distance [ a ] + w ; q . push ( - distance [ b ], b ); } } } Time Complexity \u00b6 Not yet","title":"Dijkstra"},{"location":"Algorithms/Graph/ShortestPaths/Dijkstra/#dijkstras_algorithm","text":"The algorithm finds shortest paths from the starting node to all nodes of the graph, Like Bellman-Ford algorithm . The Benefit of Dijkstra's algorithm is that it is more efficent and can be used for processing large graphs. Dijkstra's algorithm is efficient, because it only process each edge in the graph once, using the fact that there are no negative edges. Negative edges Dijkstra's algorithm requires that there are no negative weight edges in the graph","title":"Dijkstra's Algorithm"},{"location":"Algorithms/Graph/ShortestPaths/Dijkstra/#implementation","text":"Assume that the graph is stored as an adjacency lists so that $adj[a]$ contains a pair $(b, w)$ always when there is an edge from node $a$ to node $b$ with weight $w$ Use priority queue that contains nodes the nodes ordered by their distances. Using priority queue, the next node to be processed can be retrieved in logarithmic time 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 int distance []; bool processed []; priority_queue < int , int > q ; // (-dis, node) for ( int i = 1 ; i <= n ; i ++ ) distance [ i ] = INF ; distance [ s ] = 0 ; // starting node s q . push ({ 0 , x }); while ( ! q . empty ()) { int a = q . top (). second ; q . pop (); if ( processed [ a ]) continue ; processed [ a ] = true ; for ( auto u : adj [ a ]) { int b = u . first , w = u . second ; if ( distance [ a ] + w < distance [ b ]) { distance [ b ] = distance [ a ] + w ; q . push ( - distance [ b ], b ); } } }","title":"Implementation"},{"location":"Algorithms/Graph/ShortestPaths/Dijkstra/#time_complexity","text":"Not yet","title":"Time Complexity"},{"location":"Algorithms/Graph/ShortestPaths/Floyd-Warshall/","text":"Floyd-Warshall Algorithm \u00b6 Floyd-Warshall algorithm provides an alternative way to approach th problem of finding shortest paths. Floyd-Warhsall algorithm finds all shortests paths between the nodes in a single run. Floyd-Warhsall algorithm maintains a two-dimensional array that contains distances between the nodes. Floyd-warshall algorithm is easy to implement. Floyd-warhsall algorithm reduces distance by intermediate nodes in paths. Implementation \u00b6 Assume using adjacency matrix 1 2 3 4 5 6 7 8 9 10 11 12 // first build distance (2-dimensional array) int const INF = 2e9 ; int adj [][]; int distance [][]; for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 1 ; j <= n ; j ++ ) { if ( i == j ) distance [ i ][ j ] = 0 ; else if ( adj [ i ][ j ]) distance [ i ][ j ] = adj [ i ][ j ]; else distance [ i ][ j ] = INF ; } } 1 2 3 4 5 6 7 8 9 // process for ( int k = 1 ; k <= n ; k ++ ) { // k == intermediate node for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 1 ; j <= n ; j ++ ) { distance [ i ][ j ] = min ( distance [ i ][ j ], distance [ i ][ k ] + distance [ k ][ j ]) } } } Time Complexity \u00b6 $\\Omicron(n^3)$ Related Problems \u00b6 K-th Path","title":"Floyd-Warshall"},{"location":"Algorithms/Graph/ShortestPaths/Floyd-Warshall/#floyd-warshall_algorithm","text":"Floyd-Warshall algorithm provides an alternative way to approach th problem of finding shortest paths. Floyd-Warhsall algorithm finds all shortests paths between the nodes in a single run. Floyd-Warhsall algorithm maintains a two-dimensional array that contains distances between the nodes. Floyd-warshall algorithm is easy to implement. Floyd-warhsall algorithm reduces distance by intermediate nodes in paths.","title":"Floyd-Warshall Algorithm"},{"location":"Algorithms/Graph/ShortestPaths/Floyd-Warshall/#implementation","text":"Assume using adjacency matrix 1 2 3 4 5 6 7 8 9 10 11 12 // first build distance (2-dimensional array) int const INF = 2e9 ; int adj [][]; int distance [][]; for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 1 ; j <= n ; j ++ ) { if ( i == j ) distance [ i ][ j ] = 0 ; else if ( adj [ i ][ j ]) distance [ i ][ j ] = adj [ i ][ j ]; else distance [ i ][ j ] = INF ; } } 1 2 3 4 5 6 7 8 9 // process for ( int k = 1 ; k <= n ; k ++ ) { // k == intermediate node for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 1 ; j <= n ; j ++ ) { distance [ i ][ j ] = min ( distance [ i ][ j ], distance [ i ][ k ] + distance [ k ][ j ]) } } }","title":"Implementation"},{"location":"Algorithms/Graph/ShortestPaths/Floyd-Warshall/#time_complexity","text":"$\\Omicron(n^3)$","title":"Time Complexity"},{"location":"Algorithms/Graph/ShortestPaths/Floyd-Warshall/#related_problems","text":"K-th Path","title":"Related Problems"},{"location":"Algorithms/Graph/ShortestPaths/Preface/","text":"Shortest Paths \u00b6 Finding a shortest path between two nodes of a graph is an important problem that has many practical applications. In an uweighted graph, the length of a path equals the number of its edges, and we can simply use breath-first search to find a shortest path. However, in this chapter we focus on weighted graphs where more sophisticated algorithms are needed for shortest paths. Diff \u00b6 $n = \\text{number of nodes}$, $m = \\text{number of edges}$ TimeComplexity DS - Bellman-Ford $\\Omicron(nm)$ edge list neg-cycle detection SPFA $\\Omicron(nm)$ Dijkstra $\\Omicron(n + m\\lg(m))$ adjacency lists no neg edges Floyd-Warshall $\\Omicron(n^3)$ adjacency matrix finds all shortest paths between the nodes","title":"Preface"},{"location":"Algorithms/Graph/ShortestPaths/Preface/#shortest_paths","text":"Finding a shortest path between two nodes of a graph is an important problem that has many practical applications. In an uweighted graph, the length of a path equals the number of its edges, and we can simply use breath-first search to find a shortest path. However, in this chapter we focus on weighted graphs where more sophisticated algorithms are needed for shortest paths.","title":"Shortest Paths"},{"location":"Algorithms/Graph/ShortestPaths/Preface/#diff","text":"$n = \\text{number of nodes}$, $m = \\text{number of edges}$ TimeComplexity DS - Bellman-Ford $\\Omicron(nm)$ edge list neg-cycle detection SPFA $\\Omicron(nm)$ Dijkstra $\\Omicron(n + m\\lg(m))$ adjacency lists no neg edges Floyd-Warshall $\\Omicron(n^3)$ adjacency matrix finds all shortest paths between the nodes","title":"Diff"},{"location":"Algorithms/Linear/BinarySearch/","text":"Binary search \u00b6 A general method for search for an element in an array is to use for loop that iterates through the elements of the array. For example, the following code searches for an element $x$ in an array 1 2 3 4 5 for ( int i = 0 ; i < n ; i ++ ) { if ( array [ i ] == x ) { //x found at index i; } } The time complexity of this approach is $\\Omicron(n)$, because in the worst case, it is necessary to check all elements of the array. If the order of the elements is arbitrary, this is also the best possible approach, because there is no additional information available where in the array we should search for the element $x$. However, If the array is sorted, the situation is different. In this case it is possible to perform the search much faster, because the order of the elements in the array guides the search. The following binary search algorithm efficiently search for an element in a sorted array in $\\Omicron(lgn)$ time. Time Compelxity \u00b6 Algorithm time complexity Method1 $\\Omicron(lgn)$ Method2 $\\Omicron(lgn)$ Implementation \u00b6 Method 1 \u00b6 The usual way to implement binary search resembles looking for a word in a dictionary. The search maintains an active region in the array, which initially contains all array elements. Then, a number of steps is performed, each of which havles the size of the region. At each step, the search checks to middle element of the active region. If the middle elemnt is the target element, the search terminates. Otherwise, the search recursively continues to the left or right half of the region, depending on the value of the middle element. 1 2 3 4 5 6 7 8 9 int a = 0 , b = n - 1 ; while ( a <= b ) { int k = ( a + b ) / 2 ; if ( array [ k ] == x ) { // x found at index k; } if ( array [ k ] > x ) b = k - 1 ; else a = k + 1 ; } In this implementation, the active region is $a\\dots b$, and initially the region is $0\\dots n - 1$ . The algorithm havles the size of the region at each step, so the time complexity is $\\Omicron(lgn)$. Method 2 \u00b6 An alternative method to implement binary search is based on an efficient way to itertate through the elmetns of the array. The idea is to make umps and slow the speed when we get closer to the target element. The search goes through the array from left to right, and the initial jump length is $n/2$. At each step, the jump length will be havled: first $n/4$, then $n/8$, $n/16$, etc., until finally the length is 1. After the jumps, either the target element has been found or we know the it does not appear in the array. 1 2 3 4 5 6 7 int k = 0 ; for ( int b = n / 2 ; b >= 1 ; b /= 2 ) { while ( k + b < n && array [ k + b ] <= x ) k += b ; } if ( array [ k ] == x ) { // x found at index k } During the search, the variable $b$ contains the current jump length. The time complexity of the algorithm is $\\Omicron(lgn)$, because the code in the while loop is performed at most twice for each jump length.","title":"BinarySearch"},{"location":"Algorithms/Linear/BinarySearch/#binary_search","text":"A general method for search for an element in an array is to use for loop that iterates through the elements of the array. For example, the following code searches for an element $x$ in an array 1 2 3 4 5 for ( int i = 0 ; i < n ; i ++ ) { if ( array [ i ] == x ) { //x found at index i; } } The time complexity of this approach is $\\Omicron(n)$, because in the worst case, it is necessary to check all elements of the array. If the order of the elements is arbitrary, this is also the best possible approach, because there is no additional information available where in the array we should search for the element $x$. However, If the array is sorted, the situation is different. In this case it is possible to perform the search much faster, because the order of the elements in the array guides the search. The following binary search algorithm efficiently search for an element in a sorted array in $\\Omicron(lgn)$ time.","title":"Binary search"},{"location":"Algorithms/Linear/BinarySearch/#time_compelxity","text":"Algorithm time complexity Method1 $\\Omicron(lgn)$ Method2 $\\Omicron(lgn)$","title":"Time Compelxity"},{"location":"Algorithms/Linear/BinarySearch/#implementation","text":"","title":"Implementation"},{"location":"Algorithms/Linear/BinarySearch/#method_1","text":"The usual way to implement binary search resembles looking for a word in a dictionary. The search maintains an active region in the array, which initially contains all array elements. Then, a number of steps is performed, each of which havles the size of the region. At each step, the search checks to middle element of the active region. If the middle elemnt is the target element, the search terminates. Otherwise, the search recursively continues to the left or right half of the region, depending on the value of the middle element. 1 2 3 4 5 6 7 8 9 int a = 0 , b = n - 1 ; while ( a <= b ) { int k = ( a + b ) / 2 ; if ( array [ k ] == x ) { // x found at index k; } if ( array [ k ] > x ) b = k - 1 ; else a = k + 1 ; } In this implementation, the active region is $a\\dots b$, and initially the region is $0\\dots n - 1$ . The algorithm havles the size of the region at each step, so the time complexity is $\\Omicron(lgn)$.","title":"Method 1"},{"location":"Algorithms/Linear/BinarySearch/#method_2","text":"An alternative method to implement binary search is based on an efficient way to itertate through the elmetns of the array. The idea is to make umps and slow the speed when we get closer to the target element. The search goes through the array from left to right, and the initial jump length is $n/2$. At each step, the jump length will be havled: first $n/4$, then $n/8$, $n/16$, etc., until finally the length is 1. After the jumps, either the target element has been found or we know the it does not appear in the array. 1 2 3 4 5 6 7 int k = 0 ; for ( int b = n / 2 ; b >= 1 ; b /= 2 ) { while ( k + b < n && array [ k + b ] <= x ) k += b ; } if ( array [ k ] == x ) { // x found at index k } During the search, the variable $b$ contains the current jump length. The time complexity of the algorithm is $\\Omicron(lgn)$, because the code in the while loop is performed at most twice for each jump length.","title":"Method 2"},{"location":"Algorithms/Linear/LowerAndUpperBounds/","text":"Lower&Upper Bound \u00b6 The C++ standard library contains the following functions that are based on binary search and work in logarithmic time. lower_bound: returns a pointer to the first array element whose value is at least $x$. upper_bound: returns a pointer to the first array element whose value is larger than $x$. equal_range: returns both above pointers The functions assume that the array is sorted. If there is no such element, the pointer points to the element after the last array element. For example, the following code finds out whether an array contains an element with vlaue $x$. 1 2 3 4 auto k = lower_bound ( array , array + n , x ) - array ; if ( k < n && array [ k ] == x ) { // x found at index k } Then, the following code counts th enumber o f elements whose value is $x$ 1 2 3 auto a = lower_bound ( array , array + n , x ); auto b = upper_bound ( array , array + n , x ); cout << b - a << '\\n' ; Using equal_range , the code becomes shorter: 1 2 auto r = equal_range ( array , array + n , x ); cout << r . second - r . first << '\\n' ; Finding the smallest solution \u00b6 An important use for binary search is to find the position where the value of a function change. Suppose that we wish to find the smallest vlaue $k$ that is a valid solution for a problem. We are given a function ok(x) that returns true if $x$ is valid solution and false otherwise. In addition, we know that ok(x) is false when $x < k$ and true when $x \\geq k$. Now, the value of $k$ can be found using binary search. 1 2 3 4 5 int x = - 1 ; for ( int b = z ; b >= 1 ; b /= 2 ) { while ( ! ok ( x + b )) x += b ; } int k = x + 1 ; The search finds the largest value of $x$ for which ok(x) is false . Thus, the next value k = x + 1 is the smallest possible value for which ok(k) is true . The initial jump length $z$ has to be lar enough, for exmaple some vlaue for which we know beforehand that ok(z) is true . The algorithm calls the function ok $\\Omicron(lgz)$ times, so the total time complexity depends on the function ok . For example, if the function works in works in $\\Omicron(n)$ time, the total time complexity is $Omicron(nlgz)$. Finding the maximum value \u00b6 Binary search can also be used to find the maximum value for a function that is first increasing and then decreasing. Our task is to find a poisition $k$ such that $f(x) < f(x + 1 )$ when $x < k $, and $f(x) > f(x + 1)$ when $ x \\geq k$ The idea is to use binary search for finding the largest value of $x$ for which $f(x) < f(x + 1)$. This implies that $ k = x + 1$ because $f(x+1) > f(x + 2). 1 2 3 4 5 int x = - 1 ; for ( int b = z ; b >= 1 ; b /= 2 ) { while ( f ( x + b ) < f ( x + b + 1 )) x += b ; } int k = x + 1 ; Note that unlike in the ordinary binary search, here it is not allowed that consecutive values of the function are equal. In this case it would not be possible to know how to continue the search.","title":"Lower and Upper Bounds"},{"location":"Algorithms/Linear/LowerAndUpperBounds/#lowerupper_bound","text":"The C++ standard library contains the following functions that are based on binary search and work in logarithmic time. lower_bound: returns a pointer to the first array element whose value is at least $x$. upper_bound: returns a pointer to the first array element whose value is larger than $x$. equal_range: returns both above pointers The functions assume that the array is sorted. If there is no such element, the pointer points to the element after the last array element. For example, the following code finds out whether an array contains an element with vlaue $x$. 1 2 3 4 auto k = lower_bound ( array , array + n , x ) - array ; if ( k < n && array [ k ] == x ) { // x found at index k } Then, the following code counts th enumber o f elements whose value is $x$ 1 2 3 auto a = lower_bound ( array , array + n , x ); auto b = upper_bound ( array , array + n , x ); cout << b - a << '\\n' ; Using equal_range , the code becomes shorter: 1 2 auto r = equal_range ( array , array + n , x ); cout << r . second - r . first << '\\n' ;","title":"Lower&amp;Upper Bound"},{"location":"Algorithms/Linear/LowerAndUpperBounds/#finding_the_smallest_solution","text":"An important use for binary search is to find the position where the value of a function change. Suppose that we wish to find the smallest vlaue $k$ that is a valid solution for a problem. We are given a function ok(x) that returns true if $x$ is valid solution and false otherwise. In addition, we know that ok(x) is false when $x < k$ and true when $x \\geq k$. Now, the value of $k$ can be found using binary search. 1 2 3 4 5 int x = - 1 ; for ( int b = z ; b >= 1 ; b /= 2 ) { while ( ! ok ( x + b )) x += b ; } int k = x + 1 ; The search finds the largest value of $x$ for which ok(x) is false . Thus, the next value k = x + 1 is the smallest possible value for which ok(k) is true . The initial jump length $z$ has to be lar enough, for exmaple some vlaue for which we know beforehand that ok(z) is true . The algorithm calls the function ok $\\Omicron(lgz)$ times, so the total time complexity depends on the function ok . For example, if the function works in works in $\\Omicron(n)$ time, the total time complexity is $Omicron(nlgz)$.","title":"Finding the smallest solution"},{"location":"Algorithms/Linear/LowerAndUpperBounds/#finding_the_maximum_value","text":"Binary search can also be used to find the maximum value for a function that is first increasing and then decreasing. Our task is to find a poisition $k$ such that $f(x) < f(x + 1 )$ when $x < k $, and $f(x) > f(x + 1)$ when $ x \\geq k$ The idea is to use binary search for finding the largest value of $x$ for which $f(x) < f(x + 1)$. This implies that $ k = x + 1$ because $f(x+1) > f(x + 2). 1 2 3 4 5 int x = - 1 ; for ( int b = z ; b >= 1 ; b /= 2 ) { while ( f ( x + b ) < f ( x + b + 1 )) x += b ; } int k = x + 1 ; Note that unlike in the ordinary binary search, here it is not allowed that consecutive values of the function are equal. In this case it would not be possible to know how to continue the search.","title":"Finding the maximum value"},{"location":"Algorithms/RangeQueries/Preface/","text":"Range Queries \u00b6 In a range query, our task is to calculate a value based on a subarray of an array. Typical range queries are $sum_q(a, b)$: calculate the sum of values in range $[a, b]$ $min_q(a, b)$: find the minimum value in range $[a, b]$ $max_q(a, b)$: find the maximum value in range $[a, b]$ A simple way to process range queries is to use a loop that goes through all array values in the range. 1 2 3 4 5 6 7 int sum ( int a , int b ) { int s = 0 ; for ( int i = a ; i < b ; i ++ ) { s += array [ i ]; } return s ; } This function works in $\\Omicron(n)$ time, where $n$ is the size of the array. Thus, we can process $q$ queries in $\\Omicron(nq)$ time using the function. However, if both $n$ and $q$ are large, this approach is slow. Fortunately, it turns out that there are ways to process range queries much more efficiently. Table of contents \u00b6 Static array queries \u00b6 Sum queries Minimum queries Binary indexed tree(== Fenwick tree) \u00b6 Segment tree \u00b6 Other queries \u00b6 Additional techniques \u00b6 Index compression Range updates","title":"Preface"},{"location":"Algorithms/RangeQueries/Preface/#range_queries","text":"In a range query, our task is to calculate a value based on a subarray of an array. Typical range queries are $sum_q(a, b)$: calculate the sum of values in range $[a, b]$ $min_q(a, b)$: find the minimum value in range $[a, b]$ $max_q(a, b)$: find the maximum value in range $[a, b]$ A simple way to process range queries is to use a loop that goes through all array values in the range. 1 2 3 4 5 6 7 int sum ( int a , int b ) { int s = 0 ; for ( int i = a ; i < b ; i ++ ) { s += array [ i ]; } return s ; } This function works in $\\Omicron(n)$ time, where $n$ is the size of the array. Thus, we can process $q$ queries in $\\Omicron(nq)$ time using the function. However, if both $n$ and $q$ are large, this approach is slow. Fortunately, it turns out that there are ways to process range queries much more efficiently.","title":"Range Queries"},{"location":"Algorithms/RangeQueries/Preface/#table_of_contents","text":"","title":"Table of contents"},{"location":"Algorithms/RangeQueries/Preface/#static_array_queries","text":"Sum queries Minimum queries","title":"Static array queries"},{"location":"Algorithms/RangeQueries/Preface/#binary_indexed_tree_fenwick_tree","text":"","title":"Binary indexed tree(== Fenwick tree)"},{"location":"Algorithms/RangeQueries/Preface/#segment_tree","text":"","title":"Segment tree"},{"location":"Algorithms/RangeQueries/Preface/#other_queries","text":"","title":"Other queries"},{"location":"Algorithms/RangeQueries/Preface/#additional_techniques","text":"Index compression Range updates","title":"Additional techniques"},{"location":"Algorithms/RangeQueries/StaticArrayQueries/","text":"Static array queries \u00b6 We first focus on a situation the array is static , i.e., the array values are never updated between the queries. Sum queries \u00b6 We can easily process sum queries on a static array by constructing a prefix sum array . we can calculate any value of $sum_q(a, b)$ in $\\Omicron(1)$ time as follows $$ \\text{sum}_q(a, b) = \\text{sum}_q(0, b) - \\text{sum}_q(0, a - 1) $$ By defining $\\text{sum}_q(0, -1) = 0$, the above formula also holds when $a = 0$. Higher dimensions \u00b6 It is also possible to generalize this idea to higher dimensions. We can construct a two-dimensional prefix sum array that can be used to calculate the sum of any rectangular subarray in $\\Omicron(1)$ time. Each sum in such a array corresponds to a subarray that begins at the upper-left corner of the array. Minimum queries \u00b6 Minimum queries are more difficult to process than sum queries. Still, there is a quite simple $Omicron(nlgn)$ time preprocessing method after which we can answer any minimum query in $Omicron(1)$ time. Note that since minimum and maximum queries can be processed similarly, we can focus on minimum queries. The idea is to precalculate all values of $\\text{min}_q(a, b)$ where $b-a+1$(the length of the range) is a power of two. The number of precalculated value is $\\Omicron(nlgn)$, because there are $\\Omicron(lgn)$ range lengths that are powers of two. The values can be calculated efficiently using recursive formula $$ \\text{min}_q(a, b) = \\text{min}(\\text{min}_q(a, a + w -1), \\text{min}_q(a + w, b) ) $$ where $b - a + 1$ is apower of two and $w = (b - a + 1) / 2$. Calculating all those values takes $\\Omicron(nlgn)$ time. After this, any value of $min_q(a, b)$ can be calculated in $\\Omicron(1)$ time as a minimum of two precalculated values. Let $k$ be the largest power of two that does not exceed $ b - a + 1 $. We can calculate the value of $min_q(a, b)$ using the formula $$ \\text{min}_q(a, b) = \\text{min}(\\text{min}_q(a, a + k -1), \\text{min}_q(b - k + 1, b)) $$ In the above formula, the range $[a, b]$ is represented as the union of the ranges $[a, a + k -1 ]$ and $[b - k + 1, b]$, both of length $k$. Implementation \u00b6 naive implementation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 int arr [ log ( N ) + 1 ][ N ]; //preprocess for ( int k = 1 ; k <= log ( N ); k ++ ) { int w = ( 1 << ( k - 1 )); for ( int i = 0 ; i < N - w + 1 ; i ++ ) { arr [ k ][ i ] = min ( arr [ k - 1 ][ i ], arr [ k - 1 ][ i + w ]); } } // queries int minq ( int a , int b ) { int w = b - a + 1 ; int k = 0 ; while ( ( 1 << ( k + 1 )) <= w ) k ++ ; return min ( arr [ k ][ a ], arr [ k ][ b - ( 1 << k ) + 1 ]); } improved version 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 /** * File : 2357RMQimproved.cpp * Author : JCHRYS <jchrys@me.com> * Date : 09.09.2019 * Last Modified Date: 09.09.2019 * Last Modified By : JCHRYS <jchrys@me.com> */ #include <bits/stdc++.h> using namespace std ; int const MAXN = 100000 ; int const MAXLOG = 17 ; int log ( int x ) { int ans = - 1 ; while ( x ) { ans ++ ; x >>= 1 ; } return ans ; } int mintable [ MAXLOG ][ MAXN ]; int maxtable [ MAXLOG ][ MAXN ]; int N , M ; void preprocess () { // building base for ( int i = 0 ; i < N ; i ++ ) { cin >> mintable [ 0 ][ i ]; maxtable [ 0 ][ i ] = mintable [ 0 ][ i ]; } for ( int j = 1 ; j <= log ( N ); j ++ ) { for ( int i = 0 ; i + ( 1 << ( j - 1 )) < N ; i ++ ) { mintable [ j ][ i ] = min ( mintable [ j - 1 ][ i ], mintable [ j - 1 ][ i + ( 1 << ( j - 1 ))]); maxtable [ j ][ i ] = max ( maxtable [ j - 1 ][ i ], maxtable [ j - 1 ][ i + ( 1 << ( j - 1 ))]); } } } int minq ( int a , int b ) { int len = b - a + 1 ; int k = log ( len ); return min ( mintable [ k ][ a ], mintable [ k ][ b - ( 1 << k ) + 1 ]); } int maxq ( int a , int b ) { int len = b - a + 1 ; int k = log ( len ); return max ( maxtable [ k ][ a ], maxtable [ k ][ b - ( 1 << k ) + 1 ]); } int main () { cin >> N >> M ; preprocess (); for ( int i = 0 ; i < M ; i ++ ) { int a , b ; cin >> a >> b ; a -- ; b -- ; cout << minq ( a , b ) << ' ' << maxq ( a , b ) << '\\n' ; } return 0 ; }","title":"StaticArrayQueries"},{"location":"Algorithms/RangeQueries/StaticArrayQueries/#static_array_queries","text":"We first focus on a situation the array is static , i.e., the array values are never updated between the queries.","title":"Static array queries"},{"location":"Algorithms/RangeQueries/StaticArrayQueries/#sum_queries","text":"We can easily process sum queries on a static array by constructing a prefix sum array . we can calculate any value of $sum_q(a, b)$ in $\\Omicron(1)$ time as follows $$ \\text{sum}_q(a, b) = \\text{sum}_q(0, b) - \\text{sum}_q(0, a - 1) $$ By defining $\\text{sum}_q(0, -1) = 0$, the above formula also holds when $a = 0$.","title":"Sum queries"},{"location":"Algorithms/RangeQueries/StaticArrayQueries/#higher_dimensions","text":"It is also possible to generalize this idea to higher dimensions. We can construct a two-dimensional prefix sum array that can be used to calculate the sum of any rectangular subarray in $\\Omicron(1)$ time. Each sum in such a array corresponds to a subarray that begins at the upper-left corner of the array.","title":"Higher dimensions"},{"location":"Algorithms/RangeQueries/StaticArrayQueries/#minimum_queries","text":"Minimum queries are more difficult to process than sum queries. Still, there is a quite simple $Omicron(nlgn)$ time preprocessing method after which we can answer any minimum query in $Omicron(1)$ time. Note that since minimum and maximum queries can be processed similarly, we can focus on minimum queries. The idea is to precalculate all values of $\\text{min}_q(a, b)$ where $b-a+1$(the length of the range) is a power of two. The number of precalculated value is $\\Omicron(nlgn)$, because there are $\\Omicron(lgn)$ range lengths that are powers of two. The values can be calculated efficiently using recursive formula $$ \\text{min}_q(a, b) = \\text{min}(\\text{min}_q(a, a + w -1), \\text{min}_q(a + w, b) ) $$ where $b - a + 1$ is apower of two and $w = (b - a + 1) / 2$. Calculating all those values takes $\\Omicron(nlgn)$ time. After this, any value of $min_q(a, b)$ can be calculated in $\\Omicron(1)$ time as a minimum of two precalculated values. Let $k$ be the largest power of two that does not exceed $ b - a + 1 $. We can calculate the value of $min_q(a, b)$ using the formula $$ \\text{min}_q(a, b) = \\text{min}(\\text{min}_q(a, a + k -1), \\text{min}_q(b - k + 1, b)) $$ In the above formula, the range $[a, b]$ is represented as the union of the ranges $[a, a + k -1 ]$ and $[b - k + 1, b]$, both of length $k$.","title":"Minimum queries"},{"location":"Algorithms/RangeQueries/StaticArrayQueries/#implementation","text":"naive implementation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 int arr [ log ( N ) + 1 ][ N ]; //preprocess for ( int k = 1 ; k <= log ( N ); k ++ ) { int w = ( 1 << ( k - 1 )); for ( int i = 0 ; i < N - w + 1 ; i ++ ) { arr [ k ][ i ] = min ( arr [ k - 1 ][ i ], arr [ k - 1 ][ i + w ]); } } // queries int minq ( int a , int b ) { int w = b - a + 1 ; int k = 0 ; while ( ( 1 << ( k + 1 )) <= w ) k ++ ; return min ( arr [ k ][ a ], arr [ k ][ b - ( 1 << k ) + 1 ]); } improved version 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 /** * File : 2357RMQimproved.cpp * Author : JCHRYS <jchrys@me.com> * Date : 09.09.2019 * Last Modified Date: 09.09.2019 * Last Modified By : JCHRYS <jchrys@me.com> */ #include <bits/stdc++.h> using namespace std ; int const MAXN = 100000 ; int const MAXLOG = 17 ; int log ( int x ) { int ans = - 1 ; while ( x ) { ans ++ ; x >>= 1 ; } return ans ; } int mintable [ MAXLOG ][ MAXN ]; int maxtable [ MAXLOG ][ MAXN ]; int N , M ; void preprocess () { // building base for ( int i = 0 ; i < N ; i ++ ) { cin >> mintable [ 0 ][ i ]; maxtable [ 0 ][ i ] = mintable [ 0 ][ i ]; } for ( int j = 1 ; j <= log ( N ); j ++ ) { for ( int i = 0 ; i + ( 1 << ( j - 1 )) < N ; i ++ ) { mintable [ j ][ i ] = min ( mintable [ j - 1 ][ i ], mintable [ j - 1 ][ i + ( 1 << ( j - 1 ))]); maxtable [ j ][ i ] = max ( maxtable [ j - 1 ][ i ], maxtable [ j - 1 ][ i + ( 1 << ( j - 1 ))]); } } } int minq ( int a , int b ) { int len = b - a + 1 ; int k = log ( len ); return min ( mintable [ k ][ a ], mintable [ k ][ b - ( 1 << k ) + 1 ]); } int maxq ( int a , int b ) { int len = b - a + 1 ; int k = log ( len ); return max ( maxtable [ k ][ a ], maxtable [ k ][ b - ( 1 << k ) + 1 ]); } int main () { cin >> N >> M ; preprocess (); for ( int i = 0 ; i < M ; i ++ ) { int a , b ; cin >> a >> b ; a -- ; b -- ; cout << minq ( a , b ) << ' ' << maxq ( a , b ) << '\\n' ; } return 0 ; }","title":"Implementation"},{"location":"Algorithms/Sort/BubbleSort/","text":"Bubble Sort \u00b6 Bubble Sort C++ \u00b6 1 2 3 4 5 6 7 8 9 template < typename It > void BubbleSort ( It begin , It end ) { if ( begin == end ) return ; //return if container is empty for ( It i = end - 1 ; i != begin ; i -- ) { for ( It j = begin ; j != i ; j ++ ) { if ( * j > * ( j + 1 )) swap ( * j , * ( j + 1 )); } } }","title":"BubbleSort"},{"location":"Algorithms/Sort/BubbleSort/#bubble_sort","text":"Bubble Sort","title":"Bubble Sort"},{"location":"Algorithms/Sort/BubbleSort/#c","text":"1 2 3 4 5 6 7 8 9 template < typename It > void BubbleSort ( It begin , It end ) { if ( begin == end ) return ; //return if container is empty for ( It i = end - 1 ; i != begin ; i -- ) { for ( It j = begin ; j != i ; j ++ ) { if ( * j > * ( j + 1 )) swap ( * j , * ( j + 1 )); } } }","title":"C++"},{"location":"Algorithms/Sort/HeapSort/","text":"Heap Sort \u00b6 Heap is a nearly complete binary tree. we can easily implement on basic array object. Heap Structure satisfies Heap Property Heap Property max-heap-property: Parent's Key $\\geq$ Children's key min-heap-property: Parent's Key $\\leq$ Children's key Implementation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 #include <iostream> using namespace std ; int arr [ 1000000 ]; int heap_size ; template < typename T > void _swap ( T & a , T & b ) { T temp = a ; a = b ; b = temp ; } inline int parent ( int i ) { return ( i - 1 ) >> 1 ; } inline int left ( int i ) { return ( i << 1 ) + 1 ; } inline int right ( int i ) { return ( i << 1 ) + 2 ; } void max_heapify ( int i ) { int largest ; int l = left ( i ); int r = right ( i ); if ( l < heap_size && arr [ l ] > arr [ i ]) { largest = l ; } else { largest = i ; } if ( r < heap_size && arr [ r ] > arr [ largest ]) largest = r ; if ( largest != i ) { _swap ( arr [ largest ], arr [ i ]); max_heapify ( largest ); } } void build_maxheap () { for ( int i = parent ( heap_size - 1 ); i >= 0 ; i -- ) { max_heapify ( i ); } } void heapsort () { build_maxheap (); for ( int i = ( heap_size - 1 ); i >= 0 ; i -- ) { _swap ( arr [ i ], arr [ 0 ]); heap_size -- ; // reduce heap_size at here; max_heapify ( 0 ); } } int main () { ios_base :: sync_with_stdio ( 0 ); cin . tie ( 0 ); cout . tie ( 0 ); int n = 5000 ; //cin >> n; heap_size = n ; for ( int i = 0 ; i < n ; i ++ ) { arr [ i ] = rand () % 5000 ; } heapsort (); for ( int i = 0 ; i < n ; i ++ ) { cout << arr [ i ] << '\\n' ; } return 0 ; } Related Problems \u00b6 Sort","title":"HeapSort"},{"location":"Algorithms/Sort/HeapSort/#heap_sort","text":"Heap is a nearly complete binary tree. we can easily implement on basic array object. Heap Structure satisfies Heap Property Heap Property max-heap-property: Parent's Key $\\geq$ Children's key min-heap-property: Parent's Key $\\leq$ Children's key","title":"Heap Sort"},{"location":"Algorithms/Sort/HeapSort/#implementation","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 #include <iostream> using namespace std ; int arr [ 1000000 ]; int heap_size ; template < typename T > void _swap ( T & a , T & b ) { T temp = a ; a = b ; b = temp ; } inline int parent ( int i ) { return ( i - 1 ) >> 1 ; } inline int left ( int i ) { return ( i << 1 ) + 1 ; } inline int right ( int i ) { return ( i << 1 ) + 2 ; } void max_heapify ( int i ) { int largest ; int l = left ( i ); int r = right ( i ); if ( l < heap_size && arr [ l ] > arr [ i ]) { largest = l ; } else { largest = i ; } if ( r < heap_size && arr [ r ] > arr [ largest ]) largest = r ; if ( largest != i ) { _swap ( arr [ largest ], arr [ i ]); max_heapify ( largest ); } } void build_maxheap () { for ( int i = parent ( heap_size - 1 ); i >= 0 ; i -- ) { max_heapify ( i ); } } void heapsort () { build_maxheap (); for ( int i = ( heap_size - 1 ); i >= 0 ; i -- ) { _swap ( arr [ i ], arr [ 0 ]); heap_size -- ; // reduce heap_size at here; max_heapify ( 0 ); } } int main () { ios_base :: sync_with_stdio ( 0 ); cin . tie ( 0 ); cout . tie ( 0 ); int n = 5000 ; //cin >> n; heap_size = n ; for ( int i = 0 ; i < n ; i ++ ) { arr [ i ] = rand () % 5000 ; } heapsort (); for ( int i = 0 ; i < n ; i ++ ) { cout << arr [ i ] << '\\n' ; } return 0 ; }","title":"Implementation"},{"location":"Algorithms/Sort/HeapSort/#related_problems","text":"Sort","title":"Related Problems"},{"location":"Algorithms/Sort/InsertionSort/","text":"Insertion Sort \u00b6 Insertion Sort is very simple algorithm it works exactly like the way you sort a deck of card C++ \u00b6 1 2 3 4 5 6 7 8 9 10 template < typenme It > // Iterator void insertionSort ( It begin , It end ) { //TODO add comparator if ( begin == end ) return ; // return if container is empty for ( It i = begin ; i != end ; i ++ ) { for ( It j = i ; j != begin ; j -- ) { if ( * ( j - 1 ) < * j ) break ; swap ( * ( j - 1 ), * j ); } } }","title":"InsertionSort"},{"location":"Algorithms/Sort/InsertionSort/#insertion_sort","text":"Insertion Sort is very simple algorithm it works exactly like the way you sort a deck of card","title":"Insertion Sort"},{"location":"Algorithms/Sort/InsertionSort/#c","text":"1 2 3 4 5 6 7 8 9 10 template < typenme It > // Iterator void insertionSort ( It begin , It end ) { //TODO add comparator if ( begin == end ) return ; // return if container is empty for ( It i = begin ; i != end ; i ++ ) { for ( It j = i ; j != begin ; j -- ) { if ( * ( j - 1 ) < * j ) break ; swap ( * ( j - 1 ), * j ); } } }","title":"C++"},{"location":"Algorithms/Sort/QuickSort/","text":"QuickSort \u00b6 Quicksort is often the best practical choice for sorting because it is remarkably efficient on the average: its expected running time is $\\Theta(n\\text{lg}n)$. and the constant factors hidden in the $\\Theta(n\\text{lg}n)$ notation are quite small. It also has the advantage of sorting in place and it works well even in virtual-memory environments. unstable quicksort is unstable sort algorithm stable in sorting algorithm A sorting algorithm is said to be stable if two objects with equal keys appear in the same order in the sorted output as they appear in the unsorted input. Whereas a sorting algorithm is said to be unstable if there are two or more objects with equal keys which don\u2019t appear in same order before and after sorting. Timecomplexity \u00b6 Average-Case Worst-Case $\\Theta(nlgn)$ $\\Omicron(n^2)$ Description \u00b6 Quicksort, like merge sort, applies the divide-and-conquer paradigm. Here is the three-step divide-and-conquer process for sorting a typical subarray $A[p..r]$. 1. Divide \u00b6 Partition (rearrange) the array $A[p..r]$ into two (possibly empty) subarrays $A[p..q-1]$ and $A[q + 1..r]$ such that each element of $A[p.. q - 1]$ is less than or equal to $A[q]$, which is, in turn, less than or equal to each element of $A[q + 1..r]. Compute the index $q$ as part of this partitioning procedure. 2. Conquer \u00b6 Sort the two subarrays $A[p..q - 1]$ and $A[q + 1..r] by recursive calls to quicksort 3. Combine \u00b6 Because the subarrays are already sorted, no work is needed to combine them: the entire array $A[p..r]$ is now sorted Implementations \u00b6 CLRS version \u00b6 SLOW! 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 /** * File : 2571-qSort-clrs.cpp * Author : JCHRYS <jchrys@me.com> * Date : 31.08.2019 * Last Modified Date: 31.08.2019 * Last Modified By : JCHRYS <jchrys@me.com> */ #include <iostream> using namespace std ; template < typename T > void _swap ( T & a , T & b ) { T temp = a ; a = b ; b = temp ; } template < typename T > int partition ( T & A , int p , int r ) { auto pivot = A [ r ]; int i = p - 1 ; for ( int j = p ; j < r ; j ++ ) { if ( A [ j ] <= pivot ) { i ++ ; _swap ( A [ i ], A [ j ]); } } _swap ( A [ i + 1 ], A [ r ]); return i + 1 ; } template < typename T > void qSort ( T & A , int p , int r ) { if ( p < r ) { int q = partition ( A , p , r ); qSort ( A , p , q - 1 ); qSort ( A , q + 1 , r ); } } int arr [ 1000000 ]; int main () { int n = 100 ; ios_base :: sync_with_stdio ( 0 ); cin . tie ( 0 ); cout . tie ( 0 ); for ( int i = 0 ; i < n ; i ++ ) arr [ i ] = rand () % 100 ; qSort ( arr , 0 , n - 1 ); for ( int i = 0 ; i < n ; i ++ ) { cout << arr [ i ] << ' ' ; } return 0 ; } FAST version \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 /** * File : 2571-qSort-fast.cpp * Author : JCHRYS <jchrys@me.com> * Date : 31.08.2019 * Last Modified Date: 31.08.2019 * Last Modified By : JCHRYS <jchrys@me.com> */ #include <iostream> using namespace std ; template < typename T > void _swap ( T & lh , T & rh ) { T temp = lh ; lh = rh ; rh = temp ; } template < typename It , typename Comp > void qSort ( It begin , It end , Comp comp ) { if ( begin == end ) return ; //corner case //pick pivot randomly _swap ( * ( begin ), * (( end - begin ) / 2 + begin )); It pivot = begin ; It left = begin + 1 ; It right = end - 1 ; while ( left <= right ) { while ( left <= right && ! comp ( * right , * pivot )) right -- ; while ( left <= right && ! comp ( * pivot , * left )) left ++ ; if ( left <= right ) _swap ( * left , * right ); } _swap ( * right , * pivot ); qSort ( begin , right , comp ); qSort ( right + 1 , end , comp ); } bool comp ( int const & lh , int const & rh ) { return lh < rh ; } int n = 100 ; int A [ 10000000 ]; int main () { ios_base :: sync_with_stdio ( 0 ); cin . tie ( 0 ); cout . tie ( 0 ); for ( int i = 0 ; i < n ; i ++ ) A [ i ] = rand () % 100 ; qSort ( A , A + n , comp ); for ( int i = 0 ; i < n ; i ++ ) cout << A [ i ] << ' ' ; return 0 ; }","title":"QuickSort"},{"location":"Algorithms/Sort/QuickSort/#quicksort","text":"Quicksort is often the best practical choice for sorting because it is remarkably efficient on the average: its expected running time is $\\Theta(n\\text{lg}n)$. and the constant factors hidden in the $\\Theta(n\\text{lg}n)$ notation are quite small. It also has the advantage of sorting in place and it works well even in virtual-memory environments. unstable quicksort is unstable sort algorithm stable in sorting algorithm A sorting algorithm is said to be stable if two objects with equal keys appear in the same order in the sorted output as they appear in the unsorted input. Whereas a sorting algorithm is said to be unstable if there are two or more objects with equal keys which don\u2019t appear in same order before and after sorting.","title":"QuickSort"},{"location":"Algorithms/Sort/QuickSort/#timecomplexity","text":"Average-Case Worst-Case $\\Theta(nlgn)$ $\\Omicron(n^2)$","title":"Timecomplexity"},{"location":"Algorithms/Sort/QuickSort/#description","text":"Quicksort, like merge sort, applies the divide-and-conquer paradigm. Here is the three-step divide-and-conquer process for sorting a typical subarray $A[p..r]$.","title":"Description"},{"location":"Algorithms/Sort/QuickSort/#1_divide","text":"Partition (rearrange) the array $A[p..r]$ into two (possibly empty) subarrays $A[p..q-1]$ and $A[q + 1..r]$ such that each element of $A[p.. q - 1]$ is less than or equal to $A[q]$, which is, in turn, less than or equal to each element of $A[q + 1..r]. Compute the index $q$ as part of this partitioning procedure.","title":"1. Divide"},{"location":"Algorithms/Sort/QuickSort/#2_conquer","text":"Sort the two subarrays $A[p..q - 1]$ and $A[q + 1..r] by recursive calls to quicksort","title":"2. Conquer"},{"location":"Algorithms/Sort/QuickSort/#3_combine","text":"Because the subarrays are already sorted, no work is needed to combine them: the entire array $A[p..r]$ is now sorted","title":"3. Combine"},{"location":"Algorithms/Sort/QuickSort/#implementations","text":"","title":"Implementations"},{"location":"Algorithms/Sort/QuickSort/#clrs_version","text":"SLOW! 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 /** * File : 2571-qSort-clrs.cpp * Author : JCHRYS <jchrys@me.com> * Date : 31.08.2019 * Last Modified Date: 31.08.2019 * Last Modified By : JCHRYS <jchrys@me.com> */ #include <iostream> using namespace std ; template < typename T > void _swap ( T & a , T & b ) { T temp = a ; a = b ; b = temp ; } template < typename T > int partition ( T & A , int p , int r ) { auto pivot = A [ r ]; int i = p - 1 ; for ( int j = p ; j < r ; j ++ ) { if ( A [ j ] <= pivot ) { i ++ ; _swap ( A [ i ], A [ j ]); } } _swap ( A [ i + 1 ], A [ r ]); return i + 1 ; } template < typename T > void qSort ( T & A , int p , int r ) { if ( p < r ) { int q = partition ( A , p , r ); qSort ( A , p , q - 1 ); qSort ( A , q + 1 , r ); } } int arr [ 1000000 ]; int main () { int n = 100 ; ios_base :: sync_with_stdio ( 0 ); cin . tie ( 0 ); cout . tie ( 0 ); for ( int i = 0 ; i < n ; i ++ ) arr [ i ] = rand () % 100 ; qSort ( arr , 0 , n - 1 ); for ( int i = 0 ; i < n ; i ++ ) { cout << arr [ i ] << ' ' ; } return 0 ; }","title":"CLRS version"},{"location":"Algorithms/Sort/QuickSort/#fast_version","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 /** * File : 2571-qSort-fast.cpp * Author : JCHRYS <jchrys@me.com> * Date : 31.08.2019 * Last Modified Date: 31.08.2019 * Last Modified By : JCHRYS <jchrys@me.com> */ #include <iostream> using namespace std ; template < typename T > void _swap ( T & lh , T & rh ) { T temp = lh ; lh = rh ; rh = temp ; } template < typename It , typename Comp > void qSort ( It begin , It end , Comp comp ) { if ( begin == end ) return ; //corner case //pick pivot randomly _swap ( * ( begin ), * (( end - begin ) / 2 + begin )); It pivot = begin ; It left = begin + 1 ; It right = end - 1 ; while ( left <= right ) { while ( left <= right && ! comp ( * right , * pivot )) right -- ; while ( left <= right && ! comp ( * pivot , * left )) left ++ ; if ( left <= right ) _swap ( * left , * right ); } _swap ( * right , * pivot ); qSort ( begin , right , comp ); qSort ( right + 1 , end , comp ); } bool comp ( int const & lh , int const & rh ) { return lh < rh ; } int n = 100 ; int A [ 10000000 ]; int main () { ios_base :: sync_with_stdio ( 0 ); cin . tie ( 0 ); cout . tie ( 0 ); for ( int i = 0 ; i < n ; i ++ ) A [ i ] = rand () % 100 ; qSort ( A , A + n , comp ); for ( int i = 0 ; i < n ; i ++ ) cout << A [ i ] << ' ' ; return 0 ; }","title":"FAST version"},{"location":"Algorithms/String/Preface/","text":"String Algorithms \u00b6 Pattern matching problem The pattern matching problem can be easily solved in $\\Omicron(nm)$ time by a brute force algorithm that tests all positions where the pattern may occur in the string. However, in this chapter, we will se that there are more efficient algorithms that require only $\\Omicron(n+m)$ time. Terminologies \u00b6 Alphabet: The set of characters that may appear in the string is called an alphabet. Substring: A substring is a sequence of consecutive characters in a string. Subsequence: A subsequence is a sequence of (not necessarily consecutive) characters in a string in their original order Prefix: A prefix is a substring that starts at the beginning of a string. Suffix: A suffix is a substring that ends at the end of a string. Rotation: A rotation can be generate by moving the characters of a string one by one from the beginning to the end(or vice versa) Period: A preiod is a prefix of a string such that the string can be constructed by repeating the period. Border: A border is a string that is both a prefix and a suffix of a string. Lexicographical order: todo","title":"Preface"},{"location":"Algorithms/String/Preface/#string_algorithms","text":"Pattern matching problem The pattern matching problem can be easily solved in $\\Omicron(nm)$ time by a brute force algorithm that tests all positions where the pattern may occur in the string. However, in this chapter, we will se that there are more efficient algorithms that require only $\\Omicron(n+m)$ time.","title":"String Algorithms"},{"location":"Algorithms/String/Preface/#terminologies","text":"Alphabet: The set of characters that may appear in the string is called an alphabet. Substring: A substring is a sequence of consecutive characters in a string. Subsequence: A subsequence is a sequence of (not necessarily consecutive) characters in a string in their original order Prefix: A prefix is a substring that starts at the beginning of a string. Suffix: A suffix is a substring that ends at the end of a string. Rotation: A rotation can be generate by moving the characters of a string one by one from the beginning to the end(or vice versa) Period: A preiod is a prefix of a string such that the string can be constructed by repeating the period. Border: A border is a string that is both a prefix and a suffix of a string. Lexicographical order: todo","title":"Terminologies"},{"location":"Algorithms/String/StringHashing/","text":"String hashing \u00b6 String Hashing is a technique that allows us to efficiently check whether two strings are equal. The idea in string hashing is to compare hash values of strings instead of their individual characters. Time complexity \u00b6 Calculating hash values \u00b6 A Hash value of a string is a number that is calculated from the characters of the string. A usual way to implement string hashing is polynomial hashing , which means that the hash value of a string $s$ of length $n$ is $$ (s[0]A^{n-1} + s[1]A^{n-2} + \\dotsm + s[n-1]A^0) \\text{mod }B \\\\ \\text{where } A \\text{ and } B \\text{ are pre-chosen constants}. $$ Preprocessing \u00b6 Using polynomial hashing, we can calculate the hashvalue of any substring of a string s in $\\Omicron(1)$ time after an $\\Omicron(n)$ time preprocessing. The idea is to construct an array $h$ contains the hash value of the prefix $s[0\\dots k]$. The array values can be recursively calculated as follows: $$ \\begin{aligned} h[0] &= s[0] \\\\ h[k] &= (h[k-1]A + s[k]) \\text{mod } B \\end{aligned} $$ In addition, we construct array $p$ where $p[k]=A^k \\text{mod } B$. $$ \\begin{aligned} p[0] &= 1 p[k] &= (p[k-1]A) \\text{mod } B \\end{aligned} $$ Constructing these arrays takes $\\Omicron(n)$ time. After this, the hash value of any substring $s[a\\dots b]$ can be calculated in $\\Omicron(1)$ time using the formula $$ (h[b] - h[a]p[b-a+1]) \\text{mod } B $$ assuming that a > 0. If a = 0, the hash value is simply $h[b]$. Collisions and parameters \u00b6 An evident risk when comparing hash values is a collision, which means that two strings have different contents but equal hash values. collisions are always possible, because the number of different strings is larger than the nubmer of different hash values. However, the probability of a collision is small if the constants $A$ and $B$ are carefully chosen. A usual way is to choose random constants near $10^9$ Implementation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 /** * File : hashstring.cpp * Author : JCHRYS <jchrys@me.com> * Date : 02.09.2019 * Last Modified Date: 02.09.2019 * Last Modified By : JCHRYS <jchrys@me.com> */ #include <bits/stdc++.h> using namespace std ; using ll = long long ; string s = \"ALLEY\" ; ll A = 3 ; ll B = 97 ; ll h [ 1000 ]; ll p [ 1000 ]; void build ( string s ) { for ( int k = 0 ; k < ( int ) s . size (); k ++ ) { if ( k == 0 ) { h [ k ] = s [ 0 ] % B ; p [ 0 ] = 1 ; } else { h [ k ] = ( h [ k - 1 ] * A + s [ k ]) % B ; p [ k ] = ( p [ k - 1 ] * A ) % B ; } } } ll get_hash_value ( int a , int b ) { if ( a == 0 ) { return h [ b ]; } else { return ( h [ b ] - ( h [ a - 1 ] * p [ b - a + 1 ])) % B ; } } int main () { build ( s ); for ( int i = 0 ; i < ( int ) s . size (); i ++ ) { cout << h [ i ] << ' ' ; } cout << endl ; for ( int i = 0 ; i < ( int ) s . size (); i ++ ) { cout << p [ i ] << ' ' ; } return 0 ; }","title":"String Hashing"},{"location":"Algorithms/String/StringHashing/#string_hashing","text":"String Hashing is a technique that allows us to efficiently check whether two strings are equal. The idea in string hashing is to compare hash values of strings instead of their individual characters.","title":"String hashing"},{"location":"Algorithms/String/StringHashing/#time_complexity","text":"","title":"Time complexity"},{"location":"Algorithms/String/StringHashing/#calculating_hash_values","text":"A Hash value of a string is a number that is calculated from the characters of the string. A usual way to implement string hashing is polynomial hashing , which means that the hash value of a string $s$ of length $n$ is $$ (s[0]A^{n-1} + s[1]A^{n-2} + \\dotsm + s[n-1]A^0) \\text{mod }B \\\\ \\text{where } A \\text{ and } B \\text{ are pre-chosen constants}. $$","title":"Calculating hash values"},{"location":"Algorithms/String/StringHashing/#preprocessing","text":"Using polynomial hashing, we can calculate the hashvalue of any substring of a string s in $\\Omicron(1)$ time after an $\\Omicron(n)$ time preprocessing. The idea is to construct an array $h$ contains the hash value of the prefix $s[0\\dots k]$. The array values can be recursively calculated as follows: $$ \\begin{aligned} h[0] &= s[0] \\\\ h[k] &= (h[k-1]A + s[k]) \\text{mod } B \\end{aligned} $$ In addition, we construct array $p$ where $p[k]=A^k \\text{mod } B$. $$ \\begin{aligned} p[0] &= 1 p[k] &= (p[k-1]A) \\text{mod } B \\end{aligned} $$ Constructing these arrays takes $\\Omicron(n)$ time. After this, the hash value of any substring $s[a\\dots b]$ can be calculated in $\\Omicron(1)$ time using the formula $$ (h[b] - h[a]p[b-a+1]) \\text{mod } B $$ assuming that a > 0. If a = 0, the hash value is simply $h[b]$.","title":"Preprocessing"},{"location":"Algorithms/String/StringHashing/#collisions_and_parameters","text":"An evident risk when comparing hash values is a collision, which means that two strings have different contents but equal hash values. collisions are always possible, because the number of different strings is larger than the nubmer of different hash values. However, the probability of a collision is small if the constants $A$ and $B$ are carefully chosen. A usual way is to choose random constants near $10^9$","title":"Collisions and parameters"},{"location":"Algorithms/String/StringHashing/#implementation","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 /** * File : hashstring.cpp * Author : JCHRYS <jchrys@me.com> * Date : 02.09.2019 * Last Modified Date: 02.09.2019 * Last Modified By : JCHRYS <jchrys@me.com> */ #include <bits/stdc++.h> using namespace std ; using ll = long long ; string s = \"ALLEY\" ; ll A = 3 ; ll B = 97 ; ll h [ 1000 ]; ll p [ 1000 ]; void build ( string s ) { for ( int k = 0 ; k < ( int ) s . size (); k ++ ) { if ( k == 0 ) { h [ k ] = s [ 0 ] % B ; p [ 0 ] = 1 ; } else { h [ k ] = ( h [ k - 1 ] * A + s [ k ]) % B ; p [ k ] = ( p [ k - 1 ] * A ) % B ; } } } ll get_hash_value ( int a , int b ) { if ( a == 0 ) { return h [ b ]; } else { return ( h [ b ] - ( h [ a - 1 ] * p [ b - a + 1 ])) % B ; } } int main () { build ( s ); for ( int i = 0 ; i < ( int ) s . size (); i ++ ) { cout << h [ i ] << ' ' ; } cout << endl ; for ( int i = 0 ; i < ( int ) s . size (); i ++ ) { cout << p [ i ] << ' ' ; } return 0 ; }","title":"Implementation"},{"location":"Algorithms/String/Trie/","text":"Trie \u00b6 A trie is a rooted tree that maintains a set of strings. Each string in the set is stored as a chain of characters that starts at the root. If two string have a common prefix, tey also have a common chain in the tree. Using a trie, we can find the longest prefix of a given string such that the prefix belongs to the set. Moreover, by storing additional information in each node, we can calculate the number of strings that belong to the set and have a given string as a prefix. it's advantage is, LCP(Longest Common Prefix) of two of these strings is the LCA(Lowest Common Ancestor) of their node in the trie(a node that we can build the string by writing down the characters in the path from the root to that node). Time Complexity \u00b6 operation time complexity contains(string) $\\Omicron(string.size())$ add(string) $\\Omicron(string.size())$ Implementation \u00b6 A trie can be stored in an array 1 int trie [ N ][ A ]; where $N$ is the maximum number of nodes(the maximum total length of the strings in the set) and $A$ is the size of the alphabet. The nodes of a trie are numbered 0, 1, 2, ... so that the number of the root is 0, and trie[s][c] is the next node in the chain when we move from node $s$ using character $c$. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 int trie [ N ][ A ]; //initially all the number in trie are -1; int next = 1 ; void add ( string s ) { int i = 0 ; v = 0 ; while ( i < s . size ()) { if ( trie [ v ][ s [ i ]]) == - 1 ) { v = trie [ v ][ s [ i ]] = next ; ++ i ; ++ next ; } else { v = trie [ v ][ s [ i ]]; ++ i ; } } } Problems \u00b6 A Lot of Games","title":"Trie"},{"location":"Algorithms/String/Trie/#trie","text":"A trie is a rooted tree that maintains a set of strings. Each string in the set is stored as a chain of characters that starts at the root. If two string have a common prefix, tey also have a common chain in the tree. Using a trie, we can find the longest prefix of a given string such that the prefix belongs to the set. Moreover, by storing additional information in each node, we can calculate the number of strings that belong to the set and have a given string as a prefix. it's advantage is, LCP(Longest Common Prefix) of two of these strings is the LCA(Lowest Common Ancestor) of their node in the trie(a node that we can build the string by writing down the characters in the path from the root to that node).","title":"Trie"},{"location":"Algorithms/String/Trie/#time_complexity","text":"operation time complexity contains(string) $\\Omicron(string.size())$ add(string) $\\Omicron(string.size())$","title":"Time Complexity"},{"location":"Algorithms/String/Trie/#implementation","text":"A trie can be stored in an array 1 int trie [ N ][ A ]; where $N$ is the maximum number of nodes(the maximum total length of the strings in the set) and $A$ is the size of the alphabet. The nodes of a trie are numbered 0, 1, 2, ... so that the number of the root is 0, and trie[s][c] is the next node in the chain when we move from node $s$ using character $c$. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 int trie [ N ][ A ]; //initially all the number in trie are -1; int next = 1 ; void add ( string s ) { int i = 0 ; v = 0 ; while ( i < s . size ()) { if ( trie [ v ][ s [ i ]]) == - 1 ) { v = trie [ v ][ s [ i ]] = next ; ++ i ; ++ next ; } else { v = trie [ v ][ s [ i ]]; ++ i ; } } }","title":"Implementation"},{"location":"Algorithms/String/Trie/#problems","text":"A Lot of Games","title":"Problems"},{"location":"Algorithms/String/Z-Algorithm/","text":"Z-algorithm \u00b6 The Z-array $z$ of a string $s$ of length $n$ contains for each $k=0,1,\\dots,n-1$ the length of longest substring of s that begins at position $k$ and is a prefix of $s$. Thus, $z[k] = p$ tells us that $s[0\\dots p-1]$ equals $s[k\\dots k + p -1]$. Many string processing problems can be efficiently solved using Z-array. Algorithm description \u00b6 Next we describe an algorithm, called the Z-algorithm, that efficiently constructs the Z-array in $\\Omicron(n)$ time. The algorithm calculates the Z-array values from left to right by both using information already stored in the Z-array and comparing substrings character by character. To efficiently calculate the Z-array values, the algorithm maintains a range $[x, y]$ such that $s[x \\dots y]$ are equal, we can use this information when calculating Z-values for positions $x + 1, x + 2, \\dots, y$. At each poisition $k$, we first check the value of $z[k-x]$. if $k + z[k - x] < y$, we know that $z[k] = z[k - x]$. However, if $k + z[k - x] >= y, s[0\\dots y-k]$ equals $s[k \\dots y],$ and to determine the value of $z[k]$ we need to compare the substring character by character. Still, the algorithm works in $\\Omicron(n)$ time, because we start comparing at positions $y - k + 1$ and $y + 1$. Using Z-array \u00b6 It if often a matter of taste whether to use string hashing or the Z-algorithm. Unlike hashing, the Z-algorithm always works and there is no risk for collisions. On the other hand, Z-algorithm is more difficult to implement and some problems can only be solved using hashing. Implementation \u00b6 Here is a short impelmentation of the Z-algorithm that returns a vector that corresponds to the Z-array. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 vector < int > z ( string s ) { int n = s . size (); vector < int > z ( n ); int x = 0 , y = 0 ; for ( int i = 1 ; i < n ; i ++ ) { z [ i ] = max ( 0 , min ( z [ i - x ], y - i + 1 )); while ( i + z [ i ] < n && s [ z [ i ]] == s [ i + z [ i ]]) { x = i ; y = i + z [ i ]; z [ i ] ++ ; } } return z ; }","title":"Z-Algorithm"},{"location":"Algorithms/String/Z-Algorithm/#z-algorithm","text":"The Z-array $z$ of a string $s$ of length $n$ contains for each $k=0,1,\\dots,n-1$ the length of longest substring of s that begins at position $k$ and is a prefix of $s$. Thus, $z[k] = p$ tells us that $s[0\\dots p-1]$ equals $s[k\\dots k + p -1]$. Many string processing problems can be efficiently solved using Z-array.","title":"Z-algorithm"},{"location":"Algorithms/String/Z-Algorithm/#algorithm_description","text":"Next we describe an algorithm, called the Z-algorithm, that efficiently constructs the Z-array in $\\Omicron(n)$ time. The algorithm calculates the Z-array values from left to right by both using information already stored in the Z-array and comparing substrings character by character. To efficiently calculate the Z-array values, the algorithm maintains a range $[x, y]$ such that $s[x \\dots y]$ are equal, we can use this information when calculating Z-values for positions $x + 1, x + 2, \\dots, y$. At each poisition $k$, we first check the value of $z[k-x]$. if $k + z[k - x] < y$, we know that $z[k] = z[k - x]$. However, if $k + z[k - x] >= y, s[0\\dots y-k]$ equals $s[k \\dots y],$ and to determine the value of $z[k]$ we need to compare the substring character by character. Still, the algorithm works in $\\Omicron(n)$ time, because we start comparing at positions $y - k + 1$ and $y + 1$.","title":"Algorithm description"},{"location":"Algorithms/String/Z-Algorithm/#using_z-array","text":"It if often a matter of taste whether to use string hashing or the Z-algorithm. Unlike hashing, the Z-algorithm always works and there is no risk for collisions. On the other hand, Z-algorithm is more difficult to implement and some problems can only be solved using hashing.","title":"Using Z-array"},{"location":"Algorithms/String/Z-Algorithm/#implementation","text":"Here is a short impelmentation of the Z-algorithm that returns a vector that corresponds to the Z-array. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 vector < int > z ( string s ) { int n = s . size (); vector < int > z ( n ); int x = 0 , y = 0 ; for ( int i = 1 ; i < n ; i ++ ) { z [ i ] = max ( 0 , min ( z [ i - x ], y - i + 1 )); while ( i + z [ i ] < n && s [ z [ i ]] == s [ i + z [ i ]]) { x = i ; y = i + z [ i ]; z [ i ] ++ ; } } return z ; }","title":"Implementation"},{"location":"Algorithms/Tree/Basics/","text":"Tree \u00b6 A tree is a connected, acyclic graph that consist of $n$ nodes and $n-1$ edges. Removing any edges from a tree divides it into two components , and adding any edge to a tree creates a cycle . Moreover, there is always a unique path between any two nodes of a tree. Leaves \u00b6 leaves of a tree are the nodes with degree 1, i.e., with only one neighbor Rooted tree \u00b6 In a rooted tree, one of the nodes is appointed the root of the tree, and all other nodes are placed underneath the root. Children & Parent of a node \u00b6 In a rooted tree, the children of a node are its lower negibors, and the parent of a node is its upper neighbor. Recursive structure \u00b6 The structure of rooted tree is recursive each node of the tree ats as the root of subtree that contains the node itself and all nodes that are in the subtrees of its children","title":"Basics"},{"location":"Algorithms/Tree/Basics/#tree","text":"A tree is a connected, acyclic graph that consist of $n$ nodes and $n-1$ edges. Removing any edges from a tree divides it into two components , and adding any edge to a tree creates a cycle . Moreover, there is always a unique path between any two nodes of a tree.","title":"Tree"},{"location":"Algorithms/Tree/Basics/#leaves","text":"leaves of a tree are the nodes with degree 1, i.e., with only one neighbor","title":"Leaves"},{"location":"Algorithms/Tree/Basics/#rooted_tree","text":"In a rooted tree, one of the nodes is appointed the root of the tree, and all other nodes are placed underneath the root.","title":"Rooted tree"},{"location":"Algorithms/Tree/Basics/#children_parent_of_a_node","text":"In a rooted tree, the children of a node are its lower negibors, and the parent of a node is its upper neighbor.","title":"Children &amp; Parent of a node"},{"location":"Algorithms/Tree/Basics/#recursive_structure","text":"The structure of rooted tree is recursive each node of the tree ats as the root of subtree that contains the node itself and all nodes that are in the subtrees of its children","title":"Recursive structure"},{"location":"Algorithms/Tree/DP/","text":"Tree Dynamic Programming \u00b6 Dynamic programming can be used to calculate some information during a tree traversal. Time Complexities \u00b6 Not yet added The number of nodes in its subtree \u00b6 The subtree contains the node itself and all nodes in the subtrees of its children. so we can calculate the number of nodes recursively using the following code Time complexity: $\\Omicron(n)$ 1 2 3 4 5 6 7 8 9 10 int count [ # nodes ]; void dfs ( int s , int e ) { //current node s, previous node e; count [ s ] = 1 ; for ( auto u : adj [ s ]) { if ( u == e ) continue ; dfs ( u , s ); count [ s ] += count [ u ]; } } Diameter \u00b6 The Diameter of a tree is the maximum length of a path between two nodes. Algorithm 1 (based on DP) \u00b6 A general way to approach many tree problems is to first root the tree arbitrarily . After this, we can try to solve the problem separately for each subtree. Our first algorithm for calculating the diameter is based on this idea. An important observation is that every path in a rooted tree has a highest point : the highest node that belongs to the path. Thus we can calculate for each node the length of the longest path whose heighest point is the node. One of those path corresponds to the diameter of the tree. We calculate for each node $x$ two values: - toLeaf(x): the maximum length of a path from x to any leaf - maxLength(x): the maximum length of a path whose highest point is $x$ $f(x)$: Longest path starts from node $x$ and goes into its subtree. $g(x)$: Longest path starts in subtree of $x$, passes through $x$ and ends in subtree of $x$ If for all nodes $x$, we take maximum of $f(x), g(x)$, then we can get the diameter. Dynamic programming can be used to calculate the above values for all nodes in $\\Omicron(n)$ time. Implementation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 //adjacency list //adj[i] contains all neighbors of i vector < int > adj [ N ]; //functions as defined above int f [ N ], g [ N ], diameter ; // pV is parent of node V void dfs ( int V , int pV ) { //this vector will store f for all children of V vector < int > fValues ; //traverse over all children for ( auto v : adj [ V ]) { if ( v == pV ) continue ; dfs ( v , V ); fValues . push_back ( f [ v ]); } //sort to get top two values // you can also get top two values without sorting in O(N) // current complexity is n lg n sort ( fValues . begin (), fValues . end ()); f [ V ] = 1 ; if ( ! fValues . empty ()) f [ V ] += fValues . back (); if ( fValues . size () >= 2 ) g [ V ] = 2 + fValues . back () + fValues [ fValues . size () - 2 ]; diameter = max ( diameter , max ( f [ V ], g [ V ])); } More General Implementation \u00b6 with weighted edges 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 //adjacency list //adj[i] contains all neighbors of i, and weights to i -> its neighbor vector < vector < pair < int , int >>> adj [ N ]; int diameter = 0 ; void dfs ( int V , int pV ) { vector < int > fValues ; for ( auto v : adj [ V ]) { if ( v . first == pV ) continue ; dfs ( v . first , V ); fValues . push_back ( f [ v . first ] + v . second ); // fvalue of child + weight of edge; } int a = - 1 , b = - 1 ; // a is biggest, b is second to biggest for ( auto x : fValues ) { if ( x > a ) { b = a ; a = x ; } else if ( x > b ) { b = x ; } } f [ V ] = 0 ; if ( a > 0 ) f [ V ] = a ; if ( a > 0 && b > 0 ) g [ V ] = a + b ; diameter = max ( diameter , max ( f [ V ], g [ V ])); } Algorithm 2 (based on DFS) \u00b6 Another efficient way to calculate the diameter of a tree is based on two depth-first searches. First, we choose an arbitrary node $a$ in the tree and find the farthest node $b$ from $a$. Then, we find the farthest node $c$ from $b$. The diameter of the tree is the distance between $b$ and $c$. How this works? \u00b6 Not yet added implementation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 int firstFar ; int diameter = 0 ; void dfs ( int V , int pV , int dis ) { if ( diameter < dis ) { diameter = dis ; firstFar = V ; } for ( auto v : adj [ V ]) { if ( v . first == pV ) continue ; dfs ( v . first , V , dis + v . second ); // v.first == neighbor node, v.second weight of edge } } dfs ( 0 , 0 , 0 ) //initial call // diameter = 0; // not necessary; dfs ( firstFar , 0 , 0 ); // now diameter is diameter of tree resources \u00b6 https://codeforces.com/blog/entry/20935","title":"DP"},{"location":"Algorithms/Tree/DP/#tree_dynamic_programming","text":"Dynamic programming can be used to calculate some information during a tree traversal.","title":"Tree Dynamic Programming"},{"location":"Algorithms/Tree/DP/#time_complexities","text":"Not yet added","title":"Time Complexities"},{"location":"Algorithms/Tree/DP/#the_number_of_nodes_in_its_subtree","text":"The subtree contains the node itself and all nodes in the subtrees of its children. so we can calculate the number of nodes recursively using the following code Time complexity: $\\Omicron(n)$ 1 2 3 4 5 6 7 8 9 10 int count [ # nodes ]; void dfs ( int s , int e ) { //current node s, previous node e; count [ s ] = 1 ; for ( auto u : adj [ s ]) { if ( u == e ) continue ; dfs ( u , s ); count [ s ] += count [ u ]; } }","title":"The number of nodes in its subtree"},{"location":"Algorithms/Tree/DP/#diameter","text":"The Diameter of a tree is the maximum length of a path between two nodes.","title":"Diameter"},{"location":"Algorithms/Tree/DP/#algorithm_1_based_on_dp","text":"A general way to approach many tree problems is to first root the tree arbitrarily . After this, we can try to solve the problem separately for each subtree. Our first algorithm for calculating the diameter is based on this idea. An important observation is that every path in a rooted tree has a highest point : the highest node that belongs to the path. Thus we can calculate for each node the length of the longest path whose heighest point is the node. One of those path corresponds to the diameter of the tree. We calculate for each node $x$ two values: - toLeaf(x): the maximum length of a path from x to any leaf - maxLength(x): the maximum length of a path whose highest point is $x$ $f(x)$: Longest path starts from node $x$ and goes into its subtree. $g(x)$: Longest path starts in subtree of $x$, passes through $x$ and ends in subtree of $x$ If for all nodes $x$, we take maximum of $f(x), g(x)$, then we can get the diameter. Dynamic programming can be used to calculate the above values for all nodes in $\\Omicron(n)$ time.","title":"Algorithm 1 (based on DP)"},{"location":"Algorithms/Tree/DP/#implementation","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 //adjacency list //adj[i] contains all neighbors of i vector < int > adj [ N ]; //functions as defined above int f [ N ], g [ N ], diameter ; // pV is parent of node V void dfs ( int V , int pV ) { //this vector will store f for all children of V vector < int > fValues ; //traverse over all children for ( auto v : adj [ V ]) { if ( v == pV ) continue ; dfs ( v , V ); fValues . push_back ( f [ v ]); } //sort to get top two values // you can also get top two values without sorting in O(N) // current complexity is n lg n sort ( fValues . begin (), fValues . end ()); f [ V ] = 1 ; if ( ! fValues . empty ()) f [ V ] += fValues . back (); if ( fValues . size () >= 2 ) g [ V ] = 2 + fValues . back () + fValues [ fValues . size () - 2 ]; diameter = max ( diameter , max ( f [ V ], g [ V ])); }","title":"Implementation"},{"location":"Algorithms/Tree/DP/#more_general_implementation","text":"with weighted edges 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 //adjacency list //adj[i] contains all neighbors of i, and weights to i -> its neighbor vector < vector < pair < int , int >>> adj [ N ]; int diameter = 0 ; void dfs ( int V , int pV ) { vector < int > fValues ; for ( auto v : adj [ V ]) { if ( v . first == pV ) continue ; dfs ( v . first , V ); fValues . push_back ( f [ v . first ] + v . second ); // fvalue of child + weight of edge; } int a = - 1 , b = - 1 ; // a is biggest, b is second to biggest for ( auto x : fValues ) { if ( x > a ) { b = a ; a = x ; } else if ( x > b ) { b = x ; } } f [ V ] = 0 ; if ( a > 0 ) f [ V ] = a ; if ( a > 0 && b > 0 ) g [ V ] = a + b ; diameter = max ( diameter , max ( f [ V ], g [ V ])); }","title":"More General Implementation"},{"location":"Algorithms/Tree/DP/#algorithm_2_based_on_dfs","text":"Another efficient way to calculate the diameter of a tree is based on two depth-first searches. First, we choose an arbitrary node $a$ in the tree and find the farthest node $b$ from $a$. Then, we find the farthest node $c$ from $b$. The diameter of the tree is the distance between $b$ and $c$.","title":"Algorithm 2 (based on DFS)"},{"location":"Algorithms/Tree/DP/#how_this_works","text":"Not yet added","title":"How this works?"},{"location":"Algorithms/Tree/DP/#implementation_1","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 int firstFar ; int diameter = 0 ; void dfs ( int V , int pV , int dis ) { if ( diameter < dis ) { diameter = dis ; firstFar = V ; } for ( auto v : adj [ V ]) { if ( v . first == pV ) continue ; dfs ( v . first , V , dis + v . second ); // v.first == neighbor node, v.second weight of edge } } dfs ( 0 , 0 , 0 ) //initial call // diameter = 0; // not necessary; dfs ( firstFar , 0 , 0 ); // now diameter is diameter of tree","title":"implementation"},{"location":"Algorithms/Tree/DP/#resources","text":"https://codeforces.com/blog/entry/20935","title":"resources"},{"location":"Algorithms/Tree/SpanningTree/","text":"Spanning Trees \u00b6 A spanning tree of a graph consists of all nodes of the graph and some of the edges of the graph so that there is a path between any two nodes. Like trees in general, spanning trees are connected and acyclic. Usually there are several ways to construct a spanning tree. Note that a graph may have several minimum and maximum spanning trees, so the trees ar not unique. It turns out that several greedy methods can be used to construct minimum and maximum spanning trees. terminologies \u00b6 Weight of spanning tree: sum of its edge weights. Minimum spanning tree: a spanning tree whose weight is as small as possible Kruskal's Algorithms \u00b6 The initial spanning tree only contains the nodes of the graph and does not contain any edges. Then the algorithm goes through edges ordered by their weights, and always adds an edge to the tree if it does not create a cycle. The algorithm maintains the components of the tree. Initially each node of the graph belongs to a separate component. Always when an edge is added to the tree, two components are joined. Finally, all nodes belong to the same component and a minimum spanning tree has been found Implementation \u00b6 It's convinient to use the edge list representation 1 2 3 4 5 6 vector < pair < int u , int v >> edges ; //edge list sort ( vector . begin (), vector . end ()); for ( auto edge : edges ) { //using union find structure if ( ! same ( a , b )) unite ( a , b ); } efficiency The problem is how to efficiently implement the function same and unite . One possibility is to implement function same as a graph traversal and check if we can get from node a to node b . However, the time complexity of such a function would be $\\Omicron(n+m)$ and resulting algorithm would be slow, because the function same will be called for each edge in graph. Union find structure \u00b6 Using a Union find structure implements both $same$ and $unite$ functions in $\\Omicron(lg(n))$ time. thus the time complexity of Kruskal's algorithm will be $\\Omicron(mlg(n))$ Structure \u00b6 In a union-find structure, one element in each set is the representative of the set, and there is a chain from any other element of the set to the representative. The efficiency of the union-find structure depends on how the sets are joined. It turns out that we can follow a simple strategy: always connect the representative the smaller set to the representative of larger set . Using this strategy, the length of any chain will be $\\Omicron(lg(n))$ Implementation \u00b6 The union-find structure can be implemented using arrays. link contains for each element the next element in the chain or the element it self if it is representative. and the array size indicates for each representative the size of thecorresponding set. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 for ( int i = 1 ; i <= n ; i ++ ) link [ i ] = i ; for ( int i = 1 ; i <= n ; i ++ ) size [ i ] = 1 ; int find ( int x ) { // returns the representative for an element x. while ( x != link [ x ]) x = link [ x ]; return x ; } bool same ( int a , int b ) { // returns whether elements a and b belong to the same set return find ( a ) == find ( b ); } void unite ( int a , int b ) { // joins the set that contains elements a and b // it connects the smaller set to the larger set a = find ( a ); b = find ( b ); if ( size [ a ] < size [ b ]) swap ( a , b ); size [ a ] += size [ b ]; link [ b ] = a ; } Prim's algorithm \u00b6 The algorithm first adds an arbitrary node to the tree. After this, the algorithm always choose a minimum-weight edge that adds a new node to the tree. Finally all nodes have been added to the tree and a minimum tree has been found Prim's algorithm resembles Dijkstra's algorithm . But, Prim's algorithm simply selects the minimum weight edge that adds a new node to the tree. Implementation \u00b6 Like Dijkstra's algorithm , Prim's algorithm can be efficiently implemented using a priority queue. The priority queue should contain all nodes that can be conneted to the current component using a single edge, in increasing order of the weights of the corresponding edges. The time complexity of Prim's algorithm is $\\Omicron(n+ mlg(m))$ that equals the time complexity of Dijkstra's algorithm. most competitive programmers use Kruskal's algorithm. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 int V ; // #nodes(vertices) int E ; // #edges int const INF = 2e9 ; vector < vector < pair < int , int >>> adj ; //adjacency list using node = pair < int , int > ; // pair<key, node> representation. void Prim () { priority_queue < node , vector < node > , greater < node >> pq ; //mean-heap implementation using stl int starter = 0 ; // starting node or initial node. vector < int > key ( V , INF ); vector < int > parent ( V , - 1 ); vector < bool > inMST ( V , false ); pq . push ({ 0 , starter }); int MST_weight = 0 ; while ( ! pq . empty ()) { int w = pq . top (). first ; // minimum weight to add a new vertice to MST int u = pq . top (). second ; // new vertice pq . pop (); if ( inMST [ u ]) continue ; // if u already in MST continue; inMST [ u ] = true ; // else add u to MST; MST_weight += w ; for ( auto vw : adj [ u ]) { int v = vw . first ; int w = vw . second ; if ( inMST [ v ] == false && key [ v ] > w ) { // only when new key(weight) value of v is less than current value key [ v ] = w ; pq . push ({ key [ v ], v }); parent [ v ] = u ; } } } }","title":"SpanningTree"},{"location":"Algorithms/Tree/SpanningTree/#spanning_trees","text":"A spanning tree of a graph consists of all nodes of the graph and some of the edges of the graph so that there is a path between any two nodes. Like trees in general, spanning trees are connected and acyclic. Usually there are several ways to construct a spanning tree. Note that a graph may have several minimum and maximum spanning trees, so the trees ar not unique. It turns out that several greedy methods can be used to construct minimum and maximum spanning trees.","title":"Spanning Trees"},{"location":"Algorithms/Tree/SpanningTree/#terminologies","text":"Weight of spanning tree: sum of its edge weights. Minimum spanning tree: a spanning tree whose weight is as small as possible","title":"terminologies"},{"location":"Algorithms/Tree/SpanningTree/#kruskals_algorithms","text":"The initial spanning tree only contains the nodes of the graph and does not contain any edges. Then the algorithm goes through edges ordered by their weights, and always adds an edge to the tree if it does not create a cycle. The algorithm maintains the components of the tree. Initially each node of the graph belongs to a separate component. Always when an edge is added to the tree, two components are joined. Finally, all nodes belong to the same component and a minimum spanning tree has been found","title":"Kruskal's Algorithms"},{"location":"Algorithms/Tree/SpanningTree/#implementation","text":"It's convinient to use the edge list representation 1 2 3 4 5 6 vector < pair < int u , int v >> edges ; //edge list sort ( vector . begin (), vector . end ()); for ( auto edge : edges ) { //using union find structure if ( ! same ( a , b )) unite ( a , b ); } efficiency The problem is how to efficiently implement the function same and unite . One possibility is to implement function same as a graph traversal and check if we can get from node a to node b . However, the time complexity of such a function would be $\\Omicron(n+m)$ and resulting algorithm would be slow, because the function same will be called for each edge in graph.","title":"Implementation"},{"location":"Algorithms/Tree/SpanningTree/#union_find_structure","text":"Using a Union find structure implements both $same$ and $unite$ functions in $\\Omicron(lg(n))$ time. thus the time complexity of Kruskal's algorithm will be $\\Omicron(mlg(n))$","title":"Union find structure"},{"location":"Algorithms/Tree/SpanningTree/#structure","text":"In a union-find structure, one element in each set is the representative of the set, and there is a chain from any other element of the set to the representative. The efficiency of the union-find structure depends on how the sets are joined. It turns out that we can follow a simple strategy: always connect the representative the smaller set to the representative of larger set . Using this strategy, the length of any chain will be $\\Omicron(lg(n))$","title":"Structure"},{"location":"Algorithms/Tree/SpanningTree/#implementation_1","text":"The union-find structure can be implemented using arrays. link contains for each element the next element in the chain or the element it self if it is representative. and the array size indicates for each representative the size of thecorresponding set. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 for ( int i = 1 ; i <= n ; i ++ ) link [ i ] = i ; for ( int i = 1 ; i <= n ; i ++ ) size [ i ] = 1 ; int find ( int x ) { // returns the representative for an element x. while ( x != link [ x ]) x = link [ x ]; return x ; } bool same ( int a , int b ) { // returns whether elements a and b belong to the same set return find ( a ) == find ( b ); } void unite ( int a , int b ) { // joins the set that contains elements a and b // it connects the smaller set to the larger set a = find ( a ); b = find ( b ); if ( size [ a ] < size [ b ]) swap ( a , b ); size [ a ] += size [ b ]; link [ b ] = a ; }","title":"Implementation"},{"location":"Algorithms/Tree/SpanningTree/#prims_algorithm","text":"The algorithm first adds an arbitrary node to the tree. After this, the algorithm always choose a minimum-weight edge that adds a new node to the tree. Finally all nodes have been added to the tree and a minimum tree has been found Prim's algorithm resembles Dijkstra's algorithm . But, Prim's algorithm simply selects the minimum weight edge that adds a new node to the tree.","title":"Prim's algorithm"},{"location":"Algorithms/Tree/SpanningTree/#implementation_2","text":"Like Dijkstra's algorithm , Prim's algorithm can be efficiently implemented using a priority queue. The priority queue should contain all nodes that can be conneted to the current component using a single edge, in increasing order of the weights of the corresponding edges. The time complexity of Prim's algorithm is $\\Omicron(n+ mlg(m))$ that equals the time complexity of Dijkstra's algorithm. most competitive programmers use Kruskal's algorithm. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 int V ; // #nodes(vertices) int E ; // #edges int const INF = 2e9 ; vector < vector < pair < int , int >>> adj ; //adjacency list using node = pair < int , int > ; // pair<key, node> representation. void Prim () { priority_queue < node , vector < node > , greater < node >> pq ; //mean-heap implementation using stl int starter = 0 ; // starting node or initial node. vector < int > key ( V , INF ); vector < int > parent ( V , - 1 ); vector < bool > inMST ( V , false ); pq . push ({ 0 , starter }); int MST_weight = 0 ; while ( ! pq . empty ()) { int w = pq . top (). first ; // minimum weight to add a new vertice to MST int u = pq . top (). second ; // new vertice pq . pop (); if ( inMST [ u ]) continue ; // if u already in MST continue; inMST [ u ] = true ; // else add u to MST; MST_weight += w ; for ( auto vw : adj [ u ]) { int v = vw . first ; int w = vw . second ; if ( inMST [ v ] == false && key [ v ] > w ) { // only when new key(weight) value of v is less than current value key [ v ] = w ; pq . push ({ key [ v ], v }); parent [ v ] = u ; } } } }","title":"Implementation"},{"location":"Algorithms/Tree/Traversal/","text":"Tree Traversal \u00b6 General Graph Traversal algorithms can be used to traverse the nodes of a tree. However, the traversal of a tree is easier to implement than that of a general graph, because there are no cycles in the tree and it is not possible to reach a node from multiple directions. implementation \u00b6 The typical way to traverse a tree is to start a depth-first search at an arbitrary node . The following recursive function can be used assumes that we are maintaining adjacency list 1 2 3 4 5 6 7 8 void dfs ( int s , int e ) { // current node s and previous node e //process node s for ( auto u : adj [ s ]) { if ( u != e ) dfs ( u , s ); } } dfs ( x , x ) // initial call because there's no self loop","title":"Traversal"},{"location":"Algorithms/Tree/Traversal/#tree_traversal","text":"General Graph Traversal algorithms can be used to traverse the nodes of a tree. However, the traversal of a tree is easier to implement than that of a general graph, because there are no cycles in the tree and it is not possible to reach a node from multiple directions.","title":"Tree Traversal"},{"location":"Algorithms/Tree/Traversal/#implementation","text":"The typical way to traverse a tree is to start a depth-first search at an arbitrary node . The following recursive function can be used assumes that we are maintaining adjacency list 1 2 3 4 5 6 7 8 void dfs ( int s , int e ) { // current node s and previous node e //process node s for ( auto u : adj [ s ]) { if ( u != e ) dfs ( u , s ); } } dfs ( x , x ) // initial call because there's no self loop","title":"implementation"},{"location":"Algorithms/Tree/TreeQueries/","text":"Tree Queries \u00b6 What is the $k$th ancester of a node? What is the sum of values in the subtree of a node? what is the sum of values on a path between two nodes? what is the lowest common ancester of two nodes? Finding ancestors \u00b6 The $k$th ancestor of a node $x$ in a rooted tree is the node that we will reach if we move $k$ levels up from $x$. Let ancestor(x, k) denote the $k$th ancestor of node $x$. an easy way to calculate any value of ancestor(x, k) is to perform a sequence of $k$ moves in the tree. However, the time complexity of the method is $\\Omicron(k)$, which may be slow, because tree of $n$ nodes may have chain of $n$ nodes. Fortunately, using a technique similar to that used in here , any value of ancestor(x, k) can be efficiently calculated in $\\Omicron(lgk)$ time after pre processing. The idea is to precalculate all values ancestor(x, k) where $k\\leq n$ is a power of two. The preprocessing takes $\\Omicron(nlgn)$ time, because $\\Omicron(lgn)$ values are calculated for each node. After this, any value of ancestor(x, k) can be calculated in $\\Omicron(lgk)$ time by representing $k$ as a sum where each term is a power of two. Implementation \u00b6 1 not yet added Subtree and paths \u00b6 A tree traveral array contains the nodes of a rooted tree in the order in which a depth-first search from the root node visit them. example will be added we're working on it Subtree queries \u00b6 Each subtree of a tree corresponds to a subarray of the tree traversal array such that the first element of the subarray is the root node. Using this fact, we can efficiently process queries that are related to subtree of a tree. for example: update the value of a node, calculate the sum of values in the subtree of a node. The idea is to construct a tree traversal array that contains three values for each node: the identifier of the node, the size of the subtree, and the value of the node. Using this array we can calculate the sum of the values in any subtree by first finding out the size of the subtree and then the values of the corresponding nodes. To answer the queries efficiently, it suffices to store the values of the nodes in a binary indexed or segment tree .<< link will be added. After this, we can both update a value and calculate the sum of values in $\\Omicron(lgn)$ time. Implementation \u00b6 1 not yet added path queries \u00b6 Using a tree traversal array, we can also efficiently calculate sums of values on paths from the root node to any node of the tree. Consider a problem where our task is to support the following queries: change the value of a node calculate the sum of values on a path from the root to node We can solve this problem like before, but now each value in the last row of the array is the sum of values on a path from the root to the node. When the value of a node increase by $x$, the sums of all nodes in its subtree increase by $x$. Thus, to support both the operation, we should be able to increase all values in a range and retrieve a single value. This can be done in $\\Omicron(lgn)$ using a binary indexed or segment tree. Implementation \u00b6 1 not yet added Lowest Common Ancestor \u00b6 The lowest common ancestor of two nodes of a rooted tree is lowest node whose subtree contains both the nodes. A typical problem is to efficiently process queries that ask to find the lowest common ancestor of two nodes. Problems \u00b6 LCA LCA 2 LCA\uc640 \ucffc\ub9ac Method 1 \u00b6 One way to solve the problem is to use the fact that we can efficiently find the $k$th ancestor of any node in the tree. Using this, we can divide the problem of finding the lowest common ancestor into two parts. We use two pointers that initially point to the two nodes whose lowest common ancestor we should find. First, we move one of the pointers upwards so that both pointers point to nodes at the same level. After this, we determine the minimum number of steps needed to move both pointers upwards so that they will point to the same node. The node to which the pointers point after this is the lowest common ancestor. Since both parts of the algorithm can be performed in $\\Omicron(lgn)$ time using precomputed information, we can find the lowest common ancestor of any two nodes in $\\Omicron(lgn)$ time. Implementation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 //preprocess int log ( int x ) { int ans = 0 ; while ( x ) { x >>= 1 ; ans ++ ; } return ans ; } int MAXLOG = log ( MAXN ); int par [ MAXN ][ MAXLOG ]; // initially all -1 int h [ MAXN ]; void dfs ( int v , int p = - 1 ) { if ( p + 1 ) h [ v ] = h [ p ] + 1 ; par [ v ][ 0 ] = p ; for ( int i = 1 ; i < MAXLOG ; i ++ ) { if ( par [ v ][ i - 1 ] + 1 ) par [ v ][ i ] = par [ par [ v ][ i - 1 ]][ i - 1 ]; } for ( auto u : adj [ v ]) if ( p - u ) // equivalent to u != v dfs ( u , v ); } //query int LCA ( int v , int u ) { if ( h [ v ] < h [ u ]) swap ( v , u ) //v is bigger for ( int i = MAXLOG - 1 ; i >= 0 ; i -- ) if ( par [ v ][ i ] + 1 && h [ par [ v ][ i ]] >= h [ u ]) v = par [ v ][ i ]; //now h[v] == h[u] if ( v == u ) return v ; for ( int i = MAXLOG - 1 ; i >= 0 ; i -- ) if ( par [ v ][ i ] - par [ u ][ i ]) // par != par v = par [ v ][ i ], u = par [ u ][ i ]; return par [ v ][ 0 ]; } Method 2 \u00b6 Another way to solve the problem is based on a tree traversal array 1 . Once again, the idea is to traverse the nodes using depth-first search. However, we use a different tree traversal array: We add each node to th e array always the depth-first search walks through the node, and not only at the first visit. Hence, a node that has $k$ children appears $k+1$ times in the array and there are a total $2n-1$ nodes in the array. We store two values in the array: the identifier of the node and the depth of the node in the tree. Now we can find the lowest common ancestor of nodes $a$ and $b$ by finding the node with $\\text{minimum}$ depth between nodes $a$ and $b$ in the array Thus to find the lowest common ancestor of two nodes it suffices to process [range minimum query]. Since the array is static, we can process such queries in $\\Omicron(1)$ time after an $\\Omicron(nlgn)$ time preprocessing. Implementation \u00b6 1 2 will be added after RMQ is added https : //codeforces.com/blog/entry/16221 Distance of nodes \u00b6 The distance between nodes $a$ and $b$ equals the length of the path from $a$ to $b$. It turns out that the problem of calculating the distance between nodes reduces to finding their lowest common ancestor. First, we root the tree arbitrarily. After this, the distance of nodes $a$ and $b$ can be calculated using the formula $$ depth(a) + depth(b) - 2 \\sdot depth(c) $$ where $c$ is the lowest common ancestor of $a$ and $b$ and $depth(s)$ denotes the depth of node $s$ Offline algorithms \u00b6 Offline algorithms. those algorithms are given a set of queries which can be answered in any order. It is often easier to design an offline algorithm compared to an online algorithm Merging data structures \u00b6 One method to construct an offline algorithm is to perform depth-first tree traversal and maintain data structures in nodes. At each node $s$, we create a data structure d[s] that is based on the data structures of the children of $s$. Then using this data structure, all queries related to $s$ are processed. As an example, consider the following problem: We are given a tree where each node has some value. Our task is to process queries of the form \"calculate the number of nodes with value $x$ in the subtree of node $s$\". In this problem, we can use map structures to answer the queries. If we create such a data structure for each node, we can easily process all given queries, because we can handle all queries related to a node immediately after creating its data structure. However it would be too slow to create all data structure from scratch. Instead, at each node $s$, we create an initial data structure d[s] that only contains the values of $s$. After this, we go through the children of $s$ and merge d[s] and all data structures d[u] where $u$ is a child of $s$. The merging at node $s$ can be done as follows We go through the children of $s$ and at each child $u$ merge d[s] and d[u] . We always copy the contents from d[u] to d[s] . However, before this, we swap the contents of d[s] and d[u] if d[s] is smaller than d[u] . By doing this, each value is copied only $\\Omicron(lgn)$ times during the tree traversal, which ensures that the algorithm is efficient. To swap the contents of two data structres $a$ and $b$ efficiently, we can just use following code: 1 swap ( a , b ); It is guaranteed that the above code works in constant time when $a$ and $b$ are C++ standard library data structures. Lowest common ancestors \u00b6 There is also an offline algorithm for processing a set of lowest common ancestor queries. The algorithm is based on the union-find data structure , and the benefit of the algorithm is that it is easier to implement than the algorithm we discussed earlier. The algorithm is given as input a set of pairs of nodes, and it determines for each such pair the lowest common ancestor of the nodes. The algorithm performs a depth-first tree traversal and maintains disjoint sets of nodes. Initially, each node belongs to a separate set. For each set, we also store the highest node in the tree that belongs to the set. When the algorithm visits a node $x$, it goes through all nodes $y$ such that the lowest common ancestor of $x$ and $y$ has to be found. If $y$ has already been visited, the algorithm reports that the lowest common ancestor of $x$ and $y$ is the highest node in the set of $y$. Then, after processing node $x$, the algorithm joins the sets of $x$ and its parent. This technique is sometimes called the Euler tour technique. \u21a9","title":"TreeQueries"},{"location":"Algorithms/Tree/TreeQueries/#tree_queries","text":"What is the $k$th ancester of a node? What is the sum of values in the subtree of a node? what is the sum of values on a path between two nodes? what is the lowest common ancester of two nodes?","title":"Tree Queries"},{"location":"Algorithms/Tree/TreeQueries/#finding_ancestors","text":"The $k$th ancestor of a node $x$ in a rooted tree is the node that we will reach if we move $k$ levels up from $x$. Let ancestor(x, k) denote the $k$th ancestor of node $x$. an easy way to calculate any value of ancestor(x, k) is to perform a sequence of $k$ moves in the tree. However, the time complexity of the method is $\\Omicron(k)$, which may be slow, because tree of $n$ nodes may have chain of $n$ nodes. Fortunately, using a technique similar to that used in here , any value of ancestor(x, k) can be efficiently calculated in $\\Omicron(lgk)$ time after pre processing. The idea is to precalculate all values ancestor(x, k) where $k\\leq n$ is a power of two. The preprocessing takes $\\Omicron(nlgn)$ time, because $\\Omicron(lgn)$ values are calculated for each node. After this, any value of ancestor(x, k) can be calculated in $\\Omicron(lgk)$ time by representing $k$ as a sum where each term is a power of two.","title":"Finding ancestors"},{"location":"Algorithms/Tree/TreeQueries/#implementation","text":"1 not yet added","title":"Implementation"},{"location":"Algorithms/Tree/TreeQueries/#subtree_and_paths","text":"A tree traveral array contains the nodes of a rooted tree in the order in which a depth-first search from the root node visit them. example will be added we're working on it","title":"Subtree and paths"},{"location":"Algorithms/Tree/TreeQueries/#subtree_queries","text":"Each subtree of a tree corresponds to a subarray of the tree traversal array such that the first element of the subarray is the root node. Using this fact, we can efficiently process queries that are related to subtree of a tree. for example: update the value of a node, calculate the sum of values in the subtree of a node. The idea is to construct a tree traversal array that contains three values for each node: the identifier of the node, the size of the subtree, and the value of the node. Using this array we can calculate the sum of the values in any subtree by first finding out the size of the subtree and then the values of the corresponding nodes. To answer the queries efficiently, it suffices to store the values of the nodes in a binary indexed or segment tree .<< link will be added. After this, we can both update a value and calculate the sum of values in $\\Omicron(lgn)$ time.","title":"Subtree queries"},{"location":"Algorithms/Tree/TreeQueries/#implementation_1","text":"1 not yet added","title":"Implementation"},{"location":"Algorithms/Tree/TreeQueries/#path_queries","text":"Using a tree traversal array, we can also efficiently calculate sums of values on paths from the root node to any node of the tree. Consider a problem where our task is to support the following queries: change the value of a node calculate the sum of values on a path from the root to node We can solve this problem like before, but now each value in the last row of the array is the sum of values on a path from the root to the node. When the value of a node increase by $x$, the sums of all nodes in its subtree increase by $x$. Thus, to support both the operation, we should be able to increase all values in a range and retrieve a single value. This can be done in $\\Omicron(lgn)$ using a binary indexed or segment tree.","title":"path queries"},{"location":"Algorithms/Tree/TreeQueries/#implementation_2","text":"1 not yet added","title":"Implementation"},{"location":"Algorithms/Tree/TreeQueries/#lowest_common_ancestor","text":"The lowest common ancestor of two nodes of a rooted tree is lowest node whose subtree contains both the nodes. A typical problem is to efficiently process queries that ask to find the lowest common ancestor of two nodes.","title":"Lowest Common Ancestor"},{"location":"Algorithms/Tree/TreeQueries/#problems","text":"LCA LCA 2 LCA\uc640 \ucffc\ub9ac","title":"Problems"},{"location":"Algorithms/Tree/TreeQueries/#method_1","text":"One way to solve the problem is to use the fact that we can efficiently find the $k$th ancestor of any node in the tree. Using this, we can divide the problem of finding the lowest common ancestor into two parts. We use two pointers that initially point to the two nodes whose lowest common ancestor we should find. First, we move one of the pointers upwards so that both pointers point to nodes at the same level. After this, we determine the minimum number of steps needed to move both pointers upwards so that they will point to the same node. The node to which the pointers point after this is the lowest common ancestor. Since both parts of the algorithm can be performed in $\\Omicron(lgn)$ time using precomputed information, we can find the lowest common ancestor of any two nodes in $\\Omicron(lgn)$ time.","title":"Method 1"},{"location":"Algorithms/Tree/TreeQueries/#implementation_3","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 //preprocess int log ( int x ) { int ans = 0 ; while ( x ) { x >>= 1 ; ans ++ ; } return ans ; } int MAXLOG = log ( MAXN ); int par [ MAXN ][ MAXLOG ]; // initially all -1 int h [ MAXN ]; void dfs ( int v , int p = - 1 ) { if ( p + 1 ) h [ v ] = h [ p ] + 1 ; par [ v ][ 0 ] = p ; for ( int i = 1 ; i < MAXLOG ; i ++ ) { if ( par [ v ][ i - 1 ] + 1 ) par [ v ][ i ] = par [ par [ v ][ i - 1 ]][ i - 1 ]; } for ( auto u : adj [ v ]) if ( p - u ) // equivalent to u != v dfs ( u , v ); } //query int LCA ( int v , int u ) { if ( h [ v ] < h [ u ]) swap ( v , u ) //v is bigger for ( int i = MAXLOG - 1 ; i >= 0 ; i -- ) if ( par [ v ][ i ] + 1 && h [ par [ v ][ i ]] >= h [ u ]) v = par [ v ][ i ]; //now h[v] == h[u] if ( v == u ) return v ; for ( int i = MAXLOG - 1 ; i >= 0 ; i -- ) if ( par [ v ][ i ] - par [ u ][ i ]) // par != par v = par [ v ][ i ], u = par [ u ][ i ]; return par [ v ][ 0 ]; }","title":"Implementation"},{"location":"Algorithms/Tree/TreeQueries/#method_2","text":"Another way to solve the problem is based on a tree traversal array 1 . Once again, the idea is to traverse the nodes using depth-first search. However, we use a different tree traversal array: We add each node to th e array always the depth-first search walks through the node, and not only at the first visit. Hence, a node that has $k$ children appears $k+1$ times in the array and there are a total $2n-1$ nodes in the array. We store two values in the array: the identifier of the node and the depth of the node in the tree. Now we can find the lowest common ancestor of nodes $a$ and $b$ by finding the node with $\\text{minimum}$ depth between nodes $a$ and $b$ in the array Thus to find the lowest common ancestor of two nodes it suffices to process [range minimum query]. Since the array is static, we can process such queries in $\\Omicron(1)$ time after an $\\Omicron(nlgn)$ time preprocessing.","title":"Method 2"},{"location":"Algorithms/Tree/TreeQueries/#implementation_4","text":"1 2 will be added after RMQ is added https : //codeforces.com/blog/entry/16221","title":"Implementation"},{"location":"Algorithms/Tree/TreeQueries/#distance_of_nodes","text":"The distance between nodes $a$ and $b$ equals the length of the path from $a$ to $b$. It turns out that the problem of calculating the distance between nodes reduces to finding their lowest common ancestor. First, we root the tree arbitrarily. After this, the distance of nodes $a$ and $b$ can be calculated using the formula $$ depth(a) + depth(b) - 2 \\sdot depth(c) $$ where $c$ is the lowest common ancestor of $a$ and $b$ and $depth(s)$ denotes the depth of node $s$","title":"Distance of nodes"},{"location":"Algorithms/Tree/TreeQueries/#offline_algorithms","text":"Offline algorithms. those algorithms are given a set of queries which can be answered in any order. It is often easier to design an offline algorithm compared to an online algorithm","title":"Offline algorithms"},{"location":"Algorithms/Tree/TreeQueries/#merging_data_structures","text":"One method to construct an offline algorithm is to perform depth-first tree traversal and maintain data structures in nodes. At each node $s$, we create a data structure d[s] that is based on the data structures of the children of $s$. Then using this data structure, all queries related to $s$ are processed. As an example, consider the following problem: We are given a tree where each node has some value. Our task is to process queries of the form \"calculate the number of nodes with value $x$ in the subtree of node $s$\". In this problem, we can use map structures to answer the queries. If we create such a data structure for each node, we can easily process all given queries, because we can handle all queries related to a node immediately after creating its data structure. However it would be too slow to create all data structure from scratch. Instead, at each node $s$, we create an initial data structure d[s] that only contains the values of $s$. After this, we go through the children of $s$ and merge d[s] and all data structures d[u] where $u$ is a child of $s$. The merging at node $s$ can be done as follows We go through the children of $s$ and at each child $u$ merge d[s] and d[u] . We always copy the contents from d[u] to d[s] . However, before this, we swap the contents of d[s] and d[u] if d[s] is smaller than d[u] . By doing this, each value is copied only $\\Omicron(lgn)$ times during the tree traversal, which ensures that the algorithm is efficient. To swap the contents of two data structres $a$ and $b$ efficiently, we can just use following code: 1 swap ( a , b ); It is guaranteed that the above code works in constant time when $a$ and $b$ are C++ standard library data structures.","title":"Merging data structures"},{"location":"Algorithms/Tree/TreeQueries/#lowest_common_ancestors","text":"There is also an offline algorithm for processing a set of lowest common ancestor queries. The algorithm is based on the union-find data structure , and the benefit of the algorithm is that it is easier to implement than the algorithm we discussed earlier. The algorithm is given as input a set of pairs of nodes, and it determines for each such pair the lowest common ancestor of the nodes. The algorithm performs a depth-first tree traversal and maintains disjoint sets of nodes. Initially, each node belongs to a separate set. For each set, we also store the highest node in the tree that belongs to the set. When the algorithm visits a node $x$, it goes through all nodes $y$ such that the lowest common ancestor of $x$ and $y$ has to be found. If $y$ has already been visited, the algorithm reports that the lowest common ancestor of $x$ and $y$ is the highest node in the set of $y$. Then, after processing node $x$, the algorithm joins the sets of $x$ and its parent. This technique is sometimes called the Euler tour technique. \u21a9","title":"Lowest common ancestors"},{"location":"Contribute/CodeOfConduct/","text":"Code of Conduct \u00b6 Copy & go \u00b6 all codes should be working if you directly copy & paste to compiler. Compatibility with STL \u00b6 all the implementation should work with C++ STL. Example YourVector< int> v; ---snip--- sort(v.begin(), v.end()); // should be working","title":"CodeOfConduct"},{"location":"Contribute/CodeOfConduct/#code_of_conduct","text":"","title":"Code of Conduct"},{"location":"Contribute/CodeOfConduct/#copy_go","text":"all codes should be working if you directly copy & paste to compiler.","title":"Copy &amp; go"},{"location":"Contribute/CodeOfConduct/#compatibility_with_stl","text":"all the implementation should work with C++ STL. Example YourVector< int> v; ---snip--- sort(v.begin(), v.end()); // should be working","title":"Compatibility with STL"},{"location":"Contribute/Emoji/","text":"People :bowtie: :smile: :laughing: :blush: :smiley: :relaxed: :smirk: :heart_eyes: :kissing_heart: :kissing_closed_eyes: :flushed: :relieved: :satisfied: :grin: :wink: :stuck_out_tongue_winking_eye: :stuck_out_tongue_closed_eyes: :grinning: :kissing: :kissing_smiling_eyes: :stuck_out_tongue: :sleeping: :worried: :frowning: :anguished: :open_mouth: :grimacing: :confused: :hushed: :expressionless: :unamused: :sweat_smile: :sweat: :disappointed_relieved: :weary: :pensive: :disappointed: :confounded: :fearful: :cold_sweat: :persevere: :cry: :sob: :joy: :astonished: :scream: :neckbeard: :tired_face: :angry: :rage: :triumph: :sleepy: :yum: :mask: :sunglasses: :dizzy_face: :imp: :smiling_imp: :neutral_face: :no_mouth: :innocent: :alien: :yellow_heart: :blue_heart: :purple_heart: :heart: :green_heart: :broken_heart: :heartbeat: :heartpulse: :two_hearts: :revolving_hearts: :cupid: :sparkling_heart: :sparkles: :star: :star2: :dizzy: :boom: :collision: :anger: :exclamation: :question: :grey_exclamation: :grey_question: :zzz: :dash: :sweat_drops: :notes: :musical_note: :fire: :hankey: :poop: :shit: :+1: :thumbsup: :-1: :thumbsdown: :ok_hand: :punch: :facepunch: :fist: :v: :wave: :hand: :raised_hand: :open_hands: :point_up: :point_down: :point_left: :point_right: :raised_hands: :pray: :point_up_2: :clap: :muscle: :metal: :fu: :walking: :runner: :running: :couple: :family: :two_men_holding_hands: :two_women_holding_hands: :dancer: :dancers: :ok_woman: :no_good: :information_desk_person: :raising_hand: :bride_with_veil: :person_with_pouting_face: :person_frowning: :bow: :couplekiss: :couplekiss: :couple_with_heart: :massage: :haircut: :nail_care: :boy: :girl: :woman: :man: :baby: :older_woman: :older_man: :person_with_blond_hair: :man_with_gua_pi_mao: :man_with_turban: :construction_worker: :cop: :angel: :princess: :smiley_cat: :smile_cat: :heart_eyes_cat: :kissing_cat: :smirk_cat: :scream_cat: :crying_cat_face: :joy_cat: :pouting_cat: :japanese_ogre: :japanese_goblin: :see_no_evil: :hear_no_evil: :speak_no_evil: :guardsman: :skull: :feet: :lips: :kiss: :droplet: :ear: :eyes: :nose: :tongue: :love_letter: :bust_in_silhouette: :busts_in_silhouette: :speech_balloon: :thought_balloon: :feelsgood: :finnadie: :goberserk: :godmode: :hurtrealbad: :rage1: :rage2: :rage3: :rage4: :suspect: :trollface: Nature :sunny: :umbrella: :cloud: :snowflake: :snowman: :zap: :cyclone: :foggy: :ocean: :cat: :dog: :mouse: :hamster: :rabbit: :wolf: :frog: :tiger: :koala: :bear: :pig: :pig_nose: :cow: :boar: :monkey_face: :monkey: :horse: :racehorse: :camel: :sheep: :elephant: :panda_face: :snake: :bird: :baby_chick: :hatched_chick: :hatching_chick: :chicken: :penguin: :turtle: :bug: :honeybee: :ant: :beetle: :snail: :octopus: :tropical_fish: :fish: :whale: :whale2: :dolphin: :cow2: :ram: :rat: :water_buffalo: :tiger2: :rabbit2: :dragon: :goat: :rooster: :dog2: :pig2: :mouse2: :ox: :dragon_face: :blowfish: :crocodile: :dromedary_camel: :leopard: :cat2: :poodle: :paw_prints: :bouquet: :cherry_blossom: :tulip: :four_leaf_clover: :rose: :sunflower: :hibiscus: :maple_leaf: :leaves: :fallen_leaf: :herb: :mushroom: :cactus: :palm_tree: :evergreen_tree: :deciduous_tree: :chestnut: :seedling: :blossom: :ear_of_rice: :shell: :globe_with_meridians: :sun_with_face: :full_moon_with_face: :new_moon_with_face: :new_moon: :waxing_crescent_moon: :first_quarter_moon: :waxing_gibbous_moon: :full_moon: :waning_gibbous_moon: :last_quarter_moon: :waning_crescent_moon: :last_quarter_moon_with_face: :first_quarter_moon_with_face: :moon: :earth_africa: :earth_americas: :earth_asia: :volcano: :milky_way: :partly_sunny: :octocat: :squirrel: Objects :bamboo: :gift_heart: :dolls: :school_satchel: :mortar_board: :flags: :fireworks: :sparkler: :wind_chime: :rice_scene: :jack_o_lantern: :ghost: :santa: :christmas_tree: :gift: :bell: :no_bell: :tanabata_tree: :tada: :confetti_ball: :balloon: :crystal_ball: :cd: :dvd: :floppy_disk: :camera: :video_camera: :movie_camera: :computer: :tv: :iphone: :phone: :telephone: :telephone_receiver: :pager: :fax: :minidisc: :vhs: :sound: :speaker: :mute: :loudspeaker: :mega: :hourglass: :hourglass_flowing_sand: :alarm_clock: :watch: :radio: :satellite: :loop: :mag: :mag_right: :unlock: :lock: :lock_with_ink_pen: :closed_lock_with_key: :key: :bulb: :flashlight: :high_brightness: :low_brightness: :electric_plug: :battery: :calling: :email: :mailbox: :postbox: :bath: :bathtub: :shower: :toilet: :wrench: :nut_and_bolt: :hammer: :seat: :moneybag: :yen: :dollar: :pound: :euro: :credit_card: :money_with_wings: :e-mail: :inbox_tray: :outbox_tray: :envelope: :incoming_envelope: :postal_horn: :mailbox_closed: :mailbox_with_mail: :mailbox_with_no_mail: :door: :smoking: :bomb: :gun: :hocho: :pill: :syringe: :page_facing_up: :page_with_curl: :bookmark_tabs: :bar_chart: :chart_with_upwards_trend: :chart_with_downwards_trend: :scroll: :clipboard: :calendar: :date: :card_index: :file_folder: :open_file_folder: :scissors: :pushpin: :paperclip: :black_nib: :pencil2: :straight_ruler: :triangular_ruler: :closed_book: :green_book: :blue_book: :orange_book: :notebook: :notebook_with_decorative_cover: :ledger: :books: :bookmark: :name_badge: :microscope: :telescope: :newspaper: :football: :basketball: :soccer: :baseball: :tennis: :8ball: :rugby_football: :bowling: :golf: :mountain_bicyclist: :bicyclist: :horse_racing: :snowboarder: :swimmer: :surfer: :ski: :spades: :hearts: :clubs: :diamonds: :gem: :ring: :trophy: :musical_score: :musical_keyboard: :violin: :space_invader: :video_game: :black_joker: :flower_playing_cards: :game_die: :dart: :mahjong: :clapper: :memo: :pencil: :book: :art: :microphone: :headphones: :trumpet: :saxophone: :guitar: :shoe: :sandal: :high_heel: :lipstick: :boot: :shirt: :tshirt: :necktie: :womans_clothes: :dress: :running_shirt_with_sash: :jeans: :kimono: :bikini: :ribbon: :tophat: :crown: :womans_hat: :mans_shoe: :closed_umbrella: :briefcase: :handbag: :pouch: :purse: :eyeglasses: :fishing_pole_and_fish: :coffee: :tea: :sake: :baby_bottle: :beer: :beers: :cocktail: :tropical_drink: :wine_glass: :fork_and_knife: :pizza: :hamburger: :fries: :poultry_leg: :meat_on_bone: :spaghetti: :curry: :fried_shrimp: :bento: :sushi: :fish_cake: :rice_ball: :rice_cracker: :rice: :ramen: :stew: :oden: :dango: :egg: :bread: :doughnut: :custard: :icecream: :ice_cream: :shaved_ice: :birthday: :cake: :cookie: :chocolate_bar: :candy: :lollipop: :honey_pot: :apple: :green_apple: :tangerine: :lemon: :cherries: :grapes: :watermelon: :strawberry: :peach: :melon: :banana: :pear: :pineapple: :sweet_potato: :eggplant: :tomato: :corn: Places :house: :house_with_garden: :school: :office: :post_office: :hospital: :bank: :convenience_store: :love_hotel: :hotel: :wedding: :church: :department_store: :european_post_office: :city_sunrise: :city_sunset: :japanese_castle: :european_castle: :tent: :factory: :tokyo_tower: :japan: :mount_fuji: :sunrise_over_mountains: :sunrise: :stars: :statue_of_liberty: :bridge_at_night: :carousel_horse: :rainbow: :ferris_wheel: :fountain: :roller_coaster: :ship: :speedboat: :boat: :sailboat: :rowboat: :anchor: :rocket: :airplane: :helicopter: :steam_locomotive: :tram: :mountain_railway: :bike: :aerial_tramway: :suspension_railway: :mountain_cableway: :tractor: :blue_car: :oncoming_automobile: :car: :red_car: :taxi: :oncoming_taxi: :articulated_lorry: :bus: :oncoming_bus: :rotating_light: :police_car: :oncoming_police_car: :fire_engine: :ambulance: :minibus: :truck: :train: :station: :train2: :bullettrain_front: :bullettrain_side: :light_rail: :monorail: :railway_car: :trolleybus: :ticket: :fuelpump: :vertical_traffic_light: :traffic_light: :warning: :construction: :beginner: :atm: :slot_machine: :busstop: :barber: :hotsprings: :checkered_flag: :crossed_flags: :izakaya_lantern: :moyai: :circus_tent: :performing_arts: :round_pushpin: :triangular_flag_on_post: :jp: :kr: :cn: :us: :fr: :es: :it: :ru: :gb: :uk: :de: Symbols :one: :two: :three: :four: :five: :six: :seven: :eight: :nine: :keycap_ten: :1234: :zero: :hash: :symbols: :arrow_backward: :arrow_down: :arrow_forward: :arrow_left: :capital_abcd: :abcd: :abc: :arrow_lower_left: :arrow_lower_right: :arrow_right: :arrow_up: :arrow_upper_left: :arrow_upper_right: :arrow_double_down: :arrow_double_up: :arrow_down_small: :arrow_heading_down: :arrow_heading_up: :leftwards_arrow_with_hook: :arrow_right_hook: :left_right_arrow: :arrow_up_down: :arrow_up_small: :arrows_clockwise: :arrows_counterclockwise: :rewind: :fast_forward: :information_source: :ok: :twisted_rightwards_arrows: :repeat: :repeat_one: :new: :top: :up: :cool: :free: :ng: :cinema: :koko: :signal_strength: :u5272: :u5408: :u55b6: :u6307: :u6708: :u6709: :u6e80: :u7121: :u7533: :u7a7a: :u7981: :sa: :restroom: :mens: :womens: :baby_symbol: :no_smoking: :parking: :wheelchair: :metro: :baggage_claim: :accept: :wc: :potable_water: :put_litter_in_its_place: :secret: :congratulations: :m: :passport_control: :left_luggage: :customs: :ideograph_advantage: :cl: :sos: :id: :no_entry_sign: :underage: :no_mobile_phones: :do_not_litter: :non-potable_water: :no_bicycles: :no_pedestrians: :children_crossing: :no_entry: :eight_spoked_asterisk: :eight_pointed_black_star: :heart_decoration: :vs: :vibration_mode: :mobile_phone_off: :chart: :currency_exchange: :aries: :taurus: :gemini: :cancer: :leo: :virgo: :libra: :scorpius: :sagittarius: :capricorn: :aquarius: :pisces: :ophiuchus: :six_pointed_star: :negative_squared_cross_mark: :a: :b: :ab: :o2: :diamond_shape_with_a_dot_inside: :recycle: :end: :on: :soon: :clock1: :clock130: :clock10: :clock1030: :clock11: :clock1130: :clock12: :clock1230: :clock2: :clock230: :clock3: :clock330: :clock4: :clock430: :clock5: :clock530: :clock6: :clock630: :clock7: :clock730: :clock8: :clock830: :clock9: :clock930: :heavy_dollar_sign: :copyright: :registered: :tm: :x: :heavy_exclamation_mark: :bangbang: :interrobang: :o: :heavy_multiplication_x: :heavy_plus_sign: :heavy_minus_sign: :heavy_division_sign: :white_flower: :100: :heavy_check_mark: :ballot_box_with_check: :radio_button: :link: :curly_loop: :wavy_dash: :part_alternation_mark: :trident: :black_square: :black_square: :white_square: :white_square: :white_check_mark: :black_square_button: :white_square_button: :black_circle: :white_circle: :red_circle: :large_blue_circle: :large_blue_diamond: :large_orange_diamond: :small_blue_diamond: :small_orange_diamond: :small_red_triangle: :small_red_triangle_down: :shipit:","title":"Emoji"},{"location":"Contribute/HowToContribute/","text":"How To Contribute \u00b6 This is My first open source project under very active development and is also being used to ship code to everybody on codeforces , AtCoder , HackerRank , LeetCode , BaekJoonOnlineJudge and so on. I'm still working out to make contributing to this project as easy and transparent as possible, but I'm not quite there yet. Hopefully this document makes the process for contributing clear and answers some quiestions that you may have. Code of Conduct \u00b6 I adopted a Code of Conduct that i expect project participants to adhere to. You can see full document of Code Of Conduct . 1. copy & pastable \u00b6 All the codes in this site are ready-to-be-compiled that means you could just copy & paste it to see it works. 2. Compatibility \u00b6 Data Structure implementations should be compatible with C++ STL. ex) sort(Your_implementation.begin(), Your_implementation.end()) should work. Use template for your convinience \u00b6 there's a template for contribute","title":"HowToContribute"},{"location":"Contribute/HowToContribute/#how_to_contribute","text":"This is My first open source project under very active development and is also being used to ship code to everybody on codeforces , AtCoder , HackerRank , LeetCode , BaekJoonOnlineJudge and so on. I'm still working out to make contributing to this project as easy and transparent as possible, but I'm not quite there yet. Hopefully this document makes the process for contributing clear and answers some quiestions that you may have.","title":"How To Contribute"},{"location":"Contribute/HowToContribute/#code_of_conduct","text":"I adopted a Code of Conduct that i expect project participants to adhere to. You can see full document of Code Of Conduct .","title":"Code of Conduct"},{"location":"Contribute/HowToContribute/#1_copy_pastable","text":"All the codes in this site are ready-to-be-compiled that means you could just copy & paste it to see it works.","title":"1. copy &amp; pastable"},{"location":"Contribute/HowToContribute/#2_compatibility","text":"Data Structure implementations should be compatible with C++ STL. ex) sort(Your_implementation.begin(), Your_implementation.end()) should work.","title":"2. Compatibility"},{"location":"Contribute/HowToContribute/#use_template_for_your_convinience","text":"there's a template for contribute","title":"Use template for your convinience"},{"location":"Contribute/Template/","text":"Vector \u00b6 Brief explanation. Operations & time complexity \u00b6 Methods RunningTime push_back(val) O(1) pop() O(1) empty() O(1) you can use table generator Implementation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #include <cstido> #include <iostream> template < typename T > class Vector { //use TitleCase for DataStructure implementation --- snip --- } int main () { //few lines of code to test your implementation Vector < int > v ; for ( int i = 0 ; i < n ; i ++ ) { v . push_back ( rand () % 100 ); } sort ( v . begin (), v . end ()) for ( auto x : v ) { cout << x << ' ' ; } } keep your implementation self-contained. Related Problems \u00b6 title of easy problem Some hard problem Lily want a phone add difficulty information(optional) Related Topics \u00b6 Stack Analysis (Optional) \u00b6 You can add some mathematical things here using KaTex as a block tag $$ T(N) = O(N*M) $$ or as a inline tag $T(N) = O(N) $ Contributers (Optional) \u00b6 07.12.2019 contributer1 07.14.2019 typo correction contributer2 07.15.2019 add new section \"Analysis\" contributer3 07.18.2019 fix bugs in \"implementation\" contributer4 07.19.2019 improved performance \"implementation\" contributer5 07.23.2019 refactoring \"implementation\" contributer6 08.02.2019 add related problems contributer7","title":"Template"},{"location":"Contribute/Template/#vector","text":"Brief explanation.","title":"Vector"},{"location":"Contribute/Template/#operations_time_complexity","text":"Methods RunningTime push_back(val) O(1) pop() O(1) empty() O(1) you can use table generator","title":"Operations &amp; time complexity"},{"location":"Contribute/Template/#implementation","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #include <cstido> #include <iostream> template < typename T > class Vector { //use TitleCase for DataStructure implementation --- snip --- } int main () { //few lines of code to test your implementation Vector < int > v ; for ( int i = 0 ; i < n ; i ++ ) { v . push_back ( rand () % 100 ); } sort ( v . begin (), v . end ()) for ( auto x : v ) { cout << x << ' ' ; } } keep your implementation self-contained.","title":"Implementation"},{"location":"Contribute/Template/#related_problems","text":"title of easy problem Some hard problem Lily want a phone add difficulty information(optional)","title":"Related Problems"},{"location":"Contribute/Template/#related_topics","text":"Stack","title":"Related Topics"},{"location":"Contribute/Template/#analysis_optional","text":"You can add some mathematical things here using KaTex as a block tag $$ T(N) = O(N*M) $$ or as a inline tag $T(N) = O(N) $","title":"Analysis (Optional)"},{"location":"Contribute/Template/#contributers_optional","text":"07.12.2019 contributer1 07.14.2019 typo correction contributer2 07.15.2019 add new section \"Analysis\" contributer3 07.18.2019 fix bugs in \"implementation\" contributer4 07.19.2019 improved performance \"implementation\" contributer5 07.23.2019 refactoring \"implementation\" contributer6 08.02.2019 add related problems contributer7","title":"Contributers (Optional)"},{"location":"DataStructures/HashTables/HashFunctions/","text":"Hash Function \u00b6 A hash function is any function that can be used to map data of arbitrary size onto data of a fixed size. Hash Functions \u00b6 1. DJB2 \u00b6 this algorithm (k=33) was first reported by dan bernstein many years ago in comp.lang.c. another version of this algorithm (now favored by bernstein) uses xor: hash(i) = hash(i-1) * 33 ^ str[i]; the magic of number 33 (why it works better than many other constants, prime or not) has never adequately explained 1 2 3 4 5 6 7 8 9 10 unsigned long long djb2 ( char * str ) { unsigned long long hash = 5381 ; int c ; while (( c = * ( str ++ ))) { hash = ( hash << 5 ) + hash + c ; } return hash ; } 2. sdbm \u00b6 this algorithm was created for sdbm (a public-domain reimplementation of ndbm) database library. it was found to do well in scrambling bits, causing better distribution of the keys and fewer splits. it also happens to be a good general hashing function with good distribution. the actual function is hash(i) = hash(i - 1) * 65599 + str[i];; what is included below is faster version used in gawk. (there iseven a faster, duff's device version) the magic constant 65599 was picked out of thin air while experimenting with different constants, and turns out to be a prime. this is one of the algorithms used in berkeley db (see sleepy cat) and else where 1 2 3 4 5 6 7 8 9 10 unsigned long long sdbm ( char * str ) { unsigned long long hash = 5381 ; int c ; while (( c = * ( str ++ ))) { hash = c + ( hash << 6 ) + ( hash << 16 ) - hash ; } return hash ; } 3. lose lose \u00b6 This hash function appeared in K&R (1st ed) but at least the reader was warned: \"This is not the best possible algorithm, but it has the merit of extreme simplicity\". This is an understatement; It is a terrible hashing algorithm, and it could have been much better without scarificing its \"extreme simplicity.\" Many C programmers use this function without actually testing it, or checking something like Knuth's Sorting and searching, so it stuck. It is now found mixed with other respectable code, eg.cnews. Warning Don't use this algorithm, it's terrible. 1 2 3 4 5 6 7 8 9 10 unsigned long long loseLose ( char * str ) { unsigned long long hash = 0 ; int c ; while (( c = * ( str ++ ))) { hash += c ; } return hash ; }","title":"Hash Functions"},{"location":"DataStructures/HashTables/HashFunctions/#hash_function","text":"A hash function is any function that can be used to map data of arbitrary size onto data of a fixed size.","title":"Hash Function"},{"location":"DataStructures/HashTables/HashFunctions/#hash_functions","text":"","title":"Hash Functions"},{"location":"DataStructures/HashTables/HashFunctions/#1_djb2","text":"this algorithm (k=33) was first reported by dan bernstein many years ago in comp.lang.c. another version of this algorithm (now favored by bernstein) uses xor: hash(i) = hash(i-1) * 33 ^ str[i]; the magic of number 33 (why it works better than many other constants, prime or not) has never adequately explained 1 2 3 4 5 6 7 8 9 10 unsigned long long djb2 ( char * str ) { unsigned long long hash = 5381 ; int c ; while (( c = * ( str ++ ))) { hash = ( hash << 5 ) + hash + c ; } return hash ; }","title":"1. DJB2"},{"location":"DataStructures/HashTables/HashFunctions/#2_sdbm","text":"this algorithm was created for sdbm (a public-domain reimplementation of ndbm) database library. it was found to do well in scrambling bits, causing better distribution of the keys and fewer splits. it also happens to be a good general hashing function with good distribution. the actual function is hash(i) = hash(i - 1) * 65599 + str[i];; what is included below is faster version used in gawk. (there iseven a faster, duff's device version) the magic constant 65599 was picked out of thin air while experimenting with different constants, and turns out to be a prime. this is one of the algorithms used in berkeley db (see sleepy cat) and else where 1 2 3 4 5 6 7 8 9 10 unsigned long long sdbm ( char * str ) { unsigned long long hash = 5381 ; int c ; while (( c = * ( str ++ ))) { hash = c + ( hash << 6 ) + ( hash << 16 ) - hash ; } return hash ; }","title":"2. sdbm"},{"location":"DataStructures/HashTables/HashFunctions/#3_lose_lose","text":"This hash function appeared in K&R (1st ed) but at least the reader was warned: \"This is not the best possible algorithm, but it has the merit of extreme simplicity\". This is an understatement; It is a terrible hashing algorithm, and it could have been much better without scarificing its \"extreme simplicity.\" Many C programmers use this function without actually testing it, or checking something like Knuth's Sorting and searching, so it stuck. It is now found mixed with other respectable code, eg.cnews. Warning Don't use this algorithm, it's terrible. 1 2 3 4 5 6 7 8 9 10 unsigned long long loseLose ( char * str ) { unsigned long long hash = 0 ; int c ; while (( c = * ( str ++ ))) { hash += c ; } return hash ; }","title":"3. lose lose"},{"location":"DataStructures/HashTables/Preface/","text":"Preface \u00b6 Many applications require a dynamic set that supports only the dictionary operations. A Hash Table is an effective data structure for implementing dictionaries. A hash table typically uses an array of size proportional to the number of keys actually stored. Hash functions \u00b6 Instead of using the key as an array index directly, the array index is computed from the key Dealing with collisions \u00b6 Collision: two keys hash to the same slot. Since a hash table uses array of size relatively small to the number of possible keys, there is a chance to collisions in which more than one key maps to the same array index Chaining OpenAddressing PerfectHashing OPERATIONS average worst average worst average worst INSERT $O(1)$ - - SEARCH $O(n/m)$ $O(n)$ $O(1)$ $O(1)$ DELETE $O(1)$ - - 1. Chaining \u00b6 In Chaining, we place all the elements that hash to the same slot in to the same linked llist 2. Open Addressing \u00b6 Resolve Collisions with iterative hashing Perfect Hashing \u00b6 Perfect Hasing uses second level Hashtable that has no collision. perfect hashing can support searches in $O(1)\\ wosrt-case$ time, when the set is static(!= dynamic)","title":"Preface"},{"location":"DataStructures/HashTables/Preface/#preface","text":"Many applications require a dynamic set that supports only the dictionary operations. A Hash Table is an effective data structure for implementing dictionaries. A hash table typically uses an array of size proportional to the number of keys actually stored.","title":"Preface"},{"location":"DataStructures/HashTables/Preface/#hash_functions","text":"Instead of using the key as an array index directly, the array index is computed from the key","title":"Hash functions"},{"location":"DataStructures/HashTables/Preface/#dealing_with_collisions","text":"Collision: two keys hash to the same slot. Since a hash table uses array of size relatively small to the number of possible keys, there is a chance to collisions in which more than one key maps to the same array index Chaining OpenAddressing PerfectHashing OPERATIONS average worst average worst average worst INSERT $O(1)$ - - SEARCH $O(n/m)$ $O(n)$ $O(1)$ $O(1)$ DELETE $O(1)$ - -","title":"Dealing with collisions"},{"location":"DataStructures/HashTables/Preface/#1_chaining","text":"In Chaining, we place all the elements that hash to the same slot in to the same linked llist","title":"1. Chaining"},{"location":"DataStructures/HashTables/Preface/#2_open_addressing","text":"Resolve Collisions with iterative hashing","title":"2. Open Addressing"},{"location":"DataStructures/HashTables/Preface/#perfect_hashing","text":"Perfect Hasing uses second level Hashtable that has no collision. perfect hashing can support searches in $O(1)\\ wosrt-case$ time, when the set is static(!= dynamic)","title":"Perfect Hashing"},{"location":"DataStructures/Linear/Heap/","text":"Heap \u00b6 The (binary) heap data structure is an array object that we can view as a nearly complete binary tree. An array A that represent a heap is an obejct with two attributes: $A.length$, $A.heap-size$ The root of the tree is $A[1]$, and given the index $i$ of a node, we can easily compute the indices of its parent, left child and right child. The values in the nodes satisfy a heap property. heap property max-heap-property: A[parent(i)] >= A[i]. min-heap-property: A[parent(i)] <= A[i]. priority queue often $heap$ is implemented as priority queue, because of space complexity of heap, space complexity: $\\Omicron(n)$ Operations \u00b6 Member Function Running Time max_heapify() $\\Omicron(lg(n))$ build_max_heap() $\\Omicron(n)$ heapsort() $\\Omicron(nlg(n))$ max_heap_insert() $\\Omicron(lg(n))$ heap_increase_key() $\\Omicron(lg(n))$ heap_maximum() $\\Omicron(lg(n))$ Applications \u00b6 HeapSort Priority queue: A priority queue is an abstract concept like \"a list\" or \" map\"; just as a list can be implemented with a linked list or an array, a priority queue can be implemented with heap or a variety of other methods. Graph algorithms Selection algorithms Implementation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 #include <iostream> using namespace std ; template < typename T > void _swap ( T & a , T & b ) { T temp = a ; a = b ; b = temp ; } template < typename T > class heap { int _size ; int _capacity ; T * _buf ; inline int parent ( int i ) { return ( i - 1 ) >> 1 ; } inline int left ( int i ) { return ( i << 1 ) + 1 ; } inline int right ( int i ) { return ( i << 1 ) + 2 ; } void max_heapify ( int i ) { int largest ; int l = left ( i ); int r = right ( i ); if ( l < _size && _buf [ l ] > _buf [ i ]) { largest = l ; } else { largest = i ; } if ( r < _size && _buf [ r ] > _buf [ largest ]) { largest = r ; } if ( largest != i ) { _swap ( _buf [ largest ], _buf [ i ]); max_heapify ( largest ); } } void reserve ( int n ) { if ( n <= _size ) return ; T * temp = new T [ n ]; for ( int i = 0 ; i < _size ; i ++ ) { temp [ i ] = _buf [ i ]; } _capacity = n ; delete [] _buf ; _buf = temp ; } public : heap () : _size ( 0 ), _capacity ( 0 ), _buf ( 0 ){} heap ( int n ) { _size = n ; _capacity = n ; _buf = new T [ n ]; for ( int i = 0 ; i < n ; i ++ ) { _buf [ i ] = rand () % 400 ; } } void build_max_heap () { for ( int i = parent ( _size - 1 ); i >= 0 ; i -- ) max_heapify ( i ); } void heapsort () { build_max_heap (); int original_size = _size ; for ( int i = _size - 1 ; i > 0 ; i -- ) { swap ( _buf [ 0 ], _buf [ i ]); _size -- ; max_heapify ( 0 ); } _size = original_size ; } // priority queue; T maximum () { return _buf [ 0 ]; } T extract_max () { if ( _size == 0 ) { cout << \"underflow\" ; return - 1 ; } int max = _buf [ 0 ]; _buf [ 0 ] = _buf [ -- _size ]; max_heapify ( 0 ); return max ; } void increase_key ( int i , T key ) { if ( key < _buf [ i ]) { cout << \"new key is smaller than current key\" << endl ; return ; } _buf [ i ] = key ; while ( i > 0 && _buf [ parent ( i )] < _buf [ i ]) { _swap ( _buf [ i ], _buf [ parent ( i )]); i = parent ( i ); } } void insert ( T key ) { if ( _size == 0 ) { reserve ( 1 ); } else if ( _size == _capacity ) { reserve ( _size << 1 ); } _buf [ _size ] = key - 1 ; increase_key ( _size ++ , key ); } void print () { for ( int i = 0 ; i < _size ; i ++ ) { cout << _buf [ i ] << ' ' ; } cout << endl ; } }; int main () { heap < int > q ( 30 ); // make random heap; cout << \"================= random ===============\" << endl ; q . print (); cout << \"========== after build-max-heap=========\" << endl ; q . build_max_heap (); q . print (); cout << \"============ after heapsort ============\" << endl ; q . heapsort (); q . print (); cout << \"============ priority queue ===========\" << endl ; heap < int > pq ; for ( int i = 0 ; i < 1000 ; i ++ ) { pq . insert ( rand () % 100 ); } for ( int i = 0 ; i < 1000 ; i ++ ) { cout << pq . extract_max () << ' ' ; } //pq.print(); return 0 ; }","title":"Heap"},{"location":"DataStructures/Linear/Heap/#heap","text":"The (binary) heap data structure is an array object that we can view as a nearly complete binary tree. An array A that represent a heap is an obejct with two attributes: $A.length$, $A.heap-size$ The root of the tree is $A[1]$, and given the index $i$ of a node, we can easily compute the indices of its parent, left child and right child. The values in the nodes satisfy a heap property. heap property max-heap-property: A[parent(i)] >= A[i]. min-heap-property: A[parent(i)] <= A[i]. priority queue often $heap$ is implemented as priority queue, because of space complexity of heap, space complexity: $\\Omicron(n)$","title":"Heap"},{"location":"DataStructures/Linear/Heap/#operations","text":"Member Function Running Time max_heapify() $\\Omicron(lg(n))$ build_max_heap() $\\Omicron(n)$ heapsort() $\\Omicron(nlg(n))$ max_heap_insert() $\\Omicron(lg(n))$ heap_increase_key() $\\Omicron(lg(n))$ heap_maximum() $\\Omicron(lg(n))$","title":"Operations"},{"location":"DataStructures/Linear/Heap/#applications","text":"HeapSort Priority queue: A priority queue is an abstract concept like \"a list\" or \" map\"; just as a list can be implemented with a linked list or an array, a priority queue can be implemented with heap or a variety of other methods. Graph algorithms Selection algorithms","title":"Applications"},{"location":"DataStructures/Linear/Heap/#implementation","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 #include <iostream> using namespace std ; template < typename T > void _swap ( T & a , T & b ) { T temp = a ; a = b ; b = temp ; } template < typename T > class heap { int _size ; int _capacity ; T * _buf ; inline int parent ( int i ) { return ( i - 1 ) >> 1 ; } inline int left ( int i ) { return ( i << 1 ) + 1 ; } inline int right ( int i ) { return ( i << 1 ) + 2 ; } void max_heapify ( int i ) { int largest ; int l = left ( i ); int r = right ( i ); if ( l < _size && _buf [ l ] > _buf [ i ]) { largest = l ; } else { largest = i ; } if ( r < _size && _buf [ r ] > _buf [ largest ]) { largest = r ; } if ( largest != i ) { _swap ( _buf [ largest ], _buf [ i ]); max_heapify ( largest ); } } void reserve ( int n ) { if ( n <= _size ) return ; T * temp = new T [ n ]; for ( int i = 0 ; i < _size ; i ++ ) { temp [ i ] = _buf [ i ]; } _capacity = n ; delete [] _buf ; _buf = temp ; } public : heap () : _size ( 0 ), _capacity ( 0 ), _buf ( 0 ){} heap ( int n ) { _size = n ; _capacity = n ; _buf = new T [ n ]; for ( int i = 0 ; i < n ; i ++ ) { _buf [ i ] = rand () % 400 ; } } void build_max_heap () { for ( int i = parent ( _size - 1 ); i >= 0 ; i -- ) max_heapify ( i ); } void heapsort () { build_max_heap (); int original_size = _size ; for ( int i = _size - 1 ; i > 0 ; i -- ) { swap ( _buf [ 0 ], _buf [ i ]); _size -- ; max_heapify ( 0 ); } _size = original_size ; } // priority queue; T maximum () { return _buf [ 0 ]; } T extract_max () { if ( _size == 0 ) { cout << \"underflow\" ; return - 1 ; } int max = _buf [ 0 ]; _buf [ 0 ] = _buf [ -- _size ]; max_heapify ( 0 ); return max ; } void increase_key ( int i , T key ) { if ( key < _buf [ i ]) { cout << \"new key is smaller than current key\" << endl ; return ; } _buf [ i ] = key ; while ( i > 0 && _buf [ parent ( i )] < _buf [ i ]) { _swap ( _buf [ i ], _buf [ parent ( i )]); i = parent ( i ); } } void insert ( T key ) { if ( _size == 0 ) { reserve ( 1 ); } else if ( _size == _capacity ) { reserve ( _size << 1 ); } _buf [ _size ] = key - 1 ; increase_key ( _size ++ , key ); } void print () { for ( int i = 0 ; i < _size ; i ++ ) { cout << _buf [ i ] << ' ' ; } cout << endl ; } }; int main () { heap < int > q ( 30 ); // make random heap; cout << \"================= random ===============\" << endl ; q . print (); cout << \"========== after build-max-heap=========\" << endl ; q . build_max_heap (); q . print (); cout << \"============ after heapsort ============\" << endl ; q . heapsort (); q . print (); cout << \"============ priority queue ===========\" << endl ; heap < int > pq ; for ( int i = 0 ; i < 1000 ; i ++ ) { pq . insert ( rand () % 100 ); } for ( int i = 0 ; i < 1000 ; i ++ ) { cout << pq . extract_max () << ' ' ; } //pq.print(); return 0 ; }","title":"Implementation"},{"location":"DataStructures/Linear/LinkedList/","text":"LinkedList(Doubly Linked List) \u00b6 A linked list is a linear data structure, in which the elements are not stored at contiguous memory locations. The elements in a linked lists are linked using pointers. Operations & time complexity \u00b6 Member Function Running Time insert_front() $\\Omicron(1)$ insert_back() $\\Omicron(1)$ insert_after() $\\Omicron(1)$ erase $\\Omicron(1)$ search() $\\Omicron(n)$ Implementation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 #include <bits/stdc++.h> using namespace std ; template < typename T > class LinkedList { struct Node { Node * before ; Node * next ; T data ; Node ( T data ) : before ( 0 ), next ( 0 ), data ( data ){} ~ Node () { delete before ; delete next ; delete data ; } }; Node * tail ; unsigned int _size ; public : Node * head ; LinkedList () : head ( 0 ), tail ( 0 ) {} void insert_front ( T val ) { Node * temp = new Node ( val ); if ( head == 0 ) { head = temp ; tail = temp ; } else { temp -> next = head ; head -> before = temp ; head = temp ; } } void insert_back ( T val ) { Node * temp = new Node ( val ); if ( tail == 0 ) { head = temp ; tail = temp ; } else { temp -> before = tail ; tail -> next = temp ; tail = temp ; } } void insert_after ( Node * node , T val ) { Node * temp = new Node ( val ); if ( temp -> next == 0 ) { tail = temp ; } temp -> next = node -> next ; temp -> next -> before = temp ; node -> next = temp ; temp -> before = node ; } Node * search ( T val ) { //search_from head Node * it = head ; while ( it != 0 && it -> data != val ) it = it -> next ; return it ; } void erase ( Node * node ) { if ( node == 0 ) return ; if ( node -> next == 0 ) { tail = node -> before ; tail -> next = 0 ; } else if ( node -> before == 0 ) { head = node -> next ; head -> before = 0 ; } else { node -> before -> next = node -> next ; node -> next = node -> before ; } delete node ; } void print () { Node * it = head ; while ( it != 0 ) { cout << it -> data << ' ' ; it = it -> next ; } } }; int main () { LinkedList < int > list ; for ( int i = 0 ; i < 100 ; i ++ ) { list . insert_front ( i ); } for ( int i = 0 ; i < 100 ; i ++ ) { list . insert_back ( i ); } for ( int i = 0 ; i < 100 ; i ++ ) { list . insert_after ( list . head -> next , i ); } list . print (); return 0 ; } Related Problems \u00b6 NEED_TO_BE_ADDED Related Topics \u00b6 NOT_YET Analysis (Later..) \u00b6 You can add some mathematical things here using KaTex as a block tag $$ T(N) = O(N*M) $$ or as a inline tag $T(N) = O(N) $ Contributers \u00b6 08.15.2019 jchrys","title":"Linked List"},{"location":"DataStructures/Linear/LinkedList/#linkedlistdoubly_linked_list","text":"A linked list is a linear data structure, in which the elements are not stored at contiguous memory locations. The elements in a linked lists are linked using pointers.","title":"LinkedList(Doubly Linked List)"},{"location":"DataStructures/Linear/LinkedList/#operations_time_complexity","text":"Member Function Running Time insert_front() $\\Omicron(1)$ insert_back() $\\Omicron(1)$ insert_after() $\\Omicron(1)$ erase $\\Omicron(1)$ search() $\\Omicron(n)$","title":"Operations &amp; time complexity"},{"location":"DataStructures/Linear/LinkedList/#implementation","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 #include <bits/stdc++.h> using namespace std ; template < typename T > class LinkedList { struct Node { Node * before ; Node * next ; T data ; Node ( T data ) : before ( 0 ), next ( 0 ), data ( data ){} ~ Node () { delete before ; delete next ; delete data ; } }; Node * tail ; unsigned int _size ; public : Node * head ; LinkedList () : head ( 0 ), tail ( 0 ) {} void insert_front ( T val ) { Node * temp = new Node ( val ); if ( head == 0 ) { head = temp ; tail = temp ; } else { temp -> next = head ; head -> before = temp ; head = temp ; } } void insert_back ( T val ) { Node * temp = new Node ( val ); if ( tail == 0 ) { head = temp ; tail = temp ; } else { temp -> before = tail ; tail -> next = temp ; tail = temp ; } } void insert_after ( Node * node , T val ) { Node * temp = new Node ( val ); if ( temp -> next == 0 ) { tail = temp ; } temp -> next = node -> next ; temp -> next -> before = temp ; node -> next = temp ; temp -> before = node ; } Node * search ( T val ) { //search_from head Node * it = head ; while ( it != 0 && it -> data != val ) it = it -> next ; return it ; } void erase ( Node * node ) { if ( node == 0 ) return ; if ( node -> next == 0 ) { tail = node -> before ; tail -> next = 0 ; } else if ( node -> before == 0 ) { head = node -> next ; head -> before = 0 ; } else { node -> before -> next = node -> next ; node -> next = node -> before ; } delete node ; } void print () { Node * it = head ; while ( it != 0 ) { cout << it -> data << ' ' ; it = it -> next ; } } }; int main () { LinkedList < int > list ; for ( int i = 0 ; i < 100 ; i ++ ) { list . insert_front ( i ); } for ( int i = 0 ; i < 100 ; i ++ ) { list . insert_back ( i ); } for ( int i = 0 ; i < 100 ; i ++ ) { list . insert_after ( list . head -> next , i ); } list . print (); return 0 ; }","title":"Implementation"},{"location":"DataStructures/Linear/LinkedList/#related_problems","text":"NEED_TO_BE_ADDED","title":"Related Problems"},{"location":"DataStructures/Linear/LinkedList/#related_topics","text":"NOT_YET","title":"Related Topics"},{"location":"DataStructures/Linear/LinkedList/#analysis_later","text":"You can add some mathematical things here using KaTex as a block tag $$ T(N) = O(N*M) $$ or as a inline tag $T(N) = O(N) $","title":"Analysis (Later..)"},{"location":"DataStructures/Linear/LinkedList/#contributers","text":"08.15.2019 jchrys","title":"Contributers"},{"location":"DataStructures/Linear/Pair/","text":"Pair \u00b6 This class couples together a pair of values, which may be of different types (T1 and T2). The individual values can be accessed through its public members first and second. Implementation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #include <bits/stdc++.h> using namespace std ; template < typename A , typename B > struct _pair { A first ; B second ; _pair ( A a , B b ) : first ( a ), second ( b ) {} bool operator ( _pair const & rh ) const { if ( this . first == rh . first ) { return this -> second < rh . second ; } return this -> first < rh . first ; } }; int main () { vector < _pair < int , _pair < int , int >>> v ; for ( int i = 0 ; i < 20 ; i ++ ) { v . push_back ({ 1 , { rand () % 10 , i }}); } sort ( v . begin (), v . end ()); for ( auto x : v ) { cout << x . first << ' ' << x . second . first << ' ' << x . second . second << endl ; } return 0 ; }","title":"Pair"},{"location":"DataStructures/Linear/Pair/#pair","text":"This class couples together a pair of values, which may be of different types (T1 and T2). The individual values can be accessed through its public members first and second.","title":"Pair"},{"location":"DataStructures/Linear/Pair/#implementation","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #include <bits/stdc++.h> using namespace std ; template < typename A , typename B > struct _pair { A first ; B second ; _pair ( A a , B b ) : first ( a ), second ( b ) {} bool operator ( _pair const & rh ) const { if ( this . first == rh . first ) { return this -> second < rh . second ; } return this -> first < rh . first ; } }; int main () { vector < _pair < int , _pair < int , int >>> v ; for ( int i = 0 ; i < 20 ; i ++ ) { v . push_back ({ 1 , { rand () % 10 , i }}); } sort ( v . begin (), v . end ()); for ( auto x : v ) { cout << x . first << ' ' << x . second . first << ' ' << x . second . second << endl ; } return 0 ; }","title":"Implementation"},{"location":"DataStructures/Linear/Stack/","text":"Stack \u00b6 Element deleted from the set is the one most recently inserted; Stack implements last-in, first out or LIFO policy You can use array to implement Stack supported operations \u00b6 insert, delete, empty, top, size 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 template < typename T > class Stack { public : struct Node { T val ; Node * next ; Node () {}; Node ( T val ) : val ( val ), next ( 0 ){}; }; Node * head ; int _size ; Stack () { head = 0 ; _size = 0 ; } void push ( T val ) { Node * temp = new Node ( val ); if ( head == 0 ) { head = temp ; } else { temp -> next = head ; head = temp ; } _size ++ ; } void pop () { if ( empty ()) return ; Node * temp = head ; head = head -> next ; delete temp ; _size -- ; } bool empty () const { return _size == 0 ; } T top () const { return head -> val ; } int size () const { return _size ; } };","title":"Stack"},{"location":"DataStructures/Linear/Stack/#stack","text":"Element deleted from the set is the one most recently inserted; Stack implements last-in, first out or LIFO policy You can use array to implement Stack","title":"Stack"},{"location":"DataStructures/Linear/Stack/#supported_operations","text":"insert, delete, empty, top, size 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 template < typename T > class Stack { public : struct Node { T val ; Node * next ; Node () {}; Node ( T val ) : val ( val ), next ( 0 ){}; }; Node * head ; int _size ; Stack () { head = 0 ; _size = 0 ; } void push ( T val ) { Node * temp = new Node ( val ); if ( head == 0 ) { head = temp ; } else { temp -> next = head ; head = temp ; } _size ++ ; } void pop () { if ( empty ()) return ; Node * temp = head ; head = head -> next ; delete temp ; _size -- ; } bool empty () const { return _size == 0 ; } T top () const { return head -> val ; } int size () const { return _size ; } };","title":"supported operations"},{"location":"DataStructures/Linear/Vector/","text":"Vector \u00b6 Vector is Dynamic array structure in c++ Operations & time complexity \u00b6 Member Function Running Time push_back() $O(1) amortized$ pop_back() $O(1)$ empty() $O(1)$ reserve() $O(n)$ operator [] $O(1)$ Implementation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 using size_t = unsigned long ; template < typename T > class Vector { size_t _size ; size_t _capacity ; T * _buf ; public : // constructors // Vector(SomeType); //\"ordinary constructor\" Vector ( int k ) { _size = 0 ; _capacity = k ; _buf = new T [ _capacity ]; } // Vector(); //default constructor Vector () { _size = 0 ; _capacity = 0 ; _buf = new T [ _capacity ]; } // Vector(const &X); // copy constructor // Vector(&&X); //move constructor // &Vector operator=(const Vector&); //copy assignment: cleanup target and copy // &Vector operator=(Vector&&); // move assignment: cleanup target and move //~Vector(); //destructor: cleanup ~ Vector () { delete [] _buf ; } // capacity: size_t size () { return _size ; } void resize ( size_t n ) { _size = n ; } size_t capacity () { return _capacity ; }; bool empty () { return _size == 0 ; }; // unsigned int max_size(); void reserve ( size_t n ) { //Requests that the vector capacity be at least enough to contain n elements. if ( _size >= n ) return ; T * _temp = new T [ n ]; for ( size_t i = 0 ; i < _size ; i ++ ) { _temp [ i ] = _buf [ i ]; } _capacity = n ; delete [] _buf ; _buf = _temp ; } // shrink_to_fit() //element access: T back (); // operator[]() T & operator []( int idx ) { return _buf [ idx ]; } T operator []( int idx ) const { return _buf [ idx ]; } // at() // front() // data() //Modifiers void clear () { resize ( 0 ); }; void push_back ( T const & val ) { if ( _size == _capacity ) { if ( _capacity ) { reserve ( _capacity << 1 ); } else { reserve ( 1 ); } } _buf [ _size ++ ] = val ; }; void pop_back () { _size -- ; }; // assign() // insert() // erase() // emplace() // emplace_back //Iterators T * begin () { return & _buf [ 0 ]; } T * end () { return & _buf [ 0 ] + _size ; }; //T* rbegin(); //T* rend(); //T* const cbegin(); //T* const cend(); //T* const crbegin(); //T* const crend(); }; Related Problems \u00b6 Letters Shop Related Topics \u00b6 Stack Analysis (Later..) \u00b6 You can add some mathematical things here using KaTex as a block tag $$ T(N) = O(N*M) $$ or as a inline tag $T(N) = O(N) $ Contributers \u00b6 08.13.2019 jchrys","title":"Vector"},{"location":"DataStructures/Linear/Vector/#vector","text":"Vector is Dynamic array structure in c++","title":"Vector"},{"location":"DataStructures/Linear/Vector/#operations_time_complexity","text":"Member Function Running Time push_back() $O(1) amortized$ pop_back() $O(1)$ empty() $O(1)$ reserve() $O(n)$ operator [] $O(1)$","title":"Operations &amp; time complexity"},{"location":"DataStructures/Linear/Vector/#implementation","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 using size_t = unsigned long ; template < typename T > class Vector { size_t _size ; size_t _capacity ; T * _buf ; public : // constructors // Vector(SomeType); //\"ordinary constructor\" Vector ( int k ) { _size = 0 ; _capacity = k ; _buf = new T [ _capacity ]; } // Vector(); //default constructor Vector () { _size = 0 ; _capacity = 0 ; _buf = new T [ _capacity ]; } // Vector(const &X); // copy constructor // Vector(&&X); //move constructor // &Vector operator=(const Vector&); //copy assignment: cleanup target and copy // &Vector operator=(Vector&&); // move assignment: cleanup target and move //~Vector(); //destructor: cleanup ~ Vector () { delete [] _buf ; } // capacity: size_t size () { return _size ; } void resize ( size_t n ) { _size = n ; } size_t capacity () { return _capacity ; }; bool empty () { return _size == 0 ; }; // unsigned int max_size(); void reserve ( size_t n ) { //Requests that the vector capacity be at least enough to contain n elements. if ( _size >= n ) return ; T * _temp = new T [ n ]; for ( size_t i = 0 ; i < _size ; i ++ ) { _temp [ i ] = _buf [ i ]; } _capacity = n ; delete [] _buf ; _buf = _temp ; } // shrink_to_fit() //element access: T back (); // operator[]() T & operator []( int idx ) { return _buf [ idx ]; } T operator []( int idx ) const { return _buf [ idx ]; } // at() // front() // data() //Modifiers void clear () { resize ( 0 ); }; void push_back ( T const & val ) { if ( _size == _capacity ) { if ( _capacity ) { reserve ( _capacity << 1 ); } else { reserve ( 1 ); } } _buf [ _size ++ ] = val ; }; void pop_back () { _size -- ; }; // assign() // insert() // erase() // emplace() // emplace_back //Iterators T * begin () { return & _buf [ 0 ]; } T * end () { return & _buf [ 0 ] + _size ; }; //T* rbegin(); //T* rend(); //T* const cbegin(); //T* const cend(); //T* const crbegin(); //T* const crend(); };","title":"Implementation"},{"location":"DataStructures/Linear/Vector/#related_problems","text":"Letters Shop","title":"Related Problems"},{"location":"DataStructures/Linear/Vector/#related_topics","text":"Stack","title":"Related Topics"},{"location":"DataStructures/Linear/Vector/#analysis_later","text":"You can add some mathematical things here using KaTex as a block tag $$ T(N) = O(N*M) $$ or as a inline tag $T(N) = O(N) $","title":"Analysis (Later..)"},{"location":"DataStructures/Linear/Vector/#contributers","text":"08.13.2019 jchrys","title":"Contributers"},{"location":"DataStructures/Trees/BST/","text":"Binary Search Tree \u00b6 A Search tree is called Binary Search if it satisfies BST property and it's #children $\\leq$ 2. Binary Search Tree Property \u00b6 Let $x$ be a node in a binary search tree. If $y$ is a node in the left subtree of $x$, then $y.key \\leq x.key$. If $y$ is a node in the right subtree of $x$, then $y.key \\leq x.key$. Operations & timeComplexity \u00b6 $h = height(tree)$ Member Function Running Time insert() $\\Omicron(h)$ erase() $\\Omicron(h)$ inorder_tree_walk $\\Theta(n)$ find() $\\Omicron(h)$ minimum() $\\Omicron(h)$ maximum() $\\Omicron(h)$ successor() $\\Omicron(h)$ predecessor() $\\Omicron(h)$ Warning it is not guaranteed that $h = \\Omicron(log(n))$ this binary search tree is not balanced Implementation c++ \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 #include <iostream> using namespace std ; template < typename T > class Set { struct Node { T key ; Node * parent ; Node * left ; Node * right ; Node ( T key ) { this -> key = key ; this -> parent = 0 ; this -> left = 0 ; this -> right = 0 ; } }; Node * root ; unsigned int _size ; public : Set () : root ( 0 ), _size ( 0 ) { //default constructor } void insert ( T key ) { _insert ( new Node ( key )); //insert a key in to set (helper function) } void _insert ( Node * && node ) { // Node * y = 0 ; Node * x = this -> root ; while ( x != 0 ) { y = x ; if ( node -> key < x -> key ) { x = x -> left ; } else if ( node -> key == x -> key ) { return ; } else { x = x -> right ; } } node -> parent = y ; if ( y == 0 ) // when tree is empty -> you could check with _size; this -> root = node ; else if ( node -> key < y -> key ) { y -> left = node ; node -> parent = y ; } else { y -> right = node ; node -> parent = y ; } this -> _size ++ ; } Node * find ( T key ) { Node * x = this -> root ; while ( x != 0 && x -> key != key ) { if ( x -> key > key ) { x = x -> left ; } else { x = x -> right ; } } return x ; } Node * minimum () { _minimum ( this -> root ); } Node * _minimum ( Node * x ) { while ( x -> left != 0 ) { x = x -> left ; } return x ; } Node * maximum () { //returns Node* that with maximum key return _maximum ( this -> root ); } Node * _maximum ( Node * & x ) { while ( x -> right != 0 ) { x = x -> right ; } return x ; } Node * successor ( Node * x ) { if ( x -> right != 0 ) { return _minimum ( x -> right ); } Node * y = x -> parent ; while ( y != 0 && x == y -> right ) { x = y ; y = y -> parent ; } return y ; } Node * predecessor ( Node * x ) { if ( x -> left != 0 ) { return _maximum ( x -> left ); } Node * y = x -> parent ; while ( y != 0 && x == y -> left ) { x = y ; y = y -> parent ; } return y ; } unsigned int size () { return _size ; } void _inorder_tree_travel ( Node * const & node ) { if ( node == 0 ) return ; _inorder_tree_travel ( node -> left ); cout << node -> key << ' ' ; _inorder_tree_travel ( node -> right ); } void inorder_tree_travel () { _inorder_tree_travel ( this -> root ); } void transplant ( Node * u , Node * v ) { if ( u -> parent == 0 ) { this -> root = v ; } else if ( u == u -> parent -> left ) { u -> parent -> left = v ; } else { u -> parent -> right = v ; } if ( v != 0 ) { v -> parent = u -> parent ; } } void erase ( T key ) { _erase ( find ( key )); } void _erase ( Node * target ) { if ( target == 0 ) return ; if ( target -> left == 0 ) transplant ( target , target -> right ); else if ( target -> right == 0 ) transplant ( target , target -> left ); else { Node * y = _minimum ( target -> right ); if ( y -> parent != target ) { transplant ( y , y -> right ); y -> right = target -> right ; y -> right -> parent = y ; } transplant ( target , y ); y -> left = target -> left ; y -> left -> parent = y ; } delete target ; _size -- ; } unsigned int height ( Node * node ) { if ( node == 0 ) return 0 ; unsigned int lDepth = height ( node -> left ); unsigned int rDepth = height ( node -> right ); if ( lDepth > rDepth ) return lDepth + 1 ; return rDepth + 1 ; } unsigned int tree_height () { return height ( this -> root ); } }; int main () { Set < int > s ; // if input's are random; cout << \"Naive Binary Search Tree implementation\" << endl ; cout << \"-------BEST-CASE(random inputs)--------\" << endl ; cout << \"input: 10,000 random integers\" << endl ; for ( int i = 0 ; i < 10000 ; i ++ ) { s . insert ( rand () % 1000000 ); } cout << \"-----------------results----------------\" << endl ; cout << \"tree_height: \" << s . tree_height () << endl ; cout << endl << endl << endl ; cout << \"------WORST-CASE(sorted_inputs)---------\" << endl ; cout << \"input: [1, 2, 3, ..., 10000]\" << endl ; Set < int > worst ; for ( int i = 1 ; i <= 10000 ; i ++ ) { worst . insert ( i ); } cout << \"-----------------results----------------\" << endl ; cout << \"tree_height: \" << worst . tree_height () << endl ; return 0 ; } Related Problems \u00b6 Related Topics \u00b6 Analysis (Later..) \u00b6 You can add some mathematical things here using KaTex as a block tag $$ T(N) = O(N*M) $$ or as a inline tag $T(N) = O(N) $ Contributers \u00b6 08.15.2019 jchrys","title":"BST"},{"location":"DataStructures/Trees/BST/#binary_search_tree","text":"A Search tree is called Binary Search if it satisfies BST property and it's #children $\\leq$ 2.","title":"Binary Search Tree"},{"location":"DataStructures/Trees/BST/#binary_search_tree_property","text":"Let $x$ be a node in a binary search tree. If $y$ is a node in the left subtree of $x$, then $y.key \\leq x.key$. If $y$ is a node in the right subtree of $x$, then $y.key \\leq x.key$.","title":"Binary Search Tree Property"},{"location":"DataStructures/Trees/BST/#operations_timecomplexity","text":"$h = height(tree)$ Member Function Running Time insert() $\\Omicron(h)$ erase() $\\Omicron(h)$ inorder_tree_walk $\\Theta(n)$ find() $\\Omicron(h)$ minimum() $\\Omicron(h)$ maximum() $\\Omicron(h)$ successor() $\\Omicron(h)$ predecessor() $\\Omicron(h)$ Warning it is not guaranteed that $h = \\Omicron(log(n))$ this binary search tree is not balanced","title":"Operations &amp; timeComplexity"},{"location":"DataStructures/Trees/BST/#implementation_c","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 #include <iostream> using namespace std ; template < typename T > class Set { struct Node { T key ; Node * parent ; Node * left ; Node * right ; Node ( T key ) { this -> key = key ; this -> parent = 0 ; this -> left = 0 ; this -> right = 0 ; } }; Node * root ; unsigned int _size ; public : Set () : root ( 0 ), _size ( 0 ) { //default constructor } void insert ( T key ) { _insert ( new Node ( key )); //insert a key in to set (helper function) } void _insert ( Node * && node ) { // Node * y = 0 ; Node * x = this -> root ; while ( x != 0 ) { y = x ; if ( node -> key < x -> key ) { x = x -> left ; } else if ( node -> key == x -> key ) { return ; } else { x = x -> right ; } } node -> parent = y ; if ( y == 0 ) // when tree is empty -> you could check with _size; this -> root = node ; else if ( node -> key < y -> key ) { y -> left = node ; node -> parent = y ; } else { y -> right = node ; node -> parent = y ; } this -> _size ++ ; } Node * find ( T key ) { Node * x = this -> root ; while ( x != 0 && x -> key != key ) { if ( x -> key > key ) { x = x -> left ; } else { x = x -> right ; } } return x ; } Node * minimum () { _minimum ( this -> root ); } Node * _minimum ( Node * x ) { while ( x -> left != 0 ) { x = x -> left ; } return x ; } Node * maximum () { //returns Node* that with maximum key return _maximum ( this -> root ); } Node * _maximum ( Node * & x ) { while ( x -> right != 0 ) { x = x -> right ; } return x ; } Node * successor ( Node * x ) { if ( x -> right != 0 ) { return _minimum ( x -> right ); } Node * y = x -> parent ; while ( y != 0 && x == y -> right ) { x = y ; y = y -> parent ; } return y ; } Node * predecessor ( Node * x ) { if ( x -> left != 0 ) { return _maximum ( x -> left ); } Node * y = x -> parent ; while ( y != 0 && x == y -> left ) { x = y ; y = y -> parent ; } return y ; } unsigned int size () { return _size ; } void _inorder_tree_travel ( Node * const & node ) { if ( node == 0 ) return ; _inorder_tree_travel ( node -> left ); cout << node -> key << ' ' ; _inorder_tree_travel ( node -> right ); } void inorder_tree_travel () { _inorder_tree_travel ( this -> root ); } void transplant ( Node * u , Node * v ) { if ( u -> parent == 0 ) { this -> root = v ; } else if ( u == u -> parent -> left ) { u -> parent -> left = v ; } else { u -> parent -> right = v ; } if ( v != 0 ) { v -> parent = u -> parent ; } } void erase ( T key ) { _erase ( find ( key )); } void _erase ( Node * target ) { if ( target == 0 ) return ; if ( target -> left == 0 ) transplant ( target , target -> right ); else if ( target -> right == 0 ) transplant ( target , target -> left ); else { Node * y = _minimum ( target -> right ); if ( y -> parent != target ) { transplant ( y , y -> right ); y -> right = target -> right ; y -> right -> parent = y ; } transplant ( target , y ); y -> left = target -> left ; y -> left -> parent = y ; } delete target ; _size -- ; } unsigned int height ( Node * node ) { if ( node == 0 ) return 0 ; unsigned int lDepth = height ( node -> left ); unsigned int rDepth = height ( node -> right ); if ( lDepth > rDepth ) return lDepth + 1 ; return rDepth + 1 ; } unsigned int tree_height () { return height ( this -> root ); } }; int main () { Set < int > s ; // if input's are random; cout << \"Naive Binary Search Tree implementation\" << endl ; cout << \"-------BEST-CASE(random inputs)--------\" << endl ; cout << \"input: 10,000 random integers\" << endl ; for ( int i = 0 ; i < 10000 ; i ++ ) { s . insert ( rand () % 1000000 ); } cout << \"-----------------results----------------\" << endl ; cout << \"tree_height: \" << s . tree_height () << endl ; cout << endl << endl << endl ; cout << \"------WORST-CASE(sorted_inputs)---------\" << endl ; cout << \"input: [1, 2, 3, ..., 10000]\" << endl ; Set < int > worst ; for ( int i = 1 ; i <= 10000 ; i ++ ) { worst . insert ( i ); } cout << \"-----------------results----------------\" << endl ; cout << \"tree_height: \" << worst . tree_height () << endl ; return 0 ; }","title":"Implementation c++"},{"location":"DataStructures/Trees/BST/#related_problems","text":"","title":"Related Problems"},{"location":"DataStructures/Trees/BST/#related_topics","text":"","title":"Related Topics"},{"location":"DataStructures/Trees/BST/#analysis_later","text":"You can add some mathematical things here using KaTex as a block tag $$ T(N) = O(N*M) $$ or as a inline tag $T(N) = O(N) $","title":"Analysis (Later..)"},{"location":"DataStructures/Trees/BST/#contributers","text":"08.15.2019 jchrys","title":"Contributers"},{"location":"DataStructures/Trees/BinaryIndexedTree/","text":"Binary indexed tree \u00b6 A binary index tree or a Fenwick tree can be seen as a dynamic variant of a prefix sum array. It supports two $\\Omicron(lgn)$ time operations on an array: processing a range sum query and updating a value The advantage of binary indexed tree is that it allows us to efficiently update array values between sum queries. This would not be possible using a [prefix sum array], because after each update, it would be neccessary to build the whole prefix sum array again in $\\Omicron(n)$ time. Time complexity \u00b6 operation time complexity sum(a, b) $\\Omicron(lgn)$ update(i, x) $\\Omicron(lgn)$ Structure \u00b6 Even if the name of the structure is a binary indexed tree , it is usually represented as an array. we assume that all arrays are one-indexed, because it makes the implement easier Let $p(k)$ denote the largest power of two that divides $k$. We store a binary indexed tree as an array tree such that $$ \\text{tree}[k] = \\text{sum}(k-p(k) + 1, k) $$ each position $k$ contains the sum of values in a range of the original array whose length is $p(k)$ and that ends at position $k$. Using a binary indexed tree, any value of $\\text{sum}(1, k)$ can be calculated in $\\Omicron(lgn)$ time, because a range $[1, k]$ can always be divded into $\\Omicron(lgn)$ ranges whose sums are stored in the tree. Range sum query \u00b6 To calculate the value of $\\text{sum}(a, b)$ where a > 1: $$ \\text{sum}(a, b) = \\text{sum}(1, b) - \\text{sum}(1, a - 1) $$ Since we can calculate both $\\text{sum}(1, b)$ and $\\text{sum}(1, a-1)$ in $\\Omicron(lgn)$ time, the total time complexity is $\\Omicron(lgn)$. Updating \u00b6 After updating a value in the original array, several values in the binary indexed tree should be updated. Since each array element belong to $\\Omicron(lgn)$ ranges in the binary indexed tree, it suffices to update $\\Omicron(lgn)$ values in the tree. Implementation \u00b6 The key fact needed is that we can calculate anyvalue of $p(k)$ using the formula $$ p(k)=k\\&-k $$ The following function calculates the value of $sum(1, k)$ 1 2 3 4 5 6 7 8 int sum ( int k ) { int s = 0 ; while ( s >= 1 ) { s += tree [ k ]; k -= k &- k ; } return s ; } The following function increases the array value at position $k$ by $x$. 1 2 3 4 5 6 void add ( int k , int x ) { while ( k <= n ) { tree [ k ] += x ; k += k &- k ; } } Implementation (0-indexed) \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #include <bits/stdc++.h> int N = 10000 ; int f [ N ]{}; void upd ( int k , int x ) { for (; k < N ; k |= k + 1 ) f [ k ] += x ; } int get ( int pos ) { int res = 0 ; for (; pos >= 0 ; pos = ( pos & ( pos + 1 )) - 1 ) res += f [ pos ]; return res ; } int get ( int l , int r ) { return get ( r ) - get ( l - 1 ); } Problems \u00b6 \uad6c\uac04 \ud569 \uad6c\ud558\uae30","title":"Binary Indexed Tree"},{"location":"DataStructures/Trees/BinaryIndexedTree/#binary_indexed_tree","text":"A binary index tree or a Fenwick tree can be seen as a dynamic variant of a prefix sum array. It supports two $\\Omicron(lgn)$ time operations on an array: processing a range sum query and updating a value The advantage of binary indexed tree is that it allows us to efficiently update array values between sum queries. This would not be possible using a [prefix sum array], because after each update, it would be neccessary to build the whole prefix sum array again in $\\Omicron(n)$ time.","title":"Binary indexed tree"},{"location":"DataStructures/Trees/BinaryIndexedTree/#time_complexity","text":"operation time complexity sum(a, b) $\\Omicron(lgn)$ update(i, x) $\\Omicron(lgn)$","title":"Time complexity"},{"location":"DataStructures/Trees/BinaryIndexedTree/#structure","text":"Even if the name of the structure is a binary indexed tree , it is usually represented as an array. we assume that all arrays are one-indexed, because it makes the implement easier Let $p(k)$ denote the largest power of two that divides $k$. We store a binary indexed tree as an array tree such that $$ \\text{tree}[k] = \\text{sum}(k-p(k) + 1, k) $$ each position $k$ contains the sum of values in a range of the original array whose length is $p(k)$ and that ends at position $k$. Using a binary indexed tree, any value of $\\text{sum}(1, k)$ can be calculated in $\\Omicron(lgn)$ time, because a range $[1, k]$ can always be divded into $\\Omicron(lgn)$ ranges whose sums are stored in the tree.","title":"Structure"},{"location":"DataStructures/Trees/BinaryIndexedTree/#range_sum_query","text":"To calculate the value of $\\text{sum}(a, b)$ where a > 1: $$ \\text{sum}(a, b) = \\text{sum}(1, b) - \\text{sum}(1, a - 1) $$ Since we can calculate both $\\text{sum}(1, b)$ and $\\text{sum}(1, a-1)$ in $\\Omicron(lgn)$ time, the total time complexity is $\\Omicron(lgn)$.","title":"Range sum query"},{"location":"DataStructures/Trees/BinaryIndexedTree/#updating","text":"After updating a value in the original array, several values in the binary indexed tree should be updated. Since each array element belong to $\\Omicron(lgn)$ ranges in the binary indexed tree, it suffices to update $\\Omicron(lgn)$ values in the tree.","title":"Updating"},{"location":"DataStructures/Trees/BinaryIndexedTree/#implementation","text":"The key fact needed is that we can calculate anyvalue of $p(k)$ using the formula $$ p(k)=k\\&-k $$ The following function calculates the value of $sum(1, k)$ 1 2 3 4 5 6 7 8 int sum ( int k ) { int s = 0 ; while ( s >= 1 ) { s += tree [ k ]; k -= k &- k ; } return s ; } The following function increases the array value at position $k$ by $x$. 1 2 3 4 5 6 void add ( int k , int x ) { while ( k <= n ) { tree [ k ] += x ; k += k &- k ; } }","title":"Implementation"},{"location":"DataStructures/Trees/BinaryIndexedTree/#implementation_0-indexed","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #include <bits/stdc++.h> int N = 10000 ; int f [ N ]{}; void upd ( int k , int x ) { for (; k < N ; k |= k + 1 ) f [ k ] += x ; } int get ( int pos ) { int res = 0 ; for (; pos >= 0 ; pos = ( pos & ( pos + 1 )) - 1 ) res += f [ pos ]; return res ; } int get ( int l , int r ) { return get ( r ) - get ( l - 1 ); }","title":"Implementation (0-indexed)"},{"location":"DataStructures/Trees/BinaryIndexedTree/#problems","text":"\uad6c\uac04 \ud569 \uad6c\ud558\uae30","title":"Problems"},{"location":"DataStructures/Trees/RedBlackTree/","text":"Red Black Tree \u00b6 Red Black Tree is balanced binary search tree with one extra bit of storage per node: color Red Black Tree satisfies the Red-Black-Properties Red-Black-Properties Every node is black or red The root is black Every leaf(NIL) is black if a node is red, then both its children are black For each node, all simple paths from the node to descendant leaves contains the same number of black nodes. Operations & time complexity \u00b6 $N$ = number of elements in Tree Member Function Running Time insert() $\\Omicron(\\lg(N))$ erase() $\\Omicron(\\lg(N))$ inorder_tree_walk $\\Theta(N)$ find() $\\Omicron(\\lg(N))$ minimum() $\\Omicron(\\lg(N))$ maximum() $\\Omicron(\\lg(N))$ successor() $\\Omicron(\\lg(N))$ predecessor() $\\Omicron(\\lg(N))$ Note Red Black Tree is Balanced Binary Search Tree It is guaranteed that height of the tree is $\\Omicron(\\lg(N))$ in worst case Implementation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 #include <bits/stdc++.h> using namespace std ; template < typename K , typename V > class Map { struct Node { K key ; V val ; bool color ; // 0: red, 1:black Node * p ; Node * left ; Node * right ; Node ( Map & out ) : key (), val (), color ( 1 ), p ( out . NIL ), left ( out . NIL ), right ( out . NIL ){} Node ( Map & out , K key , V val ) : key ( key ), val ( val ), color ( 0 ), p ( out . NIL ), left ( out . NIL ), right ( out . NIL ){} }; Node * root ; void left_rotate ( Node * x ) { Node * y = x -> right ; // y is x's right child x -> right = y -> left ; // set y's left child to x's right child; x -> right -> p = x ; // set parent y -> p = x -> p ; //set x's parent pointing to y; if ( x -> p == NIL ) { // if x is root this -> root = y ; } else if ( x == x -> p -> left ) { //if x is left child x -> p -> left = y ; } else { x -> p -> right = y ; } x -> p = y ; // y is x's parent y -> left = x ; // x is y's left child } void right_rotate ( Node * x ) { Node * y = x -> left ; x -> left = y -> right ; x -> left -> p = x ; y -> p = x -> p ; if ( x -> p == NIL ) { this -> root = y ; } else if ( x == x -> p -> left ) { x -> p -> left = y ; } else { x -> p -> right = y ; } x -> p = y ; y -> right = x ; } void transplant ( Node * u , Node * v ) { // gives u's parent relations to v if ( u -> p == NIL ) { root = v ; } else if ( u == u -> p -> right ) { u -> p -> right = v ; } else { u -> p -> left = v ; } v -> p = u -> p ; // unconditionally because NIL can have parent also; } public : Node * NIL ; Map () { NIL = new Node ( * this ); root = NIL ; } void insert_fixup ( Node * & z ) { while ( z -> p -> color == 0 ) { Node * y ; // z's uncle if ( z -> p == z -> p -> p -> left ) { // when z's parent is left child y = z -> p -> p -> right ; if ( y -> color == 0 ) { // if uncle is red, uncles parent should be black z -> p -> color = 1 ; // recoloring and goes up y -> color = 1 ; z -> p -> p -> color = 0 ; z = z -> p -> p ; } else { if ( z == z -> p -> right ) { // if uncle is black and z is right child z = z -> p ; left_rotate ( z ); } // if uncle is black and z is right child z -> p -> color = 1 ; z -> p -> p -> color = 0 ; right_rotate ( z -> p -> p ); } } else { // when z's parent is right child y = z -> p -> p -> left ; if ( y -> color == 0 ) { z -> p -> color = 1 ; y -> color = 1 ; z -> p -> p -> color = 0 ; z = z -> p -> p ; } else { // if uncle's color is black if ( z == z -> p -> left ) { //when z is right child z = z -> p ; right_rotate ( z ); } z -> p -> color = 1 ; z -> p -> p -> color = 0 ; left_rotate ( z -> p -> p ); } } } this -> root -> color = 1 ; } void insert ( K key , V val ) { Node * z = new Node ( * this , key , val ); Node * y = NIL ; Node * x = this -> root ; while ( x != NIL ) { y = x ; if ( z -> key < x -> key ) { x = x -> left ; } else { x = x -> right ; } } z -> p = y ; if ( y == NIL ) { this -> root = z ; } else if ( z -> key < y -> key ) { y -> left = z ; } else { y -> right = z ; } // z->left = NIL; // z->right = ZIL; // z->color = 0; insert_fixup ( z ); } Node * find ( K key ) { Node * x = root ; while ( x != NIL && x -> key != key ) { if ( key < x -> key ) { x = x -> left ; } else { x = x -> right ; } } return x ; } Node * minimum ( Node * x ) { while ( x -> left != NIL ) { x = x -> left ; } return x ; } void erase_fixup ( Node * x ) { Node * w ; // sibling of x; while ( x != this -> root && x -> color == 1 ) { //only if x is black and not root if ( x == x -> p -> left ) { w = x -> p -> right ; if ( w -> color == 0 ) { // turns to case2, 3 or 4; w -> color = 1 ; x -> p -> color = 0 ; left_rotate ( x -> p ); w = x -> p -> right ; } if ( w -> left -> color == 1 && w -> right -> color == 1 ) { // case2 w -> color = 0 ; x = x -> p ; } else { if ( w -> right -> color == 1 ) { //case3 -> turns to case4 w -> left -> color = 1 ; w -> color = 0 ; right_rotate ( w ); w = x -> p -> right ; } w -> color = x -> p -> color ; // case4 -> we can make legit red-black tree x -> p -> color = 1 ; w -> right -> color = 1 ; left_rotate ( x -> p ); x = root ; } } else { // x == x->p->right w = x -> p -> left ; if ( w -> color == 0 ) { w -> color = 1 ; x -> p -> color = 0 ; right_rotate ( x -> p ); w = x -> p -> left ; } if ( w -> left -> color == 1 && w -> right -> color == 1 ) { w -> color = 0 ; x = x -> p ; } else { if ( w -> left -> color == 1 ) { w -> color = 0 ; w -> right -> color = 1 ; left_rotate ( w ); w = x -> p -> left ; } w -> color = w -> p -> color ; w -> p -> color = 1 ; w -> left -> color = 1 ; right_rotate ( x -> p ); x = root ; } } } x -> color = 1 ; } void erase ( Node * z ) { Node * y = z ; bool y_original_color = y -> color ; Node * x ; if ( z -> left == NIL ) { x = z -> right ; transplant ( z , z -> right ); } else if ( z -> right == NIL ) { x = z -> left ; transplant ( z , z -> left ); } else { y = minimum ( z -> right ); y_original_color = y -> color ; x = y -> right ; if ( y -> p == z ) { x -> p = y ; // incase of x is NIL!! we need to find it's parent! } else { transplant ( y , y -> right ); y -> right = z -> right ; y -> right -> p = y ; } transplant ( z , y ); y -> left = z -> left ; y -> left -> p = y ; y -> color = z -> color ; } if ( y_original_color == 1 ) { erase_fixup ( x ); } } void rb_printer ( Node * node , int indent ) { //prints red & black tree int count = 4 ; if ( node == NIL ) return ; indent += count ; rb_printer ( node -> right , indent ); cout << endl ; for ( int i = count ; i < indent ; i ++ ) { cout << \" \" ; } cout << ( node -> color == 0 ? \" \\033 [1;31m\" : \"\" ) << ( node == node -> p -> left ? \"l\" : \"r\" ) << node -> key << ( node -> color == 0 ? \" \\033 [0m\" : \"\" ) << endl ; rb_printer ( node -> left , indent ); } void print () { rb_printer ( this -> root , 0 ); } }; int main () { Map < int , int > m ; for ( int i = 0 ; i < 20 ; i ++ ) { m . insert ( rand () % 20 , 1 ); } m . print (); cout << \"deleting ---\" << endl ;; for ( int i = 0 ; i < 20 ; i ++ ) { int key = rand () % 20 ; auto it = m . find ( key ); cout << \"delete: \" << key << endl ; if ( it != m . NIL ) { cout << \"key exist... deleting...\" ; m . erase ( it ); m . print (); } else { cout << \"key not exist\" << endl ; } } return 0 ; } Related Problems \u00b6 NOT ADDED YET Related Topics \u00b6 BinarySearchTree Analysis (Optional) \u00b6 You can add some mathematical things here using KaTex as a block tag $$ T(N) = O(N*M) $$ or as a inline tag $T(N) = O(N) $ Contributers (Optional) \u00b6 08.18.2019 JCHRYS","title":"RedBlackTree"},{"location":"DataStructures/Trees/RedBlackTree/#red_black_tree","text":"Red Black Tree is balanced binary search tree with one extra bit of storage per node: color Red Black Tree satisfies the Red-Black-Properties Red-Black-Properties Every node is black or red The root is black Every leaf(NIL) is black if a node is red, then both its children are black For each node, all simple paths from the node to descendant leaves contains the same number of black nodes.","title":"Red Black Tree"},{"location":"DataStructures/Trees/RedBlackTree/#operations_time_complexity","text":"$N$ = number of elements in Tree Member Function Running Time insert() $\\Omicron(\\lg(N))$ erase() $\\Omicron(\\lg(N))$ inorder_tree_walk $\\Theta(N)$ find() $\\Omicron(\\lg(N))$ minimum() $\\Omicron(\\lg(N))$ maximum() $\\Omicron(\\lg(N))$ successor() $\\Omicron(\\lg(N))$ predecessor() $\\Omicron(\\lg(N))$ Note Red Black Tree is Balanced Binary Search Tree It is guaranteed that height of the tree is $\\Omicron(\\lg(N))$ in worst case","title":"Operations &amp; time complexity"},{"location":"DataStructures/Trees/RedBlackTree/#implementation","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 #include <bits/stdc++.h> using namespace std ; template < typename K , typename V > class Map { struct Node { K key ; V val ; bool color ; // 0: red, 1:black Node * p ; Node * left ; Node * right ; Node ( Map & out ) : key (), val (), color ( 1 ), p ( out . NIL ), left ( out . NIL ), right ( out . NIL ){} Node ( Map & out , K key , V val ) : key ( key ), val ( val ), color ( 0 ), p ( out . NIL ), left ( out . NIL ), right ( out . NIL ){} }; Node * root ; void left_rotate ( Node * x ) { Node * y = x -> right ; // y is x's right child x -> right = y -> left ; // set y's left child to x's right child; x -> right -> p = x ; // set parent y -> p = x -> p ; //set x's parent pointing to y; if ( x -> p == NIL ) { // if x is root this -> root = y ; } else if ( x == x -> p -> left ) { //if x is left child x -> p -> left = y ; } else { x -> p -> right = y ; } x -> p = y ; // y is x's parent y -> left = x ; // x is y's left child } void right_rotate ( Node * x ) { Node * y = x -> left ; x -> left = y -> right ; x -> left -> p = x ; y -> p = x -> p ; if ( x -> p == NIL ) { this -> root = y ; } else if ( x == x -> p -> left ) { x -> p -> left = y ; } else { x -> p -> right = y ; } x -> p = y ; y -> right = x ; } void transplant ( Node * u , Node * v ) { // gives u's parent relations to v if ( u -> p == NIL ) { root = v ; } else if ( u == u -> p -> right ) { u -> p -> right = v ; } else { u -> p -> left = v ; } v -> p = u -> p ; // unconditionally because NIL can have parent also; } public : Node * NIL ; Map () { NIL = new Node ( * this ); root = NIL ; } void insert_fixup ( Node * & z ) { while ( z -> p -> color == 0 ) { Node * y ; // z's uncle if ( z -> p == z -> p -> p -> left ) { // when z's parent is left child y = z -> p -> p -> right ; if ( y -> color == 0 ) { // if uncle is red, uncles parent should be black z -> p -> color = 1 ; // recoloring and goes up y -> color = 1 ; z -> p -> p -> color = 0 ; z = z -> p -> p ; } else { if ( z == z -> p -> right ) { // if uncle is black and z is right child z = z -> p ; left_rotate ( z ); } // if uncle is black and z is right child z -> p -> color = 1 ; z -> p -> p -> color = 0 ; right_rotate ( z -> p -> p ); } } else { // when z's parent is right child y = z -> p -> p -> left ; if ( y -> color == 0 ) { z -> p -> color = 1 ; y -> color = 1 ; z -> p -> p -> color = 0 ; z = z -> p -> p ; } else { // if uncle's color is black if ( z == z -> p -> left ) { //when z is right child z = z -> p ; right_rotate ( z ); } z -> p -> color = 1 ; z -> p -> p -> color = 0 ; left_rotate ( z -> p -> p ); } } } this -> root -> color = 1 ; } void insert ( K key , V val ) { Node * z = new Node ( * this , key , val ); Node * y = NIL ; Node * x = this -> root ; while ( x != NIL ) { y = x ; if ( z -> key < x -> key ) { x = x -> left ; } else { x = x -> right ; } } z -> p = y ; if ( y == NIL ) { this -> root = z ; } else if ( z -> key < y -> key ) { y -> left = z ; } else { y -> right = z ; } // z->left = NIL; // z->right = ZIL; // z->color = 0; insert_fixup ( z ); } Node * find ( K key ) { Node * x = root ; while ( x != NIL && x -> key != key ) { if ( key < x -> key ) { x = x -> left ; } else { x = x -> right ; } } return x ; } Node * minimum ( Node * x ) { while ( x -> left != NIL ) { x = x -> left ; } return x ; } void erase_fixup ( Node * x ) { Node * w ; // sibling of x; while ( x != this -> root && x -> color == 1 ) { //only if x is black and not root if ( x == x -> p -> left ) { w = x -> p -> right ; if ( w -> color == 0 ) { // turns to case2, 3 or 4; w -> color = 1 ; x -> p -> color = 0 ; left_rotate ( x -> p ); w = x -> p -> right ; } if ( w -> left -> color == 1 && w -> right -> color == 1 ) { // case2 w -> color = 0 ; x = x -> p ; } else { if ( w -> right -> color == 1 ) { //case3 -> turns to case4 w -> left -> color = 1 ; w -> color = 0 ; right_rotate ( w ); w = x -> p -> right ; } w -> color = x -> p -> color ; // case4 -> we can make legit red-black tree x -> p -> color = 1 ; w -> right -> color = 1 ; left_rotate ( x -> p ); x = root ; } } else { // x == x->p->right w = x -> p -> left ; if ( w -> color == 0 ) { w -> color = 1 ; x -> p -> color = 0 ; right_rotate ( x -> p ); w = x -> p -> left ; } if ( w -> left -> color == 1 && w -> right -> color == 1 ) { w -> color = 0 ; x = x -> p ; } else { if ( w -> left -> color == 1 ) { w -> color = 0 ; w -> right -> color = 1 ; left_rotate ( w ); w = x -> p -> left ; } w -> color = w -> p -> color ; w -> p -> color = 1 ; w -> left -> color = 1 ; right_rotate ( x -> p ); x = root ; } } } x -> color = 1 ; } void erase ( Node * z ) { Node * y = z ; bool y_original_color = y -> color ; Node * x ; if ( z -> left == NIL ) { x = z -> right ; transplant ( z , z -> right ); } else if ( z -> right == NIL ) { x = z -> left ; transplant ( z , z -> left ); } else { y = minimum ( z -> right ); y_original_color = y -> color ; x = y -> right ; if ( y -> p == z ) { x -> p = y ; // incase of x is NIL!! we need to find it's parent! } else { transplant ( y , y -> right ); y -> right = z -> right ; y -> right -> p = y ; } transplant ( z , y ); y -> left = z -> left ; y -> left -> p = y ; y -> color = z -> color ; } if ( y_original_color == 1 ) { erase_fixup ( x ); } } void rb_printer ( Node * node , int indent ) { //prints red & black tree int count = 4 ; if ( node == NIL ) return ; indent += count ; rb_printer ( node -> right , indent ); cout << endl ; for ( int i = count ; i < indent ; i ++ ) { cout << \" \" ; } cout << ( node -> color == 0 ? \" \\033 [1;31m\" : \"\" ) << ( node == node -> p -> left ? \"l\" : \"r\" ) << node -> key << ( node -> color == 0 ? \" \\033 [0m\" : \"\" ) << endl ; rb_printer ( node -> left , indent ); } void print () { rb_printer ( this -> root , 0 ); } }; int main () { Map < int , int > m ; for ( int i = 0 ; i < 20 ; i ++ ) { m . insert ( rand () % 20 , 1 ); } m . print (); cout << \"deleting ---\" << endl ;; for ( int i = 0 ; i < 20 ; i ++ ) { int key = rand () % 20 ; auto it = m . find ( key ); cout << \"delete: \" << key << endl ; if ( it != m . NIL ) { cout << \"key exist... deleting...\" ; m . erase ( it ); m . print (); } else { cout << \"key not exist\" << endl ; } } return 0 ; }","title":"Implementation"},{"location":"DataStructures/Trees/RedBlackTree/#related_problems","text":"NOT ADDED YET","title":"Related Problems"},{"location":"DataStructures/Trees/RedBlackTree/#related_topics","text":"BinarySearchTree","title":"Related Topics"},{"location":"DataStructures/Trees/RedBlackTree/#analysis_optional","text":"You can add some mathematical things here using KaTex as a block tag $$ T(N) = O(N*M) $$ or as a inline tag $T(N) = O(N) $","title":"Analysis (Optional)"},{"location":"DataStructures/Trees/RedBlackTree/#contributers_optional","text":"08.18.2019 JCHRYS","title":"Contributers (Optional)"},{"location":"DataStructures/Trees/SearchTree/","text":"Search Tree \u00b6 The Search tree data structure supports many dynamic-set operations, including $SEARCH,\\ MINIMUM,\\ MAXIMUM,\\ PREDECESSOR,$ $ SUCCESSOR,\\ INSERT,\\ DELETE$ so we can use a search tree as a $dictionary$ and as a $priority\\ queue$ DataStructure \u00b6 We can represent it as linked objects Each Node Containing \u00b6 Each Node Contatining pointers : rightChild , leftChild , parent","title":"SearchTree"},{"location":"DataStructures/Trees/SearchTree/#search_tree","text":"The Search tree data structure supports many dynamic-set operations, including $SEARCH,\\ MINIMUM,\\ MAXIMUM,\\ PREDECESSOR,$ $ SUCCESSOR,\\ INSERT,\\ DELETE$ so we can use a search tree as a $dictionary$ and as a $priority\\ queue$","title":"Search Tree"},{"location":"DataStructures/Trees/SearchTree/#datastructure","text":"We can represent it as linked objects","title":"DataStructure"},{"location":"DataStructures/Trees/SearchTree/#each_node_containing","text":"Each Node Contatining pointers : rightChild , leftChild , parent","title":"Each Node Containing"},{"location":"DataStructures/Trees/SegmentTree/","text":"Segment Tree \u00b6 A segment tree is a data structure that supports two operations: processing a range query and updating an array value. Segment trees can support sum queries, minimum and maximum queries and many other queries so that both operations work in $\\Omicron(lgn)$ time. DataStructure Operations Binary indexed tree sum queries Segment tree sum, minimum, maximum and many others Structure \u00b6 A segment tree is a binary tree such that the nodes on the bottom level of the tree corresponds to the array elements, and the other nodes contain information needed for processing range queries. Time complexity \u00b6 Operation Time complexity build() $\\Omicron(n)$ query(l, r) $\\Omicron(\\log(n))$ update(pos, x) $\\Omicron(\\log(n))$ Implementation \u00b6 Following implementation support single array element update 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 const int N = 1e5 ; // limit for array size int n ; // array size int t [ 2 * N ]; void build () { // build the tree for ( int i = n - 1 ; i > 0 ; i -- ) t [ i ] = t [ i << 1 ] + t [ i << 1 | 1 ]; } void modify ( int p , int value ) { // set value at position p; for ( t [ p += n ] = value ; p > 1 ; p >>= 1 ) t [ p >> 1 ] = t [ p ] + t [ p ^ 1 ]; } int query ( int l , int r ) { // sum on interval [l, r) int res = 0 ; for ( l += n , r += n ; l < r ; l >>= 1 , r >>= 1 ) { if ( l & 1 ) res += t [ l ++ ]; if ( r & 1 ) res += t [ -- r ]; } return res ; } int main () { scanf ( \"%d\" , & n ); for ( int i = 0 ; i < n ; i ++ ) scanf ( \"%d\" , t + n + i ); build (); modify ( 0 , 1 ); printf ( \"%d \\n \" , query ( 3 , 11 )); return 0 ; } Implementation \u00b6 Problems \u00b6 \uad6c\uac04 \uacf1 \uad6c\ud558\uae30 \uad6c\uac04 \ud569 \uad6c\ud558\uae30 \ucd5c\uc19f\uac12 \ucd5c\uc19f\uac12\uacfc \ucd5c\ub313\uac12","title":"Segment Tree"},{"location":"DataStructures/Trees/SegmentTree/#segment_tree","text":"A segment tree is a data structure that supports two operations: processing a range query and updating an array value. Segment trees can support sum queries, minimum and maximum queries and many other queries so that both operations work in $\\Omicron(lgn)$ time. DataStructure Operations Binary indexed tree sum queries Segment tree sum, minimum, maximum and many others","title":"Segment Tree"},{"location":"DataStructures/Trees/SegmentTree/#structure","text":"A segment tree is a binary tree such that the nodes on the bottom level of the tree corresponds to the array elements, and the other nodes contain information needed for processing range queries.","title":"Structure"},{"location":"DataStructures/Trees/SegmentTree/#time_complexity","text":"Operation Time complexity build() $\\Omicron(n)$ query(l, r) $\\Omicron(\\log(n))$ update(pos, x) $\\Omicron(\\log(n))$","title":"Time complexity"},{"location":"DataStructures/Trees/SegmentTree/#implementation","text":"Following implementation support single array element update 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 const int N = 1e5 ; // limit for array size int n ; // array size int t [ 2 * N ]; void build () { // build the tree for ( int i = n - 1 ; i > 0 ; i -- ) t [ i ] = t [ i << 1 ] + t [ i << 1 | 1 ]; } void modify ( int p , int value ) { // set value at position p; for ( t [ p += n ] = value ; p > 1 ; p >>= 1 ) t [ p >> 1 ] = t [ p ] + t [ p ^ 1 ]; } int query ( int l , int r ) { // sum on interval [l, r) int res = 0 ; for ( l += n , r += n ; l < r ; l >>= 1 , r >>= 1 ) { if ( l & 1 ) res += t [ l ++ ]; if ( r & 1 ) res += t [ -- r ]; } return res ; } int main () { scanf ( \"%d\" , & n ); for ( int i = 0 ; i < n ; i ++ ) scanf ( \"%d\" , t + n + i ); build (); modify ( 0 , 1 ); printf ( \"%d \\n \" , query ( 3 , 11 )); return 0 ; }","title":"Implementation"},{"location":"DataStructures/Trees/SegmentTree/#implementation_1","text":"","title":"Implementation"},{"location":"DataStructures/Trees/SegmentTree/#problems","text":"\uad6c\uac04 \uacf1 \uad6c\ud558\uae30 \uad6c\uac04 \ud569 \uad6c\ud558\uae30 \ucd5c\uc19f\uac12 \ucd5c\uc19f\uac12\uacfc \ucd5c\ub313\uac12","title":"Problems"},{"location":"Language/Class/","text":"Class \u00b6 C++ classes are a tool for creating new types that can be used conveniently as builtin types The Fundamental idea in defining a new type is to separate the details of the implementation from the properties essential to the correc use of it Brief Summary of classes \u00b6 A class is user-defined type A class consists of a set of members. The most common kinds of members are data members and member functions. Member functions can define the meaning of initialization, copy, move, and cleanup Members are accessed using . (dot) for objects and -> (arrow) for pointers. Operators, such as, + , ! , and [] , can be defined for a class A class is a namespace containing its members The public members provide the class's interface and the private members provide implementation details A struct is a class where members are by default public Class Basics \u00b6 class example \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class X { private : // the representation (implementation) is private int m ; public : // the user interface is public X ( int i = 0 ) : m { i } {} //a constructor (initialize the data member m) int mf ( int i ) { // a member function int old = m ; m = i ; //set a new value return old ; // return the old value } }; X var { 7 }; // a variable of type X, initialized to 7 int user ( X var , X * ptr ) { int x = var . mf ( 7 ); // access using . int y = ptr -> mf ( 9 ); // access using -> int z = var . m ; // error: cannot acces private member } 1. Member functions \u00b6 Functions declared within a class definition are called member functions 2. Default copying \u00b6 a class object can be initialized with a copy of an obejct of its class 1 2 UserClass c1 = c0 ; // initialization by copy UserClass c2 { d1 }; // initialization by copy 3. Access Control \u00b6 class is consist of two parts private part: can be used only by member functions , public part : interface to objects of class 4. class and struct \u00b6 a struct is a class in which members are by default public struct S{}; is simply short hand for class S{public: }; 5. Constructors \u00b6 a constructor is recognized by having the same name as the class it self. programmers can declare a function with the explicit purpose of initializing objects. 1 2 3 4 5 6 7 8 9 10 11 12 class Date { int d , m , y ; public : Date ( int dd , int mm , int yy ); // constructor } Date today = Date ( 23 , 6 , 1983 ); // OK Date xmas ( 25 , 12 , 1990 ); // OK -> abbreviated form Date my_birthday ; //error: initializer missing Date release1_0 ( 10 , 12 ) //error: third argument missing Date today = Date { 23 , 6 , 1982 } // good! I recommend the {} notation over the () notation for initializing, because it is explicit about what is being done we could use default values directly as default arguments 1 2 3 4 5 class Date { int d , m , y ; public : Date ( int dd = today . d , int mm = today . m , int yy = today . y ); // constructor }","title":"Class"},{"location":"Language/Class/#class","text":"C++ classes are a tool for creating new types that can be used conveniently as builtin types The Fundamental idea in defining a new type is to separate the details of the implementation from the properties essential to the correc use of it","title":"Class"},{"location":"Language/Class/#brief_summary_of_classes","text":"A class is user-defined type A class consists of a set of members. The most common kinds of members are data members and member functions. Member functions can define the meaning of initialization, copy, move, and cleanup Members are accessed using . (dot) for objects and -> (arrow) for pointers. Operators, such as, + , ! , and [] , can be defined for a class A class is a namespace containing its members The public members provide the class's interface and the private members provide implementation details A struct is a class where members are by default public","title":"Brief Summary of classes"},{"location":"Language/Class/#class_basics","text":"","title":"Class Basics"},{"location":"Language/Class/#class_example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class X { private : // the representation (implementation) is private int m ; public : // the user interface is public X ( int i = 0 ) : m { i } {} //a constructor (initialize the data member m) int mf ( int i ) { // a member function int old = m ; m = i ; //set a new value return old ; // return the old value } }; X var { 7 }; // a variable of type X, initialized to 7 int user ( X var , X * ptr ) { int x = var . mf ( 7 ); // access using . int y = ptr -> mf ( 9 ); // access using -> int z = var . m ; // error: cannot acces private member }","title":"class example"},{"location":"Language/Class/#1_member_functions","text":"Functions declared within a class definition are called member functions","title":"1. Member functions"},{"location":"Language/Class/#2_default_copying","text":"a class object can be initialized with a copy of an obejct of its class 1 2 UserClass c1 = c0 ; // initialization by copy UserClass c2 { d1 }; // initialization by copy","title":"2. Default copying"},{"location":"Language/Class/#3_access_control","text":"class is consist of two parts private part: can be used only by member functions , public part : interface to objects of class","title":"3. Access Control"},{"location":"Language/Class/#4_class_and_struct","text":"a struct is a class in which members are by default public struct S{}; is simply short hand for class S{public: };","title":"4. class and struct"},{"location":"Language/Class/#5_constructors","text":"a constructor is recognized by having the same name as the class it self. programmers can declare a function with the explicit purpose of initializing objects. 1 2 3 4 5 6 7 8 9 10 11 12 class Date { int d , m , y ; public : Date ( int dd , int mm , int yy ); // constructor } Date today = Date ( 23 , 6 , 1983 ); // OK Date xmas ( 25 , 12 , 1990 ); // OK -> abbreviated form Date my_birthday ; //error: initializer missing Date release1_0 ( 10 , 12 ) //error: third argument missing Date today = Date { 23 , 6 , 1982 } // good! I recommend the {} notation over the () notation for initializing, because it is explicit about what is being done we could use default values directly as default arguments 1 2 3 4 5 class Date { int d , m , y ; public : Date ( int dd = today . d , int mm = today . m , int yy = today . y ); // constructor }","title":"5. Constructors"},{"location":"Language/Keywords/","text":"Aliases \u00b6 Aliases are used when we want to insulate our code from details of the underlying machine. - note that naming a type after its representation than its purpose is not neccessarily a good idea. 1. typedef \u00b6 1 2 cpp typedef double decimal_places ; // is equivalent to \"using decimal_places = double;\" 2. using \u00b6 the using keyword can also be used to introduce a template alias. 1 2 template < typename T > using Vector = std :: vector < T , my_allocator < T >> but we cannot apply type specifiers, such as unsigned, to an alias. 1 2 3 using Char = char ; using Uchar = unsigned Char ; //error using Uchar = unsigned char ; // ok","title":"Keywords"},{"location":"Language/Keywords/#aliases","text":"Aliases are used when we want to insulate our code from details of the underlying machine. - note that naming a type after its representation than its purpose is not neccessarily a good idea.","title":"Aliases"},{"location":"Language/Keywords/#1_typedef","text":"1 2 cpp typedef double decimal_places ; // is equivalent to \"using decimal_places = double;\"","title":"1. typedef"},{"location":"Language/Keywords/#2_using","text":"the using keyword can also be used to introduce a template alias. 1 2 template < typename T > using Vector = std :: vector < T , my_allocator < T >> but we cannot apply type specifiers, such as unsigned, to an alias. 1 2 3 using Char = char ; using Uchar = unsigned Char ; //error using Uchar = unsigned char ; // ok","title":"2. using"},{"location":"Language/Preface/","text":"Why C++? \u00b6 What You Should Know Before.. \u00b6 you should be able to write C++ programs using components such as IOstreams and containers from C++ STL. You Should be also be familiar with the basic features of \"Modern C++\", such as auto, decltype, move semantics, and lambdas. c++17 modern C++ \u00b6 We will use number of these new features of modern C++ 1. C++11 \u00b6 Variadic templates Alias templates Move semantics, rvalue references, and perfect forwarding Standard type traits 2. C++14 \u00b6 Variable templates Generic Lambdas 3. C++17 \u00b6 Class template argument deduction Compile-time if Fold expressions Style Guide \u00b6 1. the order of constant qualifier. \u00b6 What is in front of const qualifier is always a constant 1 2 int const MAX_SIZE = 100 ; // the int is constant int * const P ; // the pointer cannot change, but int value can; 1 2 const int MAX_SIZE = 100 ; const int * P ; // you can not find what's constant value; reason1. easy to know what's constant. it's always what is in front of the const qualifier reason2. syntatical substitution principle. consider following example 1 2 3 4 5 6 7 typedef char * CHARS ; typedef CHARS const CPTR ; // constant pointer to chars // => typedef char * const CPTR ; using CHARS = char * : using CPTR = CHARS const ; // constant pointer to chars // => using CPTR = char * const ; The meaning of the second declaration is preseved when we textually replace CHARS with what it stands for; How ever if you write const before the type it qualifies. textually 1 2 3 typedef char * CHARS ; typedef const CHARS CTPR ; //const pointer to chars; // => typedef const char* CTPR // pointer to constant chars; footnote: note that typedef defines a \"type alias\" rather than a new type 1 2 3 4 typedef int newInt ; int i = 29 ; newInt j = 1999 ; i = j ; // OK 2. put the space between the & and the parameter name; \u00b6 by doing this, we emphasize the separation between the parameter type and the parameter name. 1 void foo ( int const & x ); 3. avoid declaring multiple entities in this way!. \u00b6 1 char * a , b ; according to the rules inherited from C, a is a pointer but b is an ordinary char ;","title":"Preface"},{"location":"Language/Preface/#why_c","text":"","title":"Why C++?"},{"location":"Language/Preface/#what_you_should_know_before","text":"you should be able to write C++ programs using components such as IOstreams and containers from C++ STL. You Should be also be familiar with the basic features of \"Modern C++\", such as auto, decltype, move semantics, and lambdas. c++17","title":"What You Should Know Before.."},{"location":"Language/Preface/#modern_c","text":"We will use number of these new features of modern C++","title":"modern C++"},{"location":"Language/Preface/#1_c11","text":"Variadic templates Alias templates Move semantics, rvalue references, and perfect forwarding Standard type traits","title":"1. C++11"},{"location":"Language/Preface/#2_c14","text":"Variable templates Generic Lambdas","title":"2. C++14"},{"location":"Language/Preface/#3_c17","text":"Class template argument deduction Compile-time if Fold expressions","title":"3. C++17"},{"location":"Language/Preface/#style_guide","text":"","title":"Style Guide"},{"location":"Language/Preface/#1_the_order_of_constant_qualifier","text":"What is in front of const qualifier is always a constant 1 2 int const MAX_SIZE = 100 ; // the int is constant int * const P ; // the pointer cannot change, but int value can; 1 2 const int MAX_SIZE = 100 ; const int * P ; // you can not find what's constant value; reason1. easy to know what's constant. it's always what is in front of the const qualifier reason2. syntatical substitution principle. consider following example 1 2 3 4 5 6 7 typedef char * CHARS ; typedef CHARS const CPTR ; // constant pointer to chars // => typedef char * const CPTR ; using CHARS = char * : using CPTR = CHARS const ; // constant pointer to chars // => using CPTR = char * const ; The meaning of the second declaration is preseved when we textually replace CHARS with what it stands for; How ever if you write const before the type it qualifies. textually 1 2 3 typedef char * CHARS ; typedef const CHARS CTPR ; //const pointer to chars; // => typedef const char* CTPR // pointer to constant chars; footnote: note that typedef defines a \"type alias\" rather than a new type 1 2 3 4 typedef int newInt ; int i = 29 ; newInt j = 1999 ; i = j ; // OK","title":"1. the order of constant qualifier."},{"location":"Language/Preface/#2_put_the_space_between_the_amp_and_the_parameter_name","text":"by doing this, we emphasize the separation between the parameter type and the parameter name. 1 void foo ( int const & x );","title":"2. put the space between the &amp; and the parameter name;"},{"location":"Language/Preface/#3_avoid_declaring_multiple_entities_in_this_way","text":"1 char * a , b ; according to the rules inherited from C, a is a pointer but b is an ordinary char ;","title":"3. avoid declaring multiple entities in this way!."},{"location":"MISC/Lemma/","text":"Lemma \u00b6 Lemma-1 \u00b6 if we run $dfs(root)$ in a rooted tree, then v is an ancestor of $u$ if and only if $st_v\\leq st_u\\leq ft_u\\leq ft_v$. Lemma-2 \u00b6","title":"Lemma"},{"location":"MISC/Lemma/#lemma","text":"","title":"Lemma"},{"location":"MISC/Lemma/#lemma-1","text":"if we run $dfs(root)$ in a rooted tree, then v is an ancestor of $u$ if and only if $st_v\\leq st_u\\leq ft_u\\leq ft_v$.","title":"Lemma-1"},{"location":"MISC/Lemma/#lemma-2","text":"","title":"Lemma-2"}]}